{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "from hpsklearn import HyperoptEstimator, lightgbm\n",
    "from hpsklearn import any_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [],
   "source": [
    "X = pd.read_hdf('../data/processed/train_data.h5')\n",
    "y = pd.read_hdf('../data/processed/train_labels.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, np.ravel(y), test_size=0.2, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(clip=True, feature_range=(-1.0 , 1.0))\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.49s/trial, best loss: 0.5138888888888888]\n",
      "100%|██████████| 2/2 [00:05<00:00,  5.83s/trial, best loss: 0.04166666666666663]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.20s/trial, best loss: 0.04166666666666663]\n",
      "100%|██████████| 4/4 [00:01<00:00,  1.57s/trial, best loss: 0.03981481481481486]\n",
      "100%|██████████| 5/5 [00:01<00:00,  1.76s/trial, best loss: 0.03981481481481486]\n",
      "[00:28:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 6/6 [00:10<00:00, 10.88s/trial, best loss: 0.03981481481481486]\n",
      "100%|██████████| 7/7 [00:00<00:00,  3.90trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 8/8 [00:03<00:00,  3.44s/trial, best loss: 0.019444444444444486]\n",
      "[00:28:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 9/9 [00:17<00:00, 17.30s/trial, best loss: 0.011111111111111072]\n",
      "[00:28:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 10/10 [00:06<00:00,  6.48s/trial, best loss: 0.011111111111111072]\n",
      "100%|██████████| 11/11 [00:00<00:00,  3.09trial/s, best loss: 0.011111111111111072]\n",
      "100%|██████████| 12/12 [00:00<00:00,  3.33trial/s, best loss: 0.011111111111111072]\n",
      "100%|██████████| 13/13 [00:00<00:00, 13.11trial/s, best loss: 0.011111111111111072]\n",
      "100%|██████████| 14/14 [00:09<00:00,  9.35s/trial, best loss: 0.011111111111111072]\n",
      "100%|██████████| 15/15 [00:00<00:00, 14.87trial/s, best loss: 0.011111111111111072]\n",
      "100%|██████████| 16/16 [00:01<00:00,  1.03s/trial, best loss: 0.011111111111111072]\n",
      "100%|██████████| 17/17 [00:00<00:00,  5.89trial/s, best loss: 0.011111111111111072]\n",
      "100%|██████████| 18/18 [00:00<00:00,  9.36trial/s, best loss: 0.011111111111111072]\n",
      "100%|██████████| 19/19 [00:00<00:00, 18.12trial/s, best loss: 0.011111111111111072]\n",
      "100%|██████████| 20/20 [00:00<00:00,  2.54trial/s, best loss: 0.011111111111111072]\n",
      "100%|██████████| 21/21 [00:23<00:00, 23.66s/trial, best loss: 0.011111111111111072]\n",
      "100%|██████████| 22/22 [00:03<00:00,  3.96s/trial, best loss: 0.010185185185185186]\n",
      "100%|██████████| 23/23 [00:09<00:00,  9.45s/trial, best loss: 0.010185185185185186]\n",
      "[00:29:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 24/24 [00:36<00:00, 36.19s/trial, best loss: 0.010185185185185186]\n",
      "100%|██████████| 25/25 [00:00<00:00, 16.70trial/s, best loss: 0.010185185185185186]\n",
      "100%|██████████| 26/26 [00:13<00:00, 13.61s/trial, best loss: 0.010185185185185186]\n",
      "[00:30:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 27/27 [01:00<00:00, 60.10s/trial, best loss: 0.010185185185185186]\n",
      "100%|██████████| 28/28 [00:00<00:00,  4.97trial/s, best loss: 0.010185185185185186]\n",
      "100%|██████████| 29/29 [00:00<00:00,  4.03trial/s, best loss: 0.010185185185185186]\n",
      "100%|██████████| 30/30 [00:00<00:00,  4.86trial/s, best loss: 0.010185185185185186]\n",
      "100%|██████████| 31/31 [00:34<00:00, 34.44s/trial, best loss: 0.010185185185185186]\n",
      "100%|██████████| 32/32 [00:00<00:00,  3.88trial/s, best loss: 0.010185185185185186]\n",
      "[00:31:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 33/33 [00:05<00:00,  5.93s/trial, best loss: 0.010185185185185186]\n",
      "100%|██████████| 34/34 [00:01<00:00,  1.08s/trial, best loss: 0.010185185185185186]\n",
      " 97%|█████████▋| 34/35 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:02<00:00,  2.44s/trial, best loss: 0.010185185185185186]\n",
      "100%|██████████| 36/36 [00:01<00:00,  1.32s/trial, best loss: 0.010185185185185186]\n",
      "100%|██████████| 37/37 [00:00<00:00,  5.80trial/s, best loss: 0.010185185185185186]\n",
      "[00:32:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 38/38 [00:20<00:00, 20.59s/trial, best loss: 0.010185185185185186]\n",
      "100%|██████████| 39/39 [00:00<00:00,  1.11trial/s, best loss: 0.010185185185185186]\n",
      "[00:32:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 40/40 [01:00<00:00, 60.08s/trial, best loss: 0.010185185185185186]\n",
      "100%|██████████| 41/41 [00:00<00:00,  2.81trial/s, best loss: 0.010185185185185186]\n",
      "100%|██████████| 42/42 [00:15<00:00, 15.99s/trial, best loss: 0.010185185185185186]\n",
      "100%|██████████| 43/43 [00:01<00:00,  1.25s/trial, best loss: 0.010185185185185186]\n",
      "100%|██████████| 44/44 [00:01<00:00,  1.57s/trial, best loss: 0.010185185185185186]\n",
      "100%|██████████| 45/45 [00:00<00:00, 17.21trial/s, best loss: 0.010185185185185186]\n",
      "[00:33:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 46/46 [00:27<00:00, 27.48s/trial, best loss: 0.007407407407407418]\n",
      "100%|██████████| 47/47 [00:44<00:00, 44.51s/trial, best loss: 0.007407407407407418]\n",
      "100%|██████████| 48/48 [00:00<00:00,  6.43trial/s, best loss: 0.007407407407407418]\n",
      "[00:35:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 49/49 [00:26<00:00, 26.27s/trial, best loss: 0.007407407407407418]\n",
      "100%|██████████| 50/50 [00:03<00:00,  3.63s/trial, best loss: 0.007407407407407418]\n",
      "100%|██████████| 51/51 [00:00<00:00,  1.60trial/s, best loss: 0.007407407407407418]\n",
      "100%|██████████| 52/52 [00:15<00:00, 15.99s/trial, best loss: 0.007407407407407418]\n",
      " 98%|█████████▊| 52/53 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:01<00:00,  1.94s/trial, best loss: 0.007407407407407418]\n",
      "100%|██████████| 54/54 [00:00<00:00,  8.75trial/s, best loss: 0.007407407407407418]\n",
      "100%|██████████| 55/55 [00:00<00:00,  9.35trial/s, best loss: 0.007407407407407418]\n",
      "100%|██████████| 56/56 [00:06<00:00,  6.07s/trial, best loss: 0.007407407407407418]\n",
      "100%|██████████| 57/57 [00:00<00:00,  4.36trial/s, best loss: 0.007407407407407418]\n",
      "100%|██████████| 58/58 [00:00<00:00,  6.21trial/s, best loss: 0.007407407407407418]\n",
      "100%|██████████| 59/59 [00:00<00:00,  3.28trial/s, best loss: 0.007407407407407418]\n",
      "100%|██████████| 60/60 [00:08<00:00,  8.30s/trial, best loss: 0.007407407407407418]\n",
      "[00:36:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 61/61 [00:45<00:00, 45.81s/trial, best loss: 0.007407407407407418]\n",
      "100%|██████████| 62/62 [00:00<00:00,  5.96trial/s, best loss: 0.007407407407407418]\n",
      "100%|██████████| 63/63 [00:09<00:00,  9.01s/trial, best loss: 0.007407407407407418]\n",
      "100%|██████████| 64/64 [01:00<00:00, 60.06s/trial, best loss: 0.007407407407407418]\n",
      "100%|██████████| 65/65 [00:02<00:00,  2.47s/trial, best loss: 0.007407407407407418]\n",
      "[00:38:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 66/66 [00:02<00:00,  2.39s/trial, best loss: 0.007407407407407418]\n",
      "[00:38:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 67/67 [01:00<00:00, 60.08s/trial, best loss: 0.007407407407407418]\n",
      "100%|██████████| 68/68 [00:00<00:00,  9.26trial/s, best loss: 0.007407407407407418]\n",
      "100%|██████████| 69/69 [00:00<00:00,  8.54trial/s, best loss: 0.007407407407407418]\n",
      "100%|██████████| 70/70 [00:00<00:00, 11.69trial/s, best loss: 0.007407407407407418]\n",
      "100%|██████████| 71/71 [00:00<00:00,  1.76trial/s, best loss: 0.007407407407407418]\n",
      "[00:39:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 72/72 [00:01<00:00,  1.56s/trial, best loss: 0.007407407407407418]\n",
      "100%|██████████| 73/73 [00:00<00:00,  2.02trial/s, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 74/74 [00:00<00:00,  3.40trial/s, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 75/75 [00:00<00:00, 19.42trial/s, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 76/76 [00:15<00:00, 15.45s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 77/77 [00:15<00:00, 15.74s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 78/78 [00:00<00:00, 19.41trial/s, best loss: 0.0009259259259258856]\n",
      "[00:39:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 79/79 [00:00<00:00,  1.07trial/s, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 80/80 [00:00<00:00,  1.04trial/s, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 81/81 [00:00<00:00,  3.30trial/s, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 82/82 [00:00<00:00,  1.89trial/s, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 83/83 [00:06<00:00,  6.80s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 84/84 [00:11<00:00, 11.05s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 85/85 [00:00<00:00,  6.34trial/s, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 86/86 [00:01<00:00,  1.42s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 87/87 [00:01<00:00,  1.73s/trial, best loss: 0.0009259259259258856]\n",
      "[00:40:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 88/88 [00:25<00:00, 25.47s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 89/89 [00:05<00:00,  5.32s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 90/90 [00:00<00:00,  4.36trial/s, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 91/91 [00:01<00:00,  1.81s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 92/92 [00:01<00:00,  1.62s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 93/93 [00:03<00:00,  3.90s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 94/94 [00:00<00:00, 15.58trial/s, best loss: 0.0009259259259258856]\n",
      "[00:40:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 95/95 [00:01<00:00,  1.47s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 96/96 [00:00<00:00,  1.41trial/s, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 97/97 [00:02<00:00,  2.11s/trial, best loss: 0.0009259259259258856]\n",
      "[00:40:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 98/98 [00:22<00:00, 22.52s/trial, best loss: 0.0009259259259258856]\n",
      "[00:41:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 99/99 [00:33<00:00, 33.07s/trial, best loss: 0.0009259259259258856]\n",
      "[00:41:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 100/100 [00:01<00:00,  1.90s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 101/101 [00:08<00:00,  8.01s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 102/102 [00:00<00:00,  2.01trial/s, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 103/103 [00:00<00:00, 12.11trial/s, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 104/104 [00:01<00:00,  1.63s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 105/105 [00:00<00:00,  6.01trial/s, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 106/106 [00:04<00:00,  4.19s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 107/107 [00:19<00:00, 19.99s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 108/108 [00:00<00:00,  1.72trial/s, best loss: 0.0009259259259258856]\n",
      "[00:42:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 109/109 [00:43<00:00, 43.09s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 110/110 [01:00<00:00, 60.02s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 111/111 [00:00<00:00,  2.13trial/s, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 112/112 [01:00<00:00, 60.08s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 113/113 [00:00<00:00, 11.36trial/s, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 114/114 [00:00<00:00, 16.46trial/s, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 115/115 [00:00<00:00,  9.42trial/s, best loss: 0.0009259259259258856]\n",
      " 99%|█████████▉| 115/116 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 116/116 [00:02<00:00,  2.49s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 117/117 [00:00<00:00,  8.71trial/s, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 118/118 [00:00<00:00,  4.80trial/s, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 119/119 [00:00<00:00, 17.91trial/s, best loss: 0.0009259259259258856]\n",
      "[00:45:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 120/120 [00:28<00:00, 28.50s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 121/121 [00:01<00:00,  1.82s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 122/122 [00:00<00:00,  7.62trial/s, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 123/123 [00:00<00:00,  3.73trial/s, best loss: 0.0009259259259258856]\n",
      "[00:45:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 124/124 [01:00<00:00, 60.09s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 125/125 [00:24<00:00, 24.59s/trial, best loss: 0.0009259259259258856]\n",
      "[00:47:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 126/126 [00:16<00:00, 16.78s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 127/127 [00:00<00:00,  2.67trial/s, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 128/128 [00:04<00:00,  4.76s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 129/129 [00:00<00:00,  9.82trial/s, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 130/130 [00:26<00:00, 26.06s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 131/131 [00:00<00:00,  1.69trial/s, best loss: 0.0009259259259258856]\n",
      "[00:47:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 132/132 [00:09<00:00,  9.67s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 133/133 [00:01<00:00,  1.22s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 134/134 [00:00<00:00, 11.39trial/s, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 135/135 [00:00<00:00, 15.21trial/s, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 136/136 [00:00<00:00,  5.68trial/s, best loss: 0.0009259259259258856]\n",
      "[00:48:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 137/137 [00:05<00:00,  5.16s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 138/138 [00:00<00:00,  5.18trial/s, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 139/139 [00:00<00:00,  3.90trial/s, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 140/140 [00:02<00:00,  2.44s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 141/141 [00:00<00:00,  2.15trial/s, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 142/142 [00:00<00:00, 21.09trial/s, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 143/143 [00:05<00:00,  5.66s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 144/144 [00:03<00:00,  3.49s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 145/145 [00:00<00:00,  2.63trial/s, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 146/146 [00:00<00:00, 22.81trial/s, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 147/147 [01:00<00:00, 60.08s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 148/148 [00:22<00:00, 22.58s/trial, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 149/149 [00:00<00:00, 16.31trial/s, best loss: 0.0009259259259258856]\n",
      "100%|██████████| 150/150 [00:05<00:00,  5.12s/trial, best loss: 0.0009259259259258856]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'learner': KNeighborsClassifier(algorithm='ball_tree', leaf_size=31, metric='manhattan',\n                      n_jobs=1, n_neighbors=1, p=1.4528804003307858,\n                      weights='distance'),\n 'preprocs': (MinMaxScaler(clip=True, feature_range=(-1.0, 1.0)),),\n 'ex_preprocs': ()}"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hpsklearn import HyperoptEstimator, any_classifier, any_preprocessing\n",
    "\n",
    "estim = HyperoptEstimator(classifier=any_classifier('cla'), preprocessing=any_preprocessing('pre'),max_evals=150,trial_timeout=60)\n",
    "estim.fit(X_train, y_train)\n",
    "estim.best_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.12trial/s, best loss: 0.0064814814814815325]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.65s/trial, best loss: 0.0064814814814815325]\n",
      "100%|██████████| 3/3 [00:28<00:00, 28.90s/trial, best loss: 0.0064814814814815325]\n",
      "100%|██████████| 4/4 [00:00<00:00,  1.77trial/s, best loss: 0.005555555555555536]\n",
      "100%|██████████| 5/5 [00:00<00:00,  6.05trial/s, best loss: 0.005555555555555536]\n",
      "100%|██████████| 6/6 [00:23<00:00, 23.43s/trial, best loss: 0.005555555555555536]\n",
      "100%|██████████| 7/7 [00:00<00:00,  1.25trial/s, best loss: 0.005555555555555536]\n",
      "100%|██████████| 8/8 [00:00<00:00,  5.20trial/s, best loss: 0.005555555555555536]\n",
      "100%|██████████| 9/9 [00:00<00:00,  2.89trial/s, best loss: 0.005555555555555536]\n",
      "100%|██████████| 10/10 [00:00<00:00,  1.48trial/s, best loss: 0.005555555555555536]\n",
      "100%|██████████| 11/11 [00:00<00:00,  6.87trial/s, best loss: 0.005555555555555536]\n",
      "100%|██████████| 12/12 [00:00<00:00,  1.25trial/s, best loss: 0.005555555555555536]\n",
      "100%|██████████| 13/13 [00:00<00:00,  3.95trial/s, best loss: 0.005555555555555536]\n",
      "100%|██████████| 14/14 [00:24<00:00, 24.69s/trial, best loss: 0.005555555555555536]\n",
      "100%|██████████| 15/15 [00:23<00:00, 23.61s/trial, best loss: 0.005555555555555536]\n",
      "100%|██████████| 16/16 [00:00<00:00,  2.86trial/s, best loss: 0.0037037037037036535]\n",
      "100%|██████████| 17/17 [00:00<00:00,  2.69trial/s, best loss: 0.0037037037037036535]\n",
      "100%|██████████| 18/18 [00:00<00:00,  2.66trial/s, best loss: 0.0037037037037036535]\n",
      "100%|██████████| 19/19 [00:00<00:00,  2.80trial/s, best loss: 0.0037037037037036535]\n",
      "100%|██████████| 20/20 [00:06<00:00,  6.11s/trial, best loss: 0.0037037037037036535]\n",
      "100%|██████████| 21/21 [00:00<00:00,  9.50trial/s, best loss: 0.0037037037037036535]\n",
      "100%|██████████| 22/22 [00:15<00:00, 15.29s/trial, best loss: 0.0037037037037036535]\n",
      "100%|██████████| 23/23 [00:00<00:00,  1.77trial/s, best loss: 0.0037037037037036535]\n",
      "100%|██████████| 24/24 [00:00<00:00,  9.77trial/s, best loss: 0.0037037037037036535]\n",
      "100%|██████████| 25/25 [00:00<00:00,  7.86trial/s, best loss: 0.0037037037037036535]\n",
      "100%|██████████| 26/26 [00:00<00:00,  1.37trial/s, best loss: 0.0037037037037036535]\n",
      "100%|██████████| 27/27 [00:00<00:00,  1.56trial/s, best loss: 0.0037037037037036535]\n",
      "100%|██████████| 28/28 [00:00<00:00,  1.35trial/s, best loss: 0.0037037037037036535]\n",
      "100%|██████████| 29/29 [00:00<00:00,  2.70trial/s, best loss: 0.0037037037037036535]\n",
      "100%|██████████| 30/30 [00:00<00:00,  3.05trial/s, best loss: 0.0037037037037036535]\n",
      "100%|██████████| 31/31 [00:00<00:00,  1.84trial/s, best loss: 0.0037037037037036535]\n",
      "100%|██████████| 32/32 [00:00<00:00,  7.57trial/s, best loss: 0.0037037037037036535]\n",
      "100%|██████████| 33/33 [00:40<00:00, 40.73s/trial, best loss: 0.0037037037037036535]\n",
      "100%|██████████| 34/34 [00:00<00:00,  2.69trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 35/35 [00:00<00:00,  1.85trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 36/36 [00:00<00:00,  1.65trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 37/37 [00:00<00:00,  2.51trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 38/38 [00:00<00:00,  5.04trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 39/39 [00:00<00:00,  1.26trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 40/40 [00:27<00:00, 27.73s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 41/41 [00:00<00:00,  1.30trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 42/42 [00:26<00:00, 26.13s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 43/43 [00:00<00:00,  6.96trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 44/44 [00:00<00:00,  1.46trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 45/45 [00:00<00:00,  2.61trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 46/46 [00:00<00:00,  1.54trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 47/47 [00:11<00:00, 11.72s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 48/48 [00:00<00:00,  2.46trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 49/49 [00:10<00:00, 10.64s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 50/50 [00:00<00:00,  2.16trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 51/51 [00:00<00:00,  2.67trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 52/52 [00:00<00:00,  5.73trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 53/53 [00:00<00:00,  2.53trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 54/54 [00:00<00:00,  5.90trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 55/55 [00:00<00:00,  1.10trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 56/56 [00:00<00:00, 17.04trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 57/57 [00:00<00:00,  1.67trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 58/58 [00:00<00:00,  3.64trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 59/59 [00:00<00:00,  1.18trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 60/60 [00:24<00:00, 24.03s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 61/61 [00:00<00:00,  1.69trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 62/62 [00:24<00:00, 24.69s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 63/63 [00:00<00:00,  1.40trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 64/64 [00:00<00:00,  1.26trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 65/65 [00:00<00:00,  2.42trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 66/66 [00:00<00:00,  1.13trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 67/67 [00:00<00:00,  2.59trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 68/68 [00:00<00:00,  7.33trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 69/69 [00:00<00:00,  1.65trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 70/70 [00:26<00:00, 26.38s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 71/71 [00:24<00:00, 24.69s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 72/72 [00:00<00:00,  2.54trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 73/73 [00:25<00:00, 25.13s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 74/74 [00:00<00:00,  7.51trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 75/75 [00:00<00:00,  2.57trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 76/76 [00:00<00:00,  1.44trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 77/77 [00:00<00:00,  2.62trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 78/78 [00:00<00:00,  5.84trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 79/79 [00:00<00:00,  2.57trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 80/80 [00:00<00:00, 10.91trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 81/81 [00:00<00:00,  1.26trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 82/82 [00:00<00:00,  2.36trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 83/83 [00:00<00:00,  2.79trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 84/84 [00:00<00:00,  4.78trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 85/85 [00:25<00:00, 25.01s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 86/86 [00:00<00:00,  2.54trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 87/87 [00:24<00:00, 24.96s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 88/88 [00:00<00:00,  7.71trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 89/89 [00:00<00:00,  6.37trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 90/90 [00:00<00:00,  6.90trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 91/91 [00:00<00:00,  2.87trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 92/92 [00:00<00:00,  1.57trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 93/93 [00:07<00:00,  7.90s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 94/94 [00:25<00:00, 25.18s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 95/95 [00:00<00:00, 16.65trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 96/96 [00:24<00:00, 24.90s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 97/97 [00:00<00:00,  1.63trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 98/98 [00:00<00:00,  1.45trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 99/99 [00:00<00:00,  1.56trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 100/100 [00:00<00:00,  2.63trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 101/101 [00:00<00:00,  1.71trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 102/102 [00:00<00:00,  1.44trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 103/103 [00:00<00:00,  4.76trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 104/104 [00:00<00:00,  2.32trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 105/105 [00:00<00:00,  2.57trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 106/106 [00:00<00:00,  5.63trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 107/107 [00:00<00:00,  1.31trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 108/108 [00:00<00:00,  1.59trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 109/109 [00:00<00:00,  4.66trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 110/110 [00:00<00:00,  1.15trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 111/111 [00:00<00:00,  2.34trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 112/112 [00:26<00:00, 26.58s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 113/113 [00:26<00:00, 26.69s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 114/114 [00:00<00:00,  7.52trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 115/115 [00:00<00:00,  2.64trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 116/116 [00:00<00:00,  1.62trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 117/117 [00:00<00:00,  2.82trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 118/118 [00:24<00:00, 24.90s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 119/119 [00:00<00:00,  4.99trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 120/120 [00:00<00:00,  1.68trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 121/121 [00:12<00:00, 12.19s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 122/122 [00:01<00:00,  1.02s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 123/123 [00:00<00:00,  8.14trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 124/124 [00:00<00:00,  6.37trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 125/125 [00:00<00:00,  7.51trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 126/126 [00:00<00:00,  6.12trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 127/127 [00:19<00:00, 19.13s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 128/128 [00:00<00:00,  1.81trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 129/129 [00:00<00:00,  2.14trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 130/130 [00:00<00:00,  2.75trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 131/131 [00:00<00:00,  1.93trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 132/132 [00:00<00:00,  3.06trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 133/133 [00:21<00:00, 21.66s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 134/134 [00:00<00:00,  2.41trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 135/135 [00:00<00:00,  1.96trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 136/136 [00:00<00:00,  1.76trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 137/137 [00:00<00:00,  7.87trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 138/138 [00:00<00:00,  2.80trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 139/139 [00:00<00:00,  6.89trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 140/140 [00:00<00:00,  5.77trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 141/141 [00:00<00:00,  2.44trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 142/142 [00:00<00:00,  1.87trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 143/143 [00:00<00:00, 11.60trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 144/144 [00:00<00:00,  1.78trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 145/145 [00:00<00:00,  7.85trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 146/146 [00:21<00:00, 21.28s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 147/147 [00:00<00:00,  1.37trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 148/148 [00:00<00:00,  5.38trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 149/149 [00:00<00:00,  7.76trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 150/150 [00:00<00:00, 16.42trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 151/151 [00:00<00:00,  3.15trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 152/152 [00:00<00:00,  1.96trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 153/153 [00:00<00:00,  1.37trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 154/154 [00:00<00:00,  4.03trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 155/155 [00:00<00:00,  2.98trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 156/156 [00:00<00:00,  6.86trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 157/157 [00:00<00:00,  7.54trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 158/158 [00:00<00:00,  1.92trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 159/159 [00:00<00:00,  6.96trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 160/160 [00:00<00:00,  3.04trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 161/161 [00:12<00:00, 12.27s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 162/162 [00:00<00:00,  1.78trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 163/163 [00:00<00:00,  7.81trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 164/164 [00:00<00:00, 10.71trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 165/165 [00:00<00:00,  3.01trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 166/166 [00:00<00:00,  2.86trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 167/167 [00:00<00:00, 10.62trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 168/168 [00:00<00:00,  2.74trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 169/169 [00:00<00:00,  2.68trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 170/170 [00:00<00:00,  1.70trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 171/171 [00:00<00:00,  1.34trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 172/172 [00:00<00:00, 10.15trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 173/173 [00:00<00:00,  2.86trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 174/174 [00:00<00:00,  1.98trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 175/175 [00:00<00:00,  3.73trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 176/176 [00:00<00:00,  6.91trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 177/177 [00:00<00:00,  3.30trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 178/178 [00:22<00:00, 22.04s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 179/179 [00:00<00:00,  1.83trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 180/180 [00:00<00:00,  2.94trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 181/181 [00:00<00:00,  1.49trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 182/182 [00:00<00:00,  3.02trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 183/183 [00:00<00:00, 10.66trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 184/184 [00:00<00:00,  2.90trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 185/185 [00:00<00:00,  2.02trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 186/186 [00:00<00:00,  3.35trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 187/187 [00:21<00:00, 21.29s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 188/188 [00:00<00:00,  3.18trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 189/189 [00:00<00:00,  1.12trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 190/190 [00:24<00:00, 24.89s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 191/191 [00:00<00:00,  2.51trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 192/192 [00:00<00:00,  8.95trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 193/193 [00:03<00:00,  3.77s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 194/194 [00:00<00:00,  6.78trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 195/195 [00:00<00:00, 11.02trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 196/196 [00:00<00:00,  5.99trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 197/197 [00:00<00:00,  1.19trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 198/198 [00:00<00:00,  5.19trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 199/199 [00:25<00:00, 25.56s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 200/200 [00:00<00:00,  7.66trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 201/201 [00:00<00:00,  2.40trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 202/202 [00:00<00:00,  1.19trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 203/203 [00:00<00:00,  5.43trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 204/204 [00:00<00:00,  1.08trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 205/205 [00:00<00:00,  1.11trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 206/206 [00:00<00:00,  2.46trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 207/207 [00:00<00:00,  2.51trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 208/208 [00:00<00:00,  1.54trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 209/209 [00:00<00:00,  1.46trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 210/210 [00:00<00:00,  1.30trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 211/211 [00:00<00:00,  2.25trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 212/212 [00:00<00:00,  1.59trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 213/213 [00:00<00:00,  1.08trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 214/214 [00:00<00:00,  1.64trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 215/215 [00:00<00:00,  1.67trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 216/216 [00:00<00:00,  2.44trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 217/217 [00:00<00:00,  5.02trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 218/218 [00:00<00:00,  2.41trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 219/219 [00:00<00:00, 15.44trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 220/220 [00:00<00:00,  1.30trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 221/221 [00:00<00:00,  1.64trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 222/222 [00:00<00:00,  1.06trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 223/223 [00:00<00:00,  4.82trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 224/224 [00:00<00:00,  2.53trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 225/225 [00:00<00:00,  1.32trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 226/226 [00:00<00:00, 16.22trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 227/227 [00:00<00:00, 10.32trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 228/228 [00:00<00:00,  6.25trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 229/229 [00:00<00:00,  2.42trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 230/230 [00:00<00:00,  2.47trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 231/231 [00:00<00:00,  1.74trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 232/232 [00:24<00:00, 24.81s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 233/233 [00:00<00:00,  1.47trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 234/234 [00:00<00:00,  2.50trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 235/235 [00:21<00:00, 21.94s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 236/236 [00:00<00:00,  2.55trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 237/237 [00:00<00:00,  8.82trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 238/238 [00:00<00:00,  6.14trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 239/239 [00:00<00:00,  1.46trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 240/240 [00:00<00:00,  2.23trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 241/241 [00:00<00:00,  2.44trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 242/242 [00:00<00:00,  2.18trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 243/243 [00:01<00:00,  1.02s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 244/244 [00:00<00:00,  6.26trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 245/245 [00:00<00:00,  2.47trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 246/246 [00:00<00:00,  2.52trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 247/247 [00:00<00:00,  7.89trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 248/248 [00:00<00:00,  2.50trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 249/249 [00:00<00:00,  2.21trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 250/250 [00:00<00:00,  1.36trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 251/251 [00:00<00:00,  1.57trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 252/252 [00:00<00:00,  1.61trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 253/253 [00:00<00:00,  1.04trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 254/254 [00:00<00:00,  1.13trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 255/255 [00:00<00:00,  1.64trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 256/256 [00:00<00:00,  1.76trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 257/257 [00:00<00:00,  6.90trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 258/258 [00:00<00:00,  3.00trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 259/259 [00:00<00:00,  2.84trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 260/260 [00:00<00:00,  1.33trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 261/261 [00:22<00:00, 22.59s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 262/262 [00:00<00:00,  1.91trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 263/263 [00:00<00:00,  1.93trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 264/264 [00:00<00:00,  9.23trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 265/265 [00:00<00:00,  3.39trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 266/266 [00:00<00:00,  8.09trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 267/267 [00:00<00:00,  7.03trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 268/268 [00:00<00:00, 15.20trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 269/269 [00:00<00:00,  1.75trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 270/270 [00:00<00:00,  1.90trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 271/271 [00:00<00:00,  1.59trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 272/272 [00:00<00:00,  3.14trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 273/273 [00:00<00:00,  7.12trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 274/274 [00:00<00:00,  7.44trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 275/275 [00:21<00:00, 21.68s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 276/276 [00:00<00:00,  2.38trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 277/277 [00:00<00:00,  2.40trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 278/278 [00:00<00:00, 12.62trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 279/279 [00:22<00:00, 22.83s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 280/280 [00:22<00:00, 22.25s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 281/281 [00:00<00:00,  9.31trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 282/282 [00:00<00:00,  2.01trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 283/283 [00:00<00:00,  1.91trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 284/284 [00:00<00:00,  7.17trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 285/285 [00:00<00:00, 10.67trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 286/286 [00:00<00:00,  1.68trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 287/287 [00:09<00:00,  9.28s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 288/288 [00:00<00:00,  1.88trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 289/289 [00:00<00:00,  3.04trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 290/290 [00:21<00:00, 21.83s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 291/291 [00:00<00:00,  3.28trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 292/292 [00:00<00:00,  2.94trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 293/293 [00:00<00:00,  6.64trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 294/294 [00:00<00:00, 13.21trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 295/295 [00:00<00:00,  1.79trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 296/296 [00:00<00:00,  2.40trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 297/297 [00:00<00:00,  7.78trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 298/298 [00:23<00:00, 23.31s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 299/299 [00:25<00:00, 25.65s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 300/300 [00:00<00:00,  2.69trial/s, best loss: 0.002777777777777768]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'learner': KNeighborsClassifier(algorithm='brute', leaf_size=37, metric='cityblock',\n                      n_jobs=1, n_neighbors=1, p=2.702587347700451),\n 'preprocs': (MinMaxScaler(feature_range=(0.0, 1.0)),),\n 'ex_preprocs': ()}"
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hpsklearn import HyperoptEstimator, k_neighbors_classifier, any_preprocessing\n",
    "\n",
    "estim = HyperoptEstimator(classifier=k_neighbors_classifier('knn'), max_evals=300,trial_timeout=120)\n",
    "estim.fit(X_train, y_train)\n",
    "estim.best_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9985007496251874, 1.0]\n",
      "0.9998500749625187\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = pd.read_hdf('../data/processed/train_data.h5')\n",
    "y = pd.read_hdf('../data/processed/train_labels.h5')\n",
    "\n",
    "cv = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "scores = []\n",
    "model = KNeighborsClassifier(algorithm='ball_tree', leaf_size=31, metric='manhattan',\n",
    "                      n_jobs=1, n_neighbors=1, p=1.4528804003307858,\n",
    "                      weights='distance')\n",
    "\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "\n",
    "    X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    scaler = MinMaxScaler(clip=True, feature_range=(-1.0 , 1.0))\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    model.fit(X_train, np.ravel(y_train))\n",
    "    y_pred = model.predict(X_test)\n",
    "    scores.append(f1_score(np.ravel(y_test), y_pred))\n",
    "\n",
    "#scor = score(X=X, y=y, cv=cv, model=estim.best_model()['learner'])\n",
    "\n",
    "#model.fit(X_train, y_train)\n",
    "#y_pred = model.predict(X_test)\n",
    "print(scores)\n",
    "print(np.mean(scores))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "data": {
      "text/plain": "      target\n0          1\n1          1\n2          1\n3          1\n4          1\n...      ...\n6745      -1\n6746      -1\n6747      -1\n6748      -1\n6749      -1\n\n[6750 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6745</th>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>6746</th>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>6747</th>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>6748</th>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>6749</th>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n<p>6750 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(algorithm='ball_tree', leaf_size=37, metric='l1', n_jobs=1, n_neighbors=1, p=4.568992939900089)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(f1_score(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:31:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Learning rate set to 0.021167\n",
      "0:\tlearn: 0.6596737\ttotal: 11.7ms\tremaining: 11.7s\n",
      "1:\tlearn: 0.6320083\ttotal: 30.6ms\tremaining: 15.3s\n",
      "2:\tlearn: 0.6012702\ttotal: 57.1ms\tremaining: 19s\n",
      "3:\tlearn: 0.5754175\ttotal: 90.8ms\tremaining: 22.6s\n",
      "4:\tlearn: 0.5482824\ttotal: 125ms\tremaining: 25s\n",
      "5:\tlearn: 0.5233869\ttotal: 140ms\tremaining: 23.2s\n",
      "6:\tlearn: 0.5017492\ttotal: 158ms\tremaining: 22.4s\n",
      "7:\tlearn: 0.4812764\ttotal: 174ms\tremaining: 21.6s\n",
      "8:\tlearn: 0.4614723\ttotal: 187ms\tremaining: 20.6s\n",
      "9:\tlearn: 0.4441186\ttotal: 219ms\tremaining: 21.7s\n",
      "10:\tlearn: 0.4266922\ttotal: 310ms\tremaining: 27.9s\n",
      "11:\tlearn: 0.4098757\ttotal: 388ms\tremaining: 32s\n",
      "12:\tlearn: 0.3951833\ttotal: 472ms\tremaining: 35.9s\n",
      "13:\tlearn: 0.3796103\ttotal: 552ms\tremaining: 38.9s\n",
      "14:\tlearn: 0.3645362\ttotal: 631ms\tremaining: 41.4s\n",
      "15:\tlearn: 0.3499352\ttotal: 713ms\tremaining: 43.9s\n",
      "16:\tlearn: 0.3352826\ttotal: 780ms\tremaining: 45.1s\n",
      "17:\tlearn: 0.3247301\ttotal: 854ms\tremaining: 46.6s\n",
      "18:\tlearn: 0.3138309\ttotal: 929ms\tremaining: 47.9s\n",
      "19:\tlearn: 0.3040217\ttotal: 1s\tremaining: 49.1s\n",
      "20:\tlearn: 0.2889162\ttotal: 1.07s\tremaining: 49.9s\n",
      "21:\tlearn: 0.2799810\ttotal: 1.13s\tremaining: 50.2s\n",
      "22:\tlearn: 0.2674311\ttotal: 1.19s\tremaining: 50.7s\n",
      "23:\tlearn: 0.2577557\ttotal: 1.26s\tremaining: 51.2s\n",
      "24:\tlearn: 0.2495968\ttotal: 1.32s\tremaining: 51.7s\n",
      "25:\tlearn: 0.2427125\ttotal: 1.4s\tremaining: 52.5s\n",
      "26:\tlearn: 0.2381265\ttotal: 1.47s\tremaining: 53s\n",
      "27:\tlearn: 0.2316033\ttotal: 1.54s\tremaining: 53.4s\n",
      "28:\tlearn: 0.2252170\ttotal: 1.6s\tremaining: 53.7s\n",
      "29:\tlearn: 0.2193350\ttotal: 1.67s\tremaining: 54s\n",
      "30:\tlearn: 0.2121099\ttotal: 1.74s\tremaining: 54.3s\n",
      "31:\tlearn: 0.2084541\ttotal: 1.81s\tremaining: 54.7s\n",
      "32:\tlearn: 0.2048394\ttotal: 1.87s\tremaining: 54.9s\n",
      "33:\tlearn: 0.1983800\ttotal: 1.94s\tremaining: 55s\n",
      "34:\tlearn: 0.1930656\ttotal: 2s\tremaining: 55.1s\n",
      "35:\tlearn: 0.1898954\ttotal: 2.06s\tremaining: 55.3s\n",
      "36:\tlearn: 0.1853986\ttotal: 2.13s\tremaining: 55.4s\n",
      "37:\tlearn: 0.1823281\ttotal: 2.18s\tremaining: 55.3s\n",
      "38:\tlearn: 0.1774546\ttotal: 2.25s\tremaining: 55.4s\n",
      "39:\tlearn: 0.1744370\ttotal: 2.32s\tremaining: 55.6s\n",
      "40:\tlearn: 0.1714988\ttotal: 2.38s\tremaining: 55.7s\n",
      "41:\tlearn: 0.1688055\ttotal: 2.45s\tremaining: 55.8s\n",
      "42:\tlearn: 0.1643149\ttotal: 2.52s\tremaining: 56.1s\n",
      "43:\tlearn: 0.1618087\ttotal: 2.58s\tremaining: 56.1s\n",
      "44:\tlearn: 0.1580444\ttotal: 2.65s\tremaining: 56.3s\n",
      "45:\tlearn: 0.1557523\ttotal: 2.72s\tremaining: 56.4s\n",
      "46:\tlearn: 0.1521148\ttotal: 2.78s\tremaining: 56.4s\n",
      "47:\tlearn: 0.1497424\ttotal: 2.85s\tremaining: 56.5s\n",
      "48:\tlearn: 0.1475955\ttotal: 2.91s\tremaining: 56.5s\n",
      "49:\tlearn: 0.1433501\ttotal: 2.98s\tremaining: 56.7s\n",
      "50:\tlearn: 0.1413633\ttotal: 3.04s\tremaining: 56.6s\n",
      "51:\tlearn: 0.1394605\ttotal: 3.1s\tremaining: 56.6s\n",
      "52:\tlearn: 0.1375208\ttotal: 3.17s\tremaining: 56.6s\n",
      "53:\tlearn: 0.1357949\ttotal: 3.23s\tremaining: 56.7s\n",
      "54:\tlearn: 0.1340198\ttotal: 3.3s\tremaining: 56.7s\n",
      "55:\tlearn: 0.1322219\ttotal: 3.37s\tremaining: 56.7s\n",
      "56:\tlearn: 0.1305695\ttotal: 3.43s\tremaining: 56.8s\n",
      "57:\tlearn: 0.1273818\ttotal: 3.5s\tremaining: 56.8s\n",
      "58:\tlearn: 0.1257321\ttotal: 3.56s\tremaining: 56.8s\n",
      "59:\tlearn: 0.1241278\ttotal: 3.63s\tremaining: 56.8s\n",
      "60:\tlearn: 0.1224794\ttotal: 3.69s\tremaining: 56.9s\n",
      "61:\tlearn: 0.1209067\ttotal: 3.76s\tremaining: 56.9s\n",
      "62:\tlearn: 0.1186850\ttotal: 3.83s\tremaining: 56.9s\n",
      "63:\tlearn: 0.1172049\ttotal: 3.89s\tremaining: 56.9s\n",
      "64:\tlearn: 0.1158887\ttotal: 3.96s\tremaining: 56.9s\n",
      "65:\tlearn: 0.1145444\ttotal: 4.02s\tremaining: 56.9s\n",
      "66:\tlearn: 0.1131581\ttotal: 4.09s\tremaining: 57s\n",
      "67:\tlearn: 0.1119525\ttotal: 4.16s\tremaining: 57s\n",
      "68:\tlearn: 0.1106233\ttotal: 4.22s\tremaining: 57s\n",
      "69:\tlearn: 0.1093564\ttotal: 4.29s\tremaining: 57s\n",
      "70:\tlearn: 0.1080788\ttotal: 4.36s\tremaining: 57s\n",
      "71:\tlearn: 0.1067194\ttotal: 4.42s\tremaining: 57s\n",
      "72:\tlearn: 0.1053820\ttotal: 4.49s\tremaining: 57s\n",
      "73:\tlearn: 0.1041284\ttotal: 4.56s\tremaining: 57s\n",
      "74:\tlearn: 0.1029900\ttotal: 4.62s\tremaining: 57s\n",
      "75:\tlearn: 0.1017996\ttotal: 4.69s\tremaining: 57s\n",
      "76:\tlearn: 0.1008310\ttotal: 4.77s\tremaining: 57.2s\n",
      "77:\tlearn: 0.0997468\ttotal: 4.84s\tremaining: 57.3s\n",
      "78:\tlearn: 0.0986729\ttotal: 4.92s\tremaining: 57.3s\n",
      "79:\tlearn: 0.0975584\ttotal: 4.98s\tremaining: 57.3s\n",
      "80:\tlearn: 0.0967162\ttotal: 5.05s\tremaining: 57.4s\n",
      "81:\tlearn: 0.0948228\ttotal: 5.13s\tremaining: 57.4s\n",
      "82:\tlearn: 0.0938028\ttotal: 5.21s\tremaining: 57.5s\n",
      "83:\tlearn: 0.0920285\ttotal: 5.23s\tremaining: 57s\n",
      "84:\tlearn: 0.0910561\ttotal: 5.25s\tremaining: 56.5s\n",
      "85:\tlearn: 0.0900766\ttotal: 5.27s\tremaining: 56s\n",
      "86:\tlearn: 0.0892040\ttotal: 5.29s\tremaining: 55.5s\n",
      "87:\tlearn: 0.0867232\ttotal: 5.3s\tremaining: 55s\n",
      "88:\tlearn: 0.0858856\ttotal: 5.32s\tremaining: 54.5s\n",
      "89:\tlearn: 0.0850397\ttotal: 5.34s\tremaining: 54s\n",
      "90:\tlearn: 0.0841847\ttotal: 5.36s\tremaining: 53.6s\n",
      "91:\tlearn: 0.0833780\ttotal: 5.38s\tremaining: 53.1s\n",
      "92:\tlearn: 0.0826008\ttotal: 5.4s\tremaining: 52.7s\n",
      "93:\tlearn: 0.0816155\ttotal: 5.42s\tremaining: 52.2s\n",
      "94:\tlearn: 0.0809256\ttotal: 5.44s\tremaining: 51.8s\n",
      "95:\tlearn: 0.0801814\ttotal: 5.45s\tremaining: 51.4s\n",
      "96:\tlearn: 0.0794091\ttotal: 5.47s\tremaining: 50.9s\n",
      "97:\tlearn: 0.0785927\ttotal: 5.49s\tremaining: 50.5s\n",
      "98:\tlearn: 0.0778887\ttotal: 5.5s\tremaining: 50.1s\n",
      "99:\tlearn: 0.0771732\ttotal: 5.53s\tremaining: 49.7s\n",
      "100:\tlearn: 0.0764790\ttotal: 5.54s\tremaining: 49.3s\n",
      "101:\tlearn: 0.0757521\ttotal: 5.56s\tremaining: 49s\n",
      "102:\tlearn: 0.0749915\ttotal: 5.58s\tremaining: 48.6s\n",
      "103:\tlearn: 0.0743607\ttotal: 5.59s\tremaining: 48.2s\n",
      "104:\tlearn: 0.0736418\ttotal: 5.61s\tremaining: 47.8s\n",
      "105:\tlearn: 0.0729546\ttotal: 5.63s\tremaining: 47.5s\n",
      "106:\tlearn: 0.0715372\ttotal: 5.64s\tremaining: 47.1s\n",
      "107:\tlearn: 0.0708424\ttotal: 5.66s\tremaining: 46.8s\n",
      "108:\tlearn: 0.0702148\ttotal: 5.68s\tremaining: 46.4s\n",
      "109:\tlearn: 0.0695811\ttotal: 5.7s\tremaining: 46.1s\n",
      "110:\tlearn: 0.0690051\ttotal: 5.71s\tremaining: 45.8s\n",
      "111:\tlearn: 0.0682832\ttotal: 5.73s\tremaining: 45.4s\n",
      "112:\tlearn: 0.0676994\ttotal: 5.75s\tremaining: 45.1s\n",
      "113:\tlearn: 0.0671501\ttotal: 5.76s\tremaining: 44.8s\n",
      "114:\tlearn: 0.0665567\ttotal: 5.78s\tremaining: 44.5s\n",
      "115:\tlearn: 0.0660251\ttotal: 5.8s\tremaining: 44.2s\n",
      "116:\tlearn: 0.0654135\ttotal: 5.81s\tremaining: 43.9s\n",
      "117:\tlearn: 0.0645482\ttotal: 5.83s\tremaining: 43.6s\n",
      "118:\tlearn: 0.0639170\ttotal: 5.85s\tremaining: 43.3s\n",
      "119:\tlearn: 0.0634466\ttotal: 5.86s\tremaining: 43s\n",
      "120:\tlearn: 0.0629250\ttotal: 5.88s\tremaining: 42.7s\n",
      "121:\tlearn: 0.0624109\ttotal: 5.89s\tremaining: 42.4s\n",
      "122:\tlearn: 0.0618231\ttotal: 5.91s\tremaining: 42.1s\n",
      "123:\tlearn: 0.0613289\ttotal: 5.93s\tremaining: 41.9s\n",
      "124:\tlearn: 0.0608869\ttotal: 5.94s\tremaining: 41.6s\n",
      "125:\tlearn: 0.0603503\ttotal: 5.96s\tremaining: 41.3s\n",
      "126:\tlearn: 0.0598961\ttotal: 5.98s\tremaining: 41.1s\n",
      "127:\tlearn: 0.0594993\ttotal: 5.99s\tremaining: 40.8s\n",
      "128:\tlearn: 0.0589682\ttotal: 6.01s\tremaining: 40.6s\n",
      "129:\tlearn: 0.0582703\ttotal: 6.03s\tremaining: 40.4s\n",
      "130:\tlearn: 0.0578505\ttotal: 6.05s\tremaining: 40.1s\n",
      "131:\tlearn: 0.0573845\ttotal: 6.06s\tremaining: 39.9s\n",
      "132:\tlearn: 0.0563869\ttotal: 6.08s\tremaining: 39.6s\n",
      "133:\tlearn: 0.0558991\ttotal: 6.09s\tremaining: 39.4s\n",
      "134:\tlearn: 0.0551022\ttotal: 6.11s\tremaining: 39.2s\n",
      "135:\tlearn: 0.0546783\ttotal: 6.13s\tremaining: 38.9s\n",
      "136:\tlearn: 0.0542922\ttotal: 6.14s\tremaining: 38.7s\n",
      "137:\tlearn: 0.0538497\ttotal: 6.16s\tremaining: 38.5s\n",
      "138:\tlearn: 0.0534731\ttotal: 6.18s\tremaining: 38.3s\n",
      "139:\tlearn: 0.0530414\ttotal: 6.19s\tremaining: 38.1s\n",
      "140:\tlearn: 0.0526767\ttotal: 6.21s\tremaining: 37.9s\n",
      "141:\tlearn: 0.0522883\ttotal: 6.23s\tremaining: 37.7s\n",
      "142:\tlearn: 0.0517583\ttotal: 6.25s\tremaining: 37.5s\n",
      "143:\tlearn: 0.0514357\ttotal: 6.26s\tremaining: 37.2s\n",
      "144:\tlearn: 0.0510472\ttotal: 6.28s\tremaining: 37s\n",
      "145:\tlearn: 0.0502952\ttotal: 6.29s\tremaining: 36.8s\n",
      "146:\tlearn: 0.0496497\ttotal: 6.31s\tremaining: 36.6s\n",
      "147:\tlearn: 0.0493238\ttotal: 6.33s\tremaining: 36.4s\n",
      "148:\tlearn: 0.0489561\ttotal: 6.34s\tremaining: 36.2s\n",
      "149:\tlearn: 0.0485884\ttotal: 6.36s\tremaining: 36s\n",
      "150:\tlearn: 0.0481971\ttotal: 6.38s\tremaining: 35.9s\n",
      "151:\tlearn: 0.0478631\ttotal: 6.39s\tremaining: 35.7s\n",
      "152:\tlearn: 0.0475679\ttotal: 6.41s\tremaining: 35.5s\n",
      "153:\tlearn: 0.0468580\ttotal: 6.43s\tremaining: 35.3s\n",
      "154:\tlearn: 0.0465725\ttotal: 6.45s\tremaining: 35.1s\n",
      "155:\tlearn: 0.0461528\ttotal: 6.46s\tremaining: 35s\n",
      "156:\tlearn: 0.0458651\ttotal: 6.48s\tremaining: 34.8s\n",
      "157:\tlearn: 0.0455981\ttotal: 6.5s\tremaining: 34.6s\n",
      "158:\tlearn: 0.0453029\ttotal: 6.51s\tremaining: 34.5s\n",
      "159:\tlearn: 0.0449472\ttotal: 6.53s\tremaining: 34.3s\n",
      "160:\tlearn: 0.0442100\ttotal: 6.55s\tremaining: 34.1s\n",
      "161:\tlearn: 0.0435588\ttotal: 6.57s\tremaining: 34s\n",
      "162:\tlearn: 0.0431344\ttotal: 6.58s\tremaining: 33.8s\n",
      "163:\tlearn: 0.0427984\ttotal: 6.6s\tremaining: 33.6s\n",
      "164:\tlearn: 0.0425702\ttotal: 6.62s\tremaining: 33.5s\n",
      "165:\tlearn: 0.0422865\ttotal: 6.63s\tremaining: 33.3s\n",
      "166:\tlearn: 0.0420276\ttotal: 6.65s\tremaining: 33.2s\n",
      "167:\tlearn: 0.0417720\ttotal: 6.67s\tremaining: 33s\n",
      "168:\tlearn: 0.0415057\ttotal: 6.68s\tremaining: 32.9s\n",
      "169:\tlearn: 0.0412345\ttotal: 6.7s\tremaining: 32.7s\n",
      "170:\tlearn: 0.0410153\ttotal: 6.71s\tremaining: 32.6s\n",
      "171:\tlearn: 0.0407863\ttotal: 6.73s\tremaining: 32.4s\n",
      "172:\tlearn: 0.0405560\ttotal: 6.75s\tremaining: 32.3s\n",
      "173:\tlearn: 0.0403594\ttotal: 6.76s\tremaining: 32.1s\n",
      "174:\tlearn: 0.0401684\ttotal: 6.78s\tremaining: 32s\n",
      "175:\tlearn: 0.0399686\ttotal: 6.8s\tremaining: 31.8s\n",
      "176:\tlearn: 0.0397298\ttotal: 6.82s\tremaining: 31.7s\n",
      "177:\tlearn: 0.0394183\ttotal: 6.84s\tremaining: 31.6s\n",
      "178:\tlearn: 0.0391443\ttotal: 6.85s\tremaining: 31.4s\n",
      "179:\tlearn: 0.0389678\ttotal: 6.87s\tremaining: 31.3s\n",
      "180:\tlearn: 0.0387510\ttotal: 6.88s\tremaining: 31.1s\n",
      "181:\tlearn: 0.0384897\ttotal: 6.9s\tremaining: 31s\n",
      "182:\tlearn: 0.0383296\ttotal: 6.91s\tremaining: 30.9s\n",
      "183:\tlearn: 0.0381326\ttotal: 6.92s\tremaining: 30.7s\n",
      "184:\tlearn: 0.0378960\ttotal: 6.94s\tremaining: 30.6s\n",
      "185:\tlearn: 0.0377035\ttotal: 6.96s\tremaining: 30.5s\n",
      "186:\tlearn: 0.0375008\ttotal: 6.97s\tremaining: 30.3s\n",
      "187:\tlearn: 0.0371519\ttotal: 6.99s\tremaining: 30.2s\n",
      "188:\tlearn: 0.0369122\ttotal: 7.01s\tremaining: 30.1s\n",
      "189:\tlearn: 0.0367026\ttotal: 7.02s\tremaining: 29.9s\n",
      "190:\tlearn: 0.0364696\ttotal: 7.04s\tremaining: 29.8s\n",
      "191:\tlearn: 0.0362856\ttotal: 7.05s\tremaining: 29.7s\n",
      "192:\tlearn: 0.0360933\ttotal: 7.07s\tremaining: 29.6s\n",
      "193:\tlearn: 0.0359046\ttotal: 7.08s\tremaining: 29.4s\n",
      "194:\tlearn: 0.0354062\ttotal: 7.1s\tremaining: 29.3s\n",
      "195:\tlearn: 0.0352431\ttotal: 7.12s\tremaining: 29.2s\n",
      "196:\tlearn: 0.0347600\ttotal: 7.13s\tremaining: 29.1s\n",
      "197:\tlearn: 0.0345546\ttotal: 7.15s\tremaining: 28.9s\n",
      "198:\tlearn: 0.0342785\ttotal: 7.16s\tremaining: 28.8s\n",
      "199:\tlearn: 0.0339973\ttotal: 7.17s\tremaining: 28.7s\n",
      "200:\tlearn: 0.0338331\ttotal: 7.19s\tremaining: 28.6s\n",
      "201:\tlearn: 0.0336520\ttotal: 7.21s\tremaining: 28.5s\n",
      "202:\tlearn: 0.0335090\ttotal: 7.23s\tremaining: 28.4s\n",
      "203:\tlearn: 0.0333299\ttotal: 7.24s\tremaining: 28.3s\n",
      "204:\tlearn: 0.0331354\ttotal: 7.26s\tremaining: 28.2s\n",
      "205:\tlearn: 0.0329275\ttotal: 7.27s\tremaining: 28s\n",
      "206:\tlearn: 0.0327298\ttotal: 7.29s\tremaining: 27.9s\n",
      "207:\tlearn: 0.0325284\ttotal: 7.3s\tremaining: 27.8s\n",
      "208:\tlearn: 0.0323195\ttotal: 7.32s\tremaining: 27.7s\n",
      "209:\tlearn: 0.0320256\ttotal: 7.33s\tremaining: 27.6s\n",
      "210:\tlearn: 0.0318713\ttotal: 7.35s\tremaining: 27.5s\n",
      "211:\tlearn: 0.0316815\ttotal: 7.37s\tremaining: 27.4s\n",
      "212:\tlearn: 0.0314311\ttotal: 7.38s\tremaining: 27.3s\n",
      "213:\tlearn: 0.0311789\ttotal: 7.4s\tremaining: 27.2s\n",
      "214:\tlearn: 0.0310143\ttotal: 7.41s\tremaining: 27.1s\n",
      "215:\tlearn: 0.0307711\ttotal: 7.43s\tremaining: 27s\n",
      "216:\tlearn: 0.0305829\ttotal: 7.46s\tremaining: 26.9s\n",
      "217:\tlearn: 0.0304105\ttotal: 7.48s\tremaining: 26.8s\n",
      "218:\tlearn: 0.0302341\ttotal: 7.5s\tremaining: 26.8s\n",
      "219:\tlearn: 0.0300977\ttotal: 7.52s\tremaining: 26.7s\n",
      "220:\tlearn: 0.0299909\ttotal: 7.54s\tremaining: 26.6s\n",
      "221:\tlearn: 0.0298567\ttotal: 7.56s\tremaining: 26.5s\n",
      "222:\tlearn: 0.0297033\ttotal: 7.58s\tremaining: 26.4s\n",
      "223:\tlearn: 0.0294906\ttotal: 7.6s\tremaining: 26.3s\n",
      "224:\tlearn: 0.0293846\ttotal: 7.62s\tremaining: 26.3s\n",
      "225:\tlearn: 0.0292403\ttotal: 7.64s\tremaining: 26.2s\n",
      "226:\tlearn: 0.0291307\ttotal: 7.67s\tremaining: 26.1s\n",
      "227:\tlearn: 0.0289767\ttotal: 7.68s\tremaining: 26s\n",
      "228:\tlearn: 0.0288357\ttotal: 7.7s\tremaining: 25.9s\n",
      "229:\tlearn: 0.0287066\ttotal: 7.72s\tremaining: 25.8s\n",
      "230:\tlearn: 0.0285799\ttotal: 7.74s\tremaining: 25.8s\n",
      "231:\tlearn: 0.0284380\ttotal: 7.76s\tremaining: 25.7s\n",
      "232:\tlearn: 0.0282915\ttotal: 7.78s\tremaining: 25.6s\n",
      "233:\tlearn: 0.0281695\ttotal: 7.8s\tremaining: 25.5s\n",
      "234:\tlearn: 0.0279227\ttotal: 7.82s\tremaining: 25.5s\n",
      "235:\tlearn: 0.0278000\ttotal: 7.84s\tremaining: 25.4s\n",
      "236:\tlearn: 0.0276717\ttotal: 7.87s\tremaining: 25.3s\n",
      "237:\tlearn: 0.0274791\ttotal: 7.88s\tremaining: 25.2s\n",
      "238:\tlearn: 0.0273643\ttotal: 7.9s\tremaining: 25.2s\n",
      "239:\tlearn: 0.0272286\ttotal: 7.92s\tremaining: 25.1s\n",
      "240:\tlearn: 0.0270970\ttotal: 7.94s\tremaining: 25s\n",
      "241:\tlearn: 0.0268882\ttotal: 7.96s\tremaining: 24.9s\n",
      "242:\tlearn: 0.0267522\ttotal: 7.98s\tremaining: 24.9s\n",
      "243:\tlearn: 0.0266274\ttotal: 8s\tremaining: 24.8s\n",
      "244:\tlearn: 0.0264972\ttotal: 8.02s\tremaining: 24.7s\n",
      "245:\tlearn: 0.0263994\ttotal: 8.04s\tremaining: 24.7s\n",
      "246:\tlearn: 0.0262267\ttotal: 8.07s\tremaining: 24.6s\n",
      "247:\tlearn: 0.0261581\ttotal: 8.09s\tremaining: 24.5s\n",
      "248:\tlearn: 0.0260545\ttotal: 8.12s\tremaining: 24.5s\n",
      "249:\tlearn: 0.0259215\ttotal: 8.16s\tremaining: 24.5s\n",
      "250:\tlearn: 0.0257921\ttotal: 8.2s\tremaining: 24.5s\n",
      "251:\tlearn: 0.0256506\ttotal: 8.22s\tremaining: 24.4s\n",
      "252:\tlearn: 0.0255232\ttotal: 8.24s\tremaining: 24.3s\n",
      "253:\tlearn: 0.0253790\ttotal: 8.26s\tremaining: 24.3s\n",
      "254:\tlearn: 0.0252559\ttotal: 8.28s\tremaining: 24.2s\n",
      "255:\tlearn: 0.0251471\ttotal: 8.3s\tremaining: 24.1s\n",
      "256:\tlearn: 0.0250452\ttotal: 8.32s\tremaining: 24.1s\n",
      "257:\tlearn: 0.0249448\ttotal: 8.34s\tremaining: 24s\n",
      "258:\tlearn: 0.0246582\ttotal: 8.36s\tremaining: 23.9s\n",
      "259:\tlearn: 0.0245131\ttotal: 8.38s\tremaining: 23.8s\n",
      "260:\tlearn: 0.0244122\ttotal: 8.4s\tremaining: 23.8s\n",
      "261:\tlearn: 0.0243067\ttotal: 8.42s\tremaining: 23.7s\n",
      "262:\tlearn: 0.0241803\ttotal: 8.44s\tremaining: 23.6s\n",
      "263:\tlearn: 0.0240585\ttotal: 8.46s\tremaining: 23.6s\n",
      "264:\tlearn: 0.0239486\ttotal: 8.49s\tremaining: 23.5s\n",
      "265:\tlearn: 0.0238351\ttotal: 8.5s\tremaining: 23.5s\n",
      "266:\tlearn: 0.0235862\ttotal: 8.53s\tremaining: 23.4s\n",
      "267:\tlearn: 0.0234711\ttotal: 8.54s\tremaining: 23.3s\n",
      "268:\tlearn: 0.0231933\ttotal: 8.56s\tremaining: 23.3s\n",
      "269:\tlearn: 0.0231045\ttotal: 8.58s\tremaining: 23.2s\n",
      "270:\tlearn: 0.0229918\ttotal: 8.6s\tremaining: 23.1s\n",
      "271:\tlearn: 0.0229002\ttotal: 8.62s\tremaining: 23.1s\n",
      "272:\tlearn: 0.0227959\ttotal: 8.64s\tremaining: 23s\n",
      "273:\tlearn: 0.0225761\ttotal: 8.65s\tremaining: 22.9s\n",
      "274:\tlearn: 0.0224681\ttotal: 8.67s\tremaining: 22.9s\n",
      "275:\tlearn: 0.0223764\ttotal: 8.69s\tremaining: 22.8s\n",
      "276:\tlearn: 0.0222788\ttotal: 8.71s\tremaining: 22.7s\n",
      "277:\tlearn: 0.0222015\ttotal: 8.72s\tremaining: 22.7s\n",
      "278:\tlearn: 0.0220944\ttotal: 8.73s\tremaining: 22.6s\n",
      "279:\tlearn: 0.0219928\ttotal: 8.75s\tremaining: 22.5s\n",
      "280:\tlearn: 0.0218893\ttotal: 8.77s\tremaining: 22.4s\n",
      "281:\tlearn: 0.0217953\ttotal: 8.78s\tremaining: 22.4s\n",
      "282:\tlearn: 0.0215468\ttotal: 8.8s\tremaining: 22.3s\n",
      "283:\tlearn: 0.0214214\ttotal: 8.81s\tremaining: 22.2s\n",
      "284:\tlearn: 0.0212237\ttotal: 8.83s\tremaining: 22.2s\n",
      "285:\tlearn: 0.0211603\ttotal: 8.84s\tremaining: 22.1s\n",
      "286:\tlearn: 0.0210603\ttotal: 8.86s\tremaining: 22s\n",
      "287:\tlearn: 0.0209559\ttotal: 8.87s\tremaining: 21.9s\n",
      "288:\tlearn: 0.0208478\ttotal: 8.89s\tremaining: 21.9s\n",
      "289:\tlearn: 0.0207469\ttotal: 8.91s\tremaining: 21.8s\n",
      "290:\tlearn: 0.0206184\ttotal: 8.92s\tremaining: 21.7s\n",
      "291:\tlearn: 0.0204971\ttotal: 8.93s\tremaining: 21.7s\n",
      "292:\tlearn: 0.0203027\ttotal: 8.94s\tremaining: 21.6s\n",
      "293:\tlearn: 0.0202002\ttotal: 8.96s\tremaining: 21.5s\n",
      "294:\tlearn: 0.0201131\ttotal: 8.98s\tremaining: 21.5s\n",
      "295:\tlearn: 0.0199975\ttotal: 8.99s\tremaining: 21.4s\n",
      "296:\tlearn: 0.0197958\ttotal: 9.01s\tremaining: 21.3s\n",
      "297:\tlearn: 0.0197099\ttotal: 9.02s\tremaining: 21.3s\n",
      "298:\tlearn: 0.0196109\ttotal: 9.04s\tremaining: 21.2s\n",
      "299:\tlearn: 0.0195102\ttotal: 9.05s\tremaining: 21.1s\n",
      "300:\tlearn: 0.0194071\ttotal: 9.07s\tremaining: 21.1s\n",
      "301:\tlearn: 0.0193291\ttotal: 9.09s\tremaining: 21s\n",
      "302:\tlearn: 0.0192413\ttotal: 9.1s\tremaining: 20.9s\n",
      "303:\tlearn: 0.0191447\ttotal: 9.12s\tremaining: 20.9s\n",
      "304:\tlearn: 0.0189737\ttotal: 9.13s\tremaining: 20.8s\n",
      "305:\tlearn: 0.0189133\ttotal: 9.14s\tremaining: 20.7s\n",
      "306:\tlearn: 0.0188217\ttotal: 9.16s\tremaining: 20.7s\n",
      "307:\tlearn: 0.0186729\ttotal: 9.17s\tremaining: 20.6s\n",
      "308:\tlearn: 0.0185907\ttotal: 9.19s\tremaining: 20.5s\n",
      "309:\tlearn: 0.0185063\ttotal: 9.2s\tremaining: 20.5s\n",
      "310:\tlearn: 0.0184239\ttotal: 9.21s\tremaining: 20.4s\n",
      "311:\tlearn: 0.0182597\ttotal: 9.23s\tremaining: 20.4s\n",
      "312:\tlearn: 0.0181900\ttotal: 9.24s\tremaining: 20.3s\n",
      "313:\tlearn: 0.0180993\ttotal: 9.26s\tremaining: 20.2s\n",
      "314:\tlearn: 0.0180142\ttotal: 9.28s\tremaining: 20.2s\n",
      "315:\tlearn: 0.0179108\ttotal: 9.29s\tremaining: 20.1s\n",
      "316:\tlearn: 0.0178609\ttotal: 9.31s\tremaining: 20.1s\n",
      "317:\tlearn: 0.0177765\ttotal: 9.32s\tremaining: 20s\n",
      "318:\tlearn: 0.0177045\ttotal: 9.33s\tremaining: 19.9s\n",
      "319:\tlearn: 0.0176303\ttotal: 9.35s\tremaining: 19.9s\n",
      "320:\tlearn: 0.0175663\ttotal: 9.37s\tremaining: 19.8s\n",
      "321:\tlearn: 0.0175221\ttotal: 9.38s\tremaining: 19.7s\n",
      "322:\tlearn: 0.0174463\ttotal: 9.39s\tremaining: 19.7s\n",
      "323:\tlearn: 0.0173667\ttotal: 9.41s\tremaining: 19.6s\n",
      "324:\tlearn: 0.0172984\ttotal: 9.42s\tremaining: 19.6s\n",
      "325:\tlearn: 0.0172473\ttotal: 9.43s\tremaining: 19.5s\n",
      "326:\tlearn: 0.0171529\ttotal: 9.45s\tremaining: 19.4s\n",
      "327:\tlearn: 0.0170603\ttotal: 9.47s\tremaining: 19.4s\n",
      "328:\tlearn: 0.0169688\ttotal: 9.48s\tremaining: 19.3s\n",
      "329:\tlearn: 0.0169386\ttotal: 9.5s\tremaining: 19.3s\n",
      "330:\tlearn: 0.0168249\ttotal: 9.51s\tremaining: 19.2s\n",
      "331:\tlearn: 0.0167242\ttotal: 9.53s\tremaining: 19.2s\n",
      "332:\tlearn: 0.0166697\ttotal: 9.54s\tremaining: 19.1s\n",
      "333:\tlearn: 0.0165916\ttotal: 9.56s\tremaining: 19.1s\n",
      "334:\tlearn: 0.0164770\ttotal: 9.57s\tremaining: 19s\n",
      "335:\tlearn: 0.0164060\ttotal: 9.58s\tremaining: 18.9s\n",
      "336:\tlearn: 0.0163189\ttotal: 9.6s\tremaining: 18.9s\n",
      "337:\tlearn: 0.0162484\ttotal: 9.62s\tremaining: 18.8s\n",
      "338:\tlearn: 0.0161682\ttotal: 9.63s\tremaining: 18.8s\n",
      "339:\tlearn: 0.0160900\ttotal: 9.64s\tremaining: 18.7s\n",
      "340:\tlearn: 0.0159105\ttotal: 9.66s\tremaining: 18.7s\n",
      "341:\tlearn: 0.0158382\ttotal: 9.67s\tremaining: 18.6s\n",
      "342:\tlearn: 0.0157541\ttotal: 9.69s\tremaining: 18.6s\n",
      "343:\tlearn: 0.0156813\ttotal: 9.71s\tremaining: 18.5s\n",
      "344:\tlearn: 0.0156078\ttotal: 9.72s\tremaining: 18.5s\n",
      "345:\tlearn: 0.0155496\ttotal: 9.74s\tremaining: 18.4s\n",
      "346:\tlearn: 0.0154863\ttotal: 9.75s\tremaining: 18.3s\n",
      "347:\tlearn: 0.0154361\ttotal: 9.76s\tremaining: 18.3s\n",
      "348:\tlearn: 0.0153588\ttotal: 9.77s\tremaining: 18.2s\n",
      "349:\tlearn: 0.0152913\ttotal: 9.79s\tremaining: 18.2s\n",
      "350:\tlearn: 0.0152214\ttotal: 9.81s\tremaining: 18.1s\n",
      "351:\tlearn: 0.0151274\ttotal: 9.82s\tremaining: 18.1s\n",
      "352:\tlearn: 0.0150441\ttotal: 9.83s\tremaining: 18s\n",
      "353:\tlearn: 0.0149592\ttotal: 9.85s\tremaining: 18s\n",
      "354:\tlearn: 0.0148935\ttotal: 9.86s\tremaining: 17.9s\n",
      "355:\tlearn: 0.0148001\ttotal: 9.88s\tremaining: 17.9s\n",
      "356:\tlearn: 0.0147128\ttotal: 9.89s\tremaining: 17.8s\n",
      "357:\tlearn: 0.0146653\ttotal: 9.91s\tremaining: 17.8s\n",
      "358:\tlearn: 0.0146054\ttotal: 9.92s\tremaining: 17.7s\n",
      "359:\tlearn: 0.0145450\ttotal: 9.94s\tremaining: 17.7s\n",
      "360:\tlearn: 0.0144878\ttotal: 9.96s\tremaining: 17.6s\n",
      "361:\tlearn: 0.0144238\ttotal: 9.97s\tremaining: 17.6s\n",
      "362:\tlearn: 0.0142588\ttotal: 9.99s\tremaining: 17.5s\n",
      "363:\tlearn: 0.0141880\ttotal: 10s\tremaining: 17.5s\n",
      "364:\tlearn: 0.0141356\ttotal: 10s\tremaining: 17.4s\n",
      "365:\tlearn: 0.0140624\ttotal: 10s\tremaining: 17.4s\n",
      "366:\tlearn: 0.0139904\ttotal: 10s\tremaining: 17.3s\n",
      "367:\tlearn: 0.0139545\ttotal: 10.1s\tremaining: 17.3s\n",
      "368:\tlearn: 0.0139024\ttotal: 10.1s\tremaining: 17.2s\n",
      "369:\tlearn: 0.0138343\ttotal: 10.1s\tremaining: 17.2s\n",
      "370:\tlearn: 0.0137555\ttotal: 10.1s\tremaining: 17.1s\n",
      "371:\tlearn: 0.0136955\ttotal: 10.1s\tremaining: 17.1s\n",
      "372:\tlearn: 0.0136210\ttotal: 10.1s\tremaining: 17s\n",
      "373:\tlearn: 0.0135796\ttotal: 10.1s\tremaining: 17s\n",
      "374:\tlearn: 0.0134880\ttotal: 10.2s\tremaining: 16.9s\n",
      "375:\tlearn: 0.0134416\ttotal: 10.2s\tremaining: 16.9s\n",
      "376:\tlearn: 0.0133673\ttotal: 10.2s\tremaining: 16.8s\n",
      "377:\tlearn: 0.0133163\ttotal: 10.2s\tremaining: 16.8s\n",
      "378:\tlearn: 0.0132410\ttotal: 10.2s\tremaining: 16.7s\n",
      "379:\tlearn: 0.0131194\ttotal: 10.2s\tremaining: 16.7s\n",
      "380:\tlearn: 0.0129921\ttotal: 10.2s\tremaining: 16.7s\n",
      "381:\tlearn: 0.0129342\ttotal: 10.3s\tremaining: 16.6s\n",
      "382:\tlearn: 0.0128836\ttotal: 10.3s\tremaining: 16.6s\n",
      "383:\tlearn: 0.0128372\ttotal: 10.3s\tremaining: 16.5s\n",
      "384:\tlearn: 0.0127717\ttotal: 10.3s\tremaining: 16.5s\n",
      "385:\tlearn: 0.0127079\ttotal: 10.3s\tremaining: 16.4s\n",
      "386:\tlearn: 0.0126295\ttotal: 10.3s\tremaining: 16.4s\n",
      "387:\tlearn: 0.0125795\ttotal: 10.4s\tremaining: 16.3s\n",
      "388:\tlearn: 0.0125183\ttotal: 10.4s\tremaining: 16.3s\n",
      "389:\tlearn: 0.0124575\ttotal: 10.4s\tremaining: 16.2s\n",
      "390:\tlearn: 0.0123999\ttotal: 10.4s\tremaining: 16.2s\n",
      "391:\tlearn: 0.0123666\ttotal: 10.4s\tremaining: 16.1s\n",
      "392:\tlearn: 0.0122988\ttotal: 10.4s\tremaining: 16.1s\n",
      "393:\tlearn: 0.0122148\ttotal: 10.4s\tremaining: 16.1s\n",
      "394:\tlearn: 0.0121670\ttotal: 10.5s\tremaining: 16s\n",
      "395:\tlearn: 0.0121190\ttotal: 10.5s\tremaining: 16s\n",
      "396:\tlearn: 0.0120757\ttotal: 10.5s\tremaining: 15.9s\n",
      "397:\tlearn: 0.0119995\ttotal: 10.5s\tremaining: 15.9s\n",
      "398:\tlearn: 0.0119434\ttotal: 10.5s\tremaining: 15.8s\n",
      "399:\tlearn: 0.0118183\ttotal: 10.5s\tremaining: 15.8s\n",
      "400:\tlearn: 0.0117386\ttotal: 10.5s\tremaining: 15.8s\n",
      "401:\tlearn: 0.0116960\ttotal: 10.6s\tremaining: 15.7s\n",
      "402:\tlearn: 0.0116285\ttotal: 10.6s\tremaining: 15.7s\n",
      "403:\tlearn: 0.0115824\ttotal: 10.6s\tremaining: 15.6s\n",
      "404:\tlearn: 0.0115452\ttotal: 10.6s\tremaining: 15.6s\n",
      "405:\tlearn: 0.0115102\ttotal: 10.6s\tremaining: 15.5s\n",
      "406:\tlearn: 0.0114648\ttotal: 10.6s\tremaining: 15.5s\n",
      "407:\tlearn: 0.0113500\ttotal: 10.7s\tremaining: 15.5s\n",
      "408:\tlearn: 0.0113031\ttotal: 10.7s\tremaining: 15.4s\n",
      "409:\tlearn: 0.0112520\ttotal: 10.7s\tremaining: 15.4s\n",
      "410:\tlearn: 0.0112033\ttotal: 10.7s\tremaining: 15.3s\n",
      "411:\tlearn: 0.0111597\ttotal: 10.7s\tremaining: 15.3s\n",
      "412:\tlearn: 0.0111299\ttotal: 10.7s\tremaining: 15.2s\n",
      "413:\tlearn: 0.0110887\ttotal: 10.7s\tremaining: 15.2s\n",
      "414:\tlearn: 0.0110549\ttotal: 10.8s\tremaining: 15.2s\n",
      "415:\tlearn: 0.0109725\ttotal: 10.8s\tremaining: 15.1s\n",
      "416:\tlearn: 0.0109141\ttotal: 10.8s\tremaining: 15.1s\n",
      "417:\tlearn: 0.0108765\ttotal: 10.8s\tremaining: 15s\n",
      "418:\tlearn: 0.0108451\ttotal: 10.8s\tremaining: 15s\n",
      "419:\tlearn: 0.0107972\ttotal: 10.8s\tremaining: 15s\n",
      "420:\tlearn: 0.0107340\ttotal: 10.8s\tremaining: 14.9s\n",
      "421:\tlearn: 0.0106442\ttotal: 10.9s\tremaining: 14.9s\n",
      "422:\tlearn: 0.0106127\ttotal: 10.9s\tremaining: 14.8s\n",
      "423:\tlearn: 0.0105717\ttotal: 10.9s\tremaining: 14.8s\n",
      "424:\tlearn: 0.0105216\ttotal: 10.9s\tremaining: 14.7s\n",
      "425:\tlearn: 0.0104762\ttotal: 10.9s\tremaining: 14.7s\n",
      "426:\tlearn: 0.0104464\ttotal: 10.9s\tremaining: 14.7s\n",
      "427:\tlearn: 0.0104088\ttotal: 10.9s\tremaining: 14.6s\n",
      "428:\tlearn: 0.0103662\ttotal: 11s\tremaining: 14.6s\n",
      "429:\tlearn: 0.0103229\ttotal: 11s\tremaining: 14.5s\n",
      "430:\tlearn: 0.0102742\ttotal: 11s\tremaining: 14.5s\n",
      "431:\tlearn: 0.0102347\ttotal: 11s\tremaining: 14.5s\n",
      "432:\tlearn: 0.0102062\ttotal: 11s\tremaining: 14.4s\n",
      "433:\tlearn: 0.0101711\ttotal: 11s\tremaining: 14.4s\n",
      "434:\tlearn: 0.0100680\ttotal: 11s\tremaining: 14.3s\n",
      "435:\tlearn: 0.0100266\ttotal: 11.1s\tremaining: 14.3s\n",
      "436:\tlearn: 0.0099904\ttotal: 11.1s\tremaining: 14.3s\n",
      "437:\tlearn: 0.0099524\ttotal: 11.1s\tremaining: 14.2s\n",
      "438:\tlearn: 0.0099097\ttotal: 11.1s\tremaining: 14.2s\n",
      "439:\tlearn: 0.0098718\ttotal: 11.1s\tremaining: 14.1s\n",
      "440:\tlearn: 0.0098387\ttotal: 11.1s\tremaining: 14.1s\n",
      "441:\tlearn: 0.0098047\ttotal: 11.1s\tremaining: 14.1s\n",
      "442:\tlearn: 0.0097803\ttotal: 11.2s\tremaining: 14s\n",
      "443:\tlearn: 0.0097502\ttotal: 11.2s\tremaining: 14s\n",
      "444:\tlearn: 0.0096741\ttotal: 11.2s\tremaining: 13.9s\n",
      "445:\tlearn: 0.0096340\ttotal: 11.2s\tremaining: 13.9s\n",
      "446:\tlearn: 0.0096030\ttotal: 11.2s\tremaining: 13.9s\n",
      "447:\tlearn: 0.0095697\ttotal: 11.2s\tremaining: 13.8s\n",
      "448:\tlearn: 0.0095192\ttotal: 11.2s\tremaining: 13.8s\n",
      "449:\tlearn: 0.0094358\ttotal: 11.3s\tremaining: 13.8s\n",
      "450:\tlearn: 0.0094065\ttotal: 11.3s\tremaining: 13.7s\n",
      "451:\tlearn: 0.0093353\ttotal: 11.3s\tremaining: 13.7s\n",
      "452:\tlearn: 0.0092922\ttotal: 11.3s\tremaining: 13.6s\n",
      "453:\tlearn: 0.0092263\ttotal: 11.3s\tremaining: 13.6s\n",
      "454:\tlearn: 0.0091426\ttotal: 11.3s\tremaining: 13.6s\n",
      "455:\tlearn: 0.0090859\ttotal: 11.3s\tremaining: 13.5s\n",
      "456:\tlearn: 0.0090472\ttotal: 11.4s\tremaining: 13.5s\n",
      "457:\tlearn: 0.0090020\ttotal: 11.4s\tremaining: 13.5s\n",
      "458:\tlearn: 0.0089572\ttotal: 11.4s\tremaining: 13.4s\n",
      "459:\tlearn: 0.0089320\ttotal: 11.4s\tremaining: 13.4s\n",
      "460:\tlearn: 0.0088594\ttotal: 11.4s\tremaining: 13.3s\n",
      "461:\tlearn: 0.0088416\ttotal: 11.4s\tremaining: 13.3s\n",
      "462:\tlearn: 0.0087920\ttotal: 11.4s\tremaining: 13.3s\n",
      "463:\tlearn: 0.0087761\ttotal: 11.5s\tremaining: 13.2s\n",
      "464:\tlearn: 0.0087414\ttotal: 11.5s\tremaining: 13.2s\n",
      "465:\tlearn: 0.0087032\ttotal: 11.5s\tremaining: 13.2s\n",
      "466:\tlearn: 0.0086572\ttotal: 11.5s\tremaining: 13.1s\n",
      "467:\tlearn: 0.0086165\ttotal: 11.5s\tremaining: 13.1s\n",
      "468:\tlearn: 0.0085998\ttotal: 11.5s\tremaining: 13s\n",
      "469:\tlearn: 0.0085572\ttotal: 11.5s\tremaining: 13s\n",
      "470:\tlearn: 0.0085368\ttotal: 11.5s\tremaining: 13s\n",
      "471:\tlearn: 0.0084995\ttotal: 11.6s\tremaining: 12.9s\n",
      "472:\tlearn: 0.0084661\ttotal: 11.6s\tremaining: 12.9s\n",
      "473:\tlearn: 0.0084298\ttotal: 11.6s\tremaining: 12.9s\n",
      "474:\tlearn: 0.0083760\ttotal: 11.6s\tremaining: 12.8s\n",
      "475:\tlearn: 0.0083102\ttotal: 11.6s\tremaining: 12.8s\n",
      "476:\tlearn: 0.0082445\ttotal: 11.6s\tremaining: 12.8s\n",
      "477:\tlearn: 0.0081929\ttotal: 11.6s\tremaining: 12.7s\n",
      "478:\tlearn: 0.0081598\ttotal: 11.7s\tremaining: 12.7s\n",
      "479:\tlearn: 0.0081142\ttotal: 11.7s\tremaining: 12.6s\n",
      "480:\tlearn: 0.0080886\ttotal: 11.7s\tremaining: 12.6s\n",
      "481:\tlearn: 0.0080477\ttotal: 11.7s\tremaining: 12.6s\n",
      "482:\tlearn: 0.0080278\ttotal: 11.7s\tremaining: 12.5s\n",
      "483:\tlearn: 0.0080034\ttotal: 11.7s\tremaining: 12.5s\n",
      "484:\tlearn: 0.0079641\ttotal: 11.7s\tremaining: 12.5s\n",
      "485:\tlearn: 0.0079175\ttotal: 11.8s\tremaining: 12.4s\n",
      "486:\tlearn: 0.0078813\ttotal: 11.8s\tremaining: 12.4s\n",
      "487:\tlearn: 0.0078543\ttotal: 11.8s\tremaining: 12.4s\n",
      "488:\tlearn: 0.0078234\ttotal: 11.8s\tremaining: 12.3s\n",
      "489:\tlearn: 0.0077762\ttotal: 11.8s\tremaining: 12.3s\n",
      "490:\tlearn: 0.0077454\ttotal: 11.8s\tremaining: 12.3s\n",
      "491:\tlearn: 0.0077098\ttotal: 11.8s\tremaining: 12.2s\n",
      "492:\tlearn: 0.0076824\ttotal: 11.9s\tremaining: 12.2s\n",
      "493:\tlearn: 0.0076612\ttotal: 11.9s\tremaining: 12.2s\n",
      "494:\tlearn: 0.0076359\ttotal: 11.9s\tremaining: 12.1s\n",
      "495:\tlearn: 0.0076081\ttotal: 11.9s\tremaining: 12.1s\n",
      "496:\tlearn: 0.0075787\ttotal: 11.9s\tremaining: 12.1s\n",
      "497:\tlearn: 0.0075367\ttotal: 11.9s\tremaining: 12s\n",
      "498:\tlearn: 0.0074887\ttotal: 11.9s\tremaining: 12s\n",
      "499:\tlearn: 0.0074465\ttotal: 12s\tremaining: 12s\n",
      "500:\tlearn: 0.0074156\ttotal: 12s\tremaining: 11.9s\n",
      "501:\tlearn: 0.0073927\ttotal: 12s\tremaining: 11.9s\n",
      "502:\tlearn: 0.0073759\ttotal: 12s\tremaining: 11.9s\n",
      "503:\tlearn: 0.0073542\ttotal: 12s\tremaining: 11.8s\n",
      "504:\tlearn: 0.0073167\ttotal: 12s\tremaining: 11.8s\n",
      "505:\tlearn: 0.0072624\ttotal: 12s\tremaining: 11.8s\n",
      "506:\tlearn: 0.0072319\ttotal: 12.1s\tremaining: 11.7s\n",
      "507:\tlearn: 0.0071961\ttotal: 12.1s\tremaining: 11.7s\n",
      "508:\tlearn: 0.0071788\ttotal: 12.1s\tremaining: 11.7s\n",
      "509:\tlearn: 0.0071263\ttotal: 12.1s\tremaining: 11.6s\n",
      "510:\tlearn: 0.0070989\ttotal: 12.1s\tremaining: 11.6s\n",
      "511:\tlearn: 0.0070673\ttotal: 12.1s\tremaining: 11.6s\n",
      "512:\tlearn: 0.0070356\ttotal: 12.1s\tremaining: 11.5s\n",
      "513:\tlearn: 0.0070019\ttotal: 12.2s\tremaining: 11.5s\n",
      "514:\tlearn: 0.0069674\ttotal: 12.2s\tremaining: 11.5s\n",
      "515:\tlearn: 0.0069339\ttotal: 12.2s\tremaining: 11.4s\n",
      "516:\tlearn: 0.0069079\ttotal: 12.2s\tremaining: 11.4s\n",
      "517:\tlearn: 0.0068989\ttotal: 12.2s\tremaining: 11.4s\n",
      "518:\tlearn: 0.0068718\ttotal: 12.2s\tremaining: 11.3s\n",
      "519:\tlearn: 0.0068552\ttotal: 12.2s\tremaining: 11.3s\n",
      "520:\tlearn: 0.0068328\ttotal: 12.3s\tremaining: 11.3s\n",
      "521:\tlearn: 0.0067956\ttotal: 12.3s\tremaining: 11.2s\n",
      "522:\tlearn: 0.0067736\ttotal: 12.3s\tremaining: 11.2s\n",
      "523:\tlearn: 0.0067563\ttotal: 12.3s\tremaining: 11.2s\n",
      "524:\tlearn: 0.0067301\ttotal: 12.3s\tremaining: 11.1s\n",
      "525:\tlearn: 0.0066850\ttotal: 12.3s\tremaining: 11.1s\n",
      "526:\tlearn: 0.0066581\ttotal: 12.3s\tremaining: 11.1s\n",
      "527:\tlearn: 0.0066451\ttotal: 12.4s\tremaining: 11s\n",
      "528:\tlearn: 0.0066118\ttotal: 12.4s\tremaining: 11s\n",
      "529:\tlearn: 0.0065750\ttotal: 12.4s\tremaining: 11s\n",
      "530:\tlearn: 0.0065565\ttotal: 12.4s\tremaining: 11s\n",
      "531:\tlearn: 0.0065418\ttotal: 12.4s\tremaining: 10.9s\n",
      "532:\tlearn: 0.0065203\ttotal: 12.4s\tremaining: 10.9s\n",
      "533:\tlearn: 0.0064984\ttotal: 12.4s\tremaining: 10.9s\n",
      "534:\tlearn: 0.0064846\ttotal: 12.5s\tremaining: 10.8s\n",
      "535:\tlearn: 0.0064744\ttotal: 12.5s\tremaining: 10.8s\n",
      "536:\tlearn: 0.0064622\ttotal: 12.5s\tremaining: 10.8s\n",
      "537:\tlearn: 0.0064390\ttotal: 12.5s\tremaining: 10.7s\n",
      "538:\tlearn: 0.0063994\ttotal: 12.5s\tremaining: 10.7s\n",
      "539:\tlearn: 0.0063828\ttotal: 12.5s\tremaining: 10.7s\n",
      "540:\tlearn: 0.0063538\ttotal: 12.5s\tremaining: 10.6s\n",
      "541:\tlearn: 0.0063230\ttotal: 12.5s\tremaining: 10.6s\n",
      "542:\tlearn: 0.0062995\ttotal: 12.6s\tremaining: 10.6s\n",
      "543:\tlearn: 0.0062615\ttotal: 12.6s\tremaining: 10.5s\n",
      "544:\tlearn: 0.0062251\ttotal: 12.6s\tremaining: 10.5s\n",
      "545:\tlearn: 0.0061912\ttotal: 12.6s\tremaining: 10.5s\n",
      "546:\tlearn: 0.0061585\ttotal: 12.6s\tremaining: 10.5s\n",
      "547:\tlearn: 0.0061344\ttotal: 12.6s\tremaining: 10.4s\n",
      "548:\tlearn: 0.0060861\ttotal: 12.6s\tremaining: 10.4s\n",
      "549:\tlearn: 0.0060731\ttotal: 12.7s\tremaining: 10.4s\n",
      "550:\tlearn: 0.0060502\ttotal: 12.7s\tremaining: 10.3s\n",
      "551:\tlearn: 0.0060285\ttotal: 12.7s\tremaining: 10.3s\n",
      "552:\tlearn: 0.0059963\ttotal: 12.7s\tremaining: 10.3s\n",
      "553:\tlearn: 0.0059658\ttotal: 12.7s\tremaining: 10.2s\n",
      "554:\tlearn: 0.0059477\ttotal: 12.7s\tremaining: 10.2s\n",
      "555:\tlearn: 0.0059140\ttotal: 12.7s\tremaining: 10.2s\n",
      "556:\tlearn: 0.0058947\ttotal: 12.8s\tremaining: 10.2s\n",
      "557:\tlearn: 0.0058658\ttotal: 12.8s\tremaining: 10.1s\n",
      "558:\tlearn: 0.0058432\ttotal: 12.8s\tremaining: 10.1s\n",
      "559:\tlearn: 0.0058225\ttotal: 12.8s\tremaining: 10.1s\n",
      "560:\tlearn: 0.0058109\ttotal: 12.8s\tremaining: 10s\n",
      "561:\tlearn: 0.0057871\ttotal: 12.8s\tremaining: 10s\n",
      "562:\tlearn: 0.0057685\ttotal: 12.8s\tremaining: 9.97s\n",
      "563:\tlearn: 0.0057486\ttotal: 12.9s\tremaining: 9.94s\n",
      "564:\tlearn: 0.0057242\ttotal: 12.9s\tremaining: 9.91s\n",
      "565:\tlearn: 0.0056952\ttotal: 12.9s\tremaining: 9.88s\n",
      "566:\tlearn: 0.0056664\ttotal: 12.9s\tremaining: 9.85s\n",
      "567:\tlearn: 0.0056554\ttotal: 12.9s\tremaining: 9.83s\n",
      "568:\tlearn: 0.0056486\ttotal: 12.9s\tremaining: 9.8s\n",
      "569:\tlearn: 0.0056277\ttotal: 12.9s\tremaining: 9.77s\n",
      "570:\tlearn: 0.0056128\ttotal: 13s\tremaining: 9.74s\n",
      "571:\tlearn: 0.0055888\ttotal: 13s\tremaining: 9.71s\n",
      "572:\tlearn: 0.0055562\ttotal: 13s\tremaining: 9.68s\n",
      "573:\tlearn: 0.0055380\ttotal: 13s\tremaining: 9.65s\n",
      "574:\tlearn: 0.0055170\ttotal: 13s\tremaining: 9.62s\n",
      "575:\tlearn: 0.0055071\ttotal: 13s\tremaining: 9.59s\n",
      "576:\tlearn: 0.0054763\ttotal: 13s\tremaining: 9.56s\n",
      "577:\tlearn: 0.0054642\ttotal: 13.1s\tremaining: 9.53s\n",
      "578:\tlearn: 0.0054494\ttotal: 13.1s\tremaining: 9.5s\n",
      "579:\tlearn: 0.0054370\ttotal: 13.1s\tremaining: 9.47s\n",
      "580:\tlearn: 0.0054231\ttotal: 13.1s\tremaining: 9.44s\n",
      "581:\tlearn: 0.0053985\ttotal: 13.1s\tremaining: 9.41s\n",
      "582:\tlearn: 0.0053856\ttotal: 13.1s\tremaining: 9.38s\n",
      "583:\tlearn: 0.0053526\ttotal: 13.1s\tremaining: 9.35s\n",
      "584:\tlearn: 0.0053343\ttotal: 13.1s\tremaining: 9.33s\n",
      "585:\tlearn: 0.0053201\ttotal: 13.2s\tremaining: 9.3s\n",
      "586:\tlearn: 0.0052925\ttotal: 13.2s\tremaining: 9.27s\n",
      "587:\tlearn: 0.0052730\ttotal: 13.2s\tremaining: 9.24s\n",
      "588:\tlearn: 0.0052542\ttotal: 13.2s\tremaining: 9.21s\n",
      "589:\tlearn: 0.0052349\ttotal: 13.2s\tremaining: 9.18s\n",
      "590:\tlearn: 0.0052039\ttotal: 13.2s\tremaining: 9.15s\n",
      "591:\tlearn: 0.0051899\ttotal: 13.2s\tremaining: 9.13s\n",
      "592:\tlearn: 0.0051637\ttotal: 13.3s\tremaining: 9.1s\n",
      "593:\tlearn: 0.0051474\ttotal: 13.3s\tremaining: 9.07s\n",
      "594:\tlearn: 0.0051223\ttotal: 13.3s\tremaining: 9.04s\n",
      "595:\tlearn: 0.0051049\ttotal: 13.3s\tremaining: 9.01s\n",
      "596:\tlearn: 0.0050865\ttotal: 13.3s\tremaining: 8.98s\n",
      "597:\tlearn: 0.0050669\ttotal: 13.3s\tremaining: 8.96s\n",
      "598:\tlearn: 0.0050591\ttotal: 13.3s\tremaining: 8.93s\n",
      "599:\tlearn: 0.0050487\ttotal: 13.3s\tremaining: 8.9s\n",
      "600:\tlearn: 0.0050310\ttotal: 13.4s\tremaining: 8.87s\n",
      "601:\tlearn: 0.0050127\ttotal: 13.4s\tremaining: 8.84s\n",
      "602:\tlearn: 0.0050015\ttotal: 13.4s\tremaining: 8.82s\n",
      "603:\tlearn: 0.0049827\ttotal: 13.4s\tremaining: 8.79s\n",
      "604:\tlearn: 0.0049697\ttotal: 13.4s\tremaining: 8.76s\n",
      "605:\tlearn: 0.0049595\ttotal: 13.4s\tremaining: 8.74s\n",
      "606:\tlearn: 0.0049317\ttotal: 13.4s\tremaining: 8.71s\n",
      "607:\tlearn: 0.0049135\ttotal: 13.5s\tremaining: 8.68s\n",
      "608:\tlearn: 0.0048880\ttotal: 13.5s\tremaining: 8.65s\n",
      "609:\tlearn: 0.0048610\ttotal: 13.5s\tremaining: 8.62s\n",
      "610:\tlearn: 0.0048452\ttotal: 13.5s\tremaining: 8.6s\n",
      "611:\tlearn: 0.0048227\ttotal: 13.5s\tremaining: 8.57s\n",
      "612:\tlearn: 0.0047997\ttotal: 13.5s\tremaining: 8.54s\n",
      "613:\tlearn: 0.0047830\ttotal: 13.5s\tremaining: 8.51s\n",
      "614:\tlearn: 0.0047533\ttotal: 13.6s\tremaining: 8.49s\n",
      "615:\tlearn: 0.0047443\ttotal: 13.6s\tremaining: 8.46s\n",
      "616:\tlearn: 0.0047333\ttotal: 13.6s\tremaining: 8.43s\n",
      "617:\tlearn: 0.0047180\ttotal: 13.6s\tremaining: 8.4s\n",
      "618:\tlearn: 0.0047002\ttotal: 13.6s\tremaining: 8.38s\n",
      "619:\tlearn: 0.0046920\ttotal: 13.6s\tremaining: 8.35s\n",
      "620:\tlearn: 0.0046724\ttotal: 13.6s\tremaining: 8.32s\n",
      "621:\tlearn: 0.0046491\ttotal: 13.7s\tremaining: 8.3s\n",
      "622:\tlearn: 0.0046407\ttotal: 13.7s\tremaining: 8.27s\n",
      "623:\tlearn: 0.0046240\ttotal: 13.7s\tremaining: 8.24s\n",
      "624:\tlearn: 0.0046134\ttotal: 13.7s\tremaining: 8.21s\n",
      "625:\tlearn: 0.0045990\ttotal: 13.7s\tremaining: 8.19s\n",
      "626:\tlearn: 0.0045835\ttotal: 13.7s\tremaining: 8.16s\n",
      "627:\tlearn: 0.0045706\ttotal: 13.7s\tremaining: 8.13s\n",
      "628:\tlearn: 0.0045436\ttotal: 13.7s\tremaining: 8.11s\n",
      "629:\tlearn: 0.0045317\ttotal: 13.8s\tremaining: 8.08s\n",
      "630:\tlearn: 0.0045126\ttotal: 13.8s\tremaining: 8.05s\n",
      "631:\tlearn: 0.0044919\ttotal: 13.8s\tremaining: 8.03s\n",
      "632:\tlearn: 0.0044688\ttotal: 13.8s\tremaining: 8s\n",
      "633:\tlearn: 0.0044600\ttotal: 13.8s\tremaining: 7.97s\n",
      "634:\tlearn: 0.0044529\ttotal: 13.8s\tremaining: 7.95s\n",
      "635:\tlearn: 0.0044450\ttotal: 13.8s\tremaining: 7.92s\n",
      "636:\tlearn: 0.0044268\ttotal: 13.9s\tremaining: 7.9s\n",
      "637:\tlearn: 0.0044119\ttotal: 13.9s\tremaining: 7.87s\n",
      "638:\tlearn: 0.0043954\ttotal: 13.9s\tremaining: 7.84s\n",
      "639:\tlearn: 0.0043699\ttotal: 13.9s\tremaining: 7.82s\n",
      "640:\tlearn: 0.0043555\ttotal: 13.9s\tremaining: 7.79s\n",
      "641:\tlearn: 0.0043340\ttotal: 13.9s\tremaining: 7.77s\n",
      "642:\tlearn: 0.0043127\ttotal: 13.9s\tremaining: 7.74s\n",
      "643:\tlearn: 0.0042937\ttotal: 14s\tremaining: 7.71s\n",
      "644:\tlearn: 0.0042791\ttotal: 14s\tremaining: 7.69s\n",
      "645:\tlearn: 0.0042606\ttotal: 14s\tremaining: 7.66s\n",
      "646:\tlearn: 0.0042523\ttotal: 14s\tremaining: 7.63s\n",
      "647:\tlearn: 0.0042340\ttotal: 14s\tremaining: 7.61s\n",
      "648:\tlearn: 0.0042228\ttotal: 14s\tremaining: 7.58s\n",
      "649:\tlearn: 0.0042176\ttotal: 14s\tremaining: 7.56s\n",
      "650:\tlearn: 0.0041996\ttotal: 14.1s\tremaining: 7.53s\n",
      "651:\tlearn: 0.0041878\ttotal: 14.1s\tremaining: 7.51s\n",
      "652:\tlearn: 0.0041693\ttotal: 14.1s\tremaining: 7.48s\n",
      "653:\tlearn: 0.0041597\ttotal: 14.1s\tremaining: 7.46s\n",
      "654:\tlearn: 0.0041458\ttotal: 14.1s\tremaining: 7.43s\n",
      "655:\tlearn: 0.0041305\ttotal: 14.1s\tremaining: 7.4s\n",
      "656:\tlearn: 0.0041127\ttotal: 14.1s\tremaining: 7.38s\n",
      "657:\tlearn: 0.0040869\ttotal: 14.1s\tremaining: 7.35s\n",
      "658:\tlearn: 0.0040725\ttotal: 14.2s\tremaining: 7.33s\n",
      "659:\tlearn: 0.0040578\ttotal: 14.2s\tremaining: 7.3s\n",
      "660:\tlearn: 0.0040411\ttotal: 14.2s\tremaining: 7.28s\n",
      "661:\tlearn: 0.0040280\ttotal: 14.2s\tremaining: 7.25s\n",
      "662:\tlearn: 0.0040142\ttotal: 14.2s\tremaining: 7.22s\n",
      "663:\tlearn: 0.0040041\ttotal: 14.2s\tremaining: 7.2s\n",
      "664:\tlearn: 0.0039866\ttotal: 14.2s\tremaining: 7.17s\n",
      "665:\tlearn: 0.0039799\ttotal: 14.3s\tremaining: 7.15s\n",
      "666:\tlearn: 0.0039688\ttotal: 14.3s\tremaining: 7.13s\n",
      "667:\tlearn: 0.0039580\ttotal: 14.3s\tremaining: 7.1s\n",
      "668:\tlearn: 0.0039477\ttotal: 14.3s\tremaining: 7.08s\n",
      "669:\tlearn: 0.0039393\ttotal: 14.3s\tremaining: 7.05s\n",
      "670:\tlearn: 0.0039208\ttotal: 14.3s\tremaining: 7.03s\n",
      "671:\tlearn: 0.0039004\ttotal: 14.3s\tremaining: 7s\n",
      "672:\tlearn: 0.0038930\ttotal: 14.4s\tremaining: 6.98s\n",
      "673:\tlearn: 0.0038843\ttotal: 14.4s\tremaining: 6.95s\n",
      "674:\tlearn: 0.0038718\ttotal: 14.4s\tremaining: 6.93s\n",
      "675:\tlearn: 0.0038641\ttotal: 14.4s\tremaining: 6.9s\n",
      "676:\tlearn: 0.0038606\ttotal: 14.4s\tremaining: 6.88s\n",
      "677:\tlearn: 0.0038475\ttotal: 14.4s\tremaining: 6.85s\n",
      "678:\tlearn: 0.0038300\ttotal: 14.4s\tremaining: 6.83s\n",
      "679:\tlearn: 0.0038226\ttotal: 14.5s\tremaining: 6.8s\n",
      "680:\tlearn: 0.0038014\ttotal: 14.5s\tremaining: 6.78s\n",
      "681:\tlearn: 0.0037903\ttotal: 14.5s\tremaining: 6.75s\n",
      "682:\tlearn: 0.0037820\ttotal: 14.5s\tremaining: 6.73s\n",
      "683:\tlearn: 0.0037729\ttotal: 14.5s\tremaining: 6.7s\n",
      "684:\tlearn: 0.0037667\ttotal: 14.5s\tremaining: 6.68s\n",
      "685:\tlearn: 0.0037550\ttotal: 14.5s\tremaining: 6.65s\n",
      "686:\tlearn: 0.0037401\ttotal: 14.6s\tremaining: 6.63s\n",
      "687:\tlearn: 0.0037198\ttotal: 14.6s\tremaining: 6.61s\n",
      "688:\tlearn: 0.0037028\ttotal: 14.6s\tremaining: 6.58s\n",
      "689:\tlearn: 0.0036941\ttotal: 14.6s\tremaining: 6.56s\n",
      "690:\tlearn: 0.0036826\ttotal: 14.6s\tremaining: 6.53s\n",
      "691:\tlearn: 0.0036666\ttotal: 14.6s\tremaining: 6.51s\n",
      "692:\tlearn: 0.0036591\ttotal: 14.6s\tremaining: 6.48s\n",
      "693:\tlearn: 0.0036431\ttotal: 14.7s\tremaining: 6.46s\n",
      "694:\tlearn: 0.0036320\ttotal: 14.7s\tremaining: 6.44s\n",
      "695:\tlearn: 0.0036191\ttotal: 14.7s\tremaining: 6.41s\n",
      "696:\tlearn: 0.0036111\ttotal: 14.7s\tremaining: 6.39s\n",
      "697:\tlearn: 0.0035966\ttotal: 14.7s\tremaining: 6.36s\n",
      "698:\tlearn: 0.0035840\ttotal: 14.7s\tremaining: 6.34s\n",
      "699:\tlearn: 0.0035753\ttotal: 14.7s\tremaining: 6.31s\n",
      "700:\tlearn: 0.0035660\ttotal: 14.7s\tremaining: 6.29s\n",
      "701:\tlearn: 0.0035573\ttotal: 14.8s\tremaining: 6.26s\n",
      "702:\tlearn: 0.0035459\ttotal: 14.8s\tremaining: 6.24s\n",
      "703:\tlearn: 0.0035278\ttotal: 14.8s\tremaining: 6.22s\n",
      "704:\tlearn: 0.0035164\ttotal: 14.8s\tremaining: 6.19s\n",
      "705:\tlearn: 0.0035104\ttotal: 14.8s\tremaining: 6.17s\n",
      "706:\tlearn: 0.0034935\ttotal: 14.8s\tremaining: 6.14s\n",
      "707:\tlearn: 0.0034880\ttotal: 14.8s\tremaining: 6.12s\n",
      "708:\tlearn: 0.0034729\ttotal: 14.9s\tremaining: 6.1s\n",
      "709:\tlearn: 0.0034674\ttotal: 14.9s\tremaining: 6.07s\n",
      "710:\tlearn: 0.0034499\ttotal: 14.9s\tremaining: 6.05s\n",
      "711:\tlearn: 0.0034360\ttotal: 14.9s\tremaining: 6.03s\n",
      "712:\tlearn: 0.0034305\ttotal: 14.9s\tremaining: 6s\n",
      "713:\tlearn: 0.0034159\ttotal: 14.9s\tremaining: 5.98s\n",
      "714:\tlearn: 0.0034052\ttotal: 14.9s\tremaining: 5.96s\n",
      "715:\tlearn: 0.0033922\ttotal: 15s\tremaining: 5.93s\n",
      "716:\tlearn: 0.0033811\ttotal: 15s\tremaining: 5.91s\n",
      "717:\tlearn: 0.0033625\ttotal: 15s\tremaining: 5.88s\n",
      "718:\tlearn: 0.0033509\ttotal: 15s\tremaining: 5.86s\n",
      "719:\tlearn: 0.0033441\ttotal: 15s\tremaining: 5.83s\n",
      "720:\tlearn: 0.0033359\ttotal: 15s\tremaining: 5.81s\n",
      "721:\tlearn: 0.0033227\ttotal: 15s\tremaining: 5.79s\n",
      "722:\tlearn: 0.0033153\ttotal: 15s\tremaining: 5.76s\n",
      "723:\tlearn: 0.0033078\ttotal: 15.1s\tremaining: 5.74s\n",
      "724:\tlearn: 0.0032919\ttotal: 15.1s\tremaining: 5.72s\n",
      "725:\tlearn: 0.0032773\ttotal: 15.1s\tremaining: 5.69s\n",
      "726:\tlearn: 0.0032665\ttotal: 15.1s\tremaining: 5.67s\n",
      "727:\tlearn: 0.0032521\ttotal: 15.1s\tremaining: 5.65s\n",
      "728:\tlearn: 0.0032405\ttotal: 15.1s\tremaining: 5.62s\n",
      "729:\tlearn: 0.0032334\ttotal: 15.1s\tremaining: 5.6s\n",
      "730:\tlearn: 0.0032193\ttotal: 15.1s\tremaining: 5.57s\n",
      "731:\tlearn: 0.0032068\ttotal: 15.2s\tremaining: 5.55s\n",
      "732:\tlearn: 0.0031986\ttotal: 15.2s\tremaining: 5.53s\n",
      "733:\tlearn: 0.0031915\ttotal: 15.2s\tremaining: 5.5s\n",
      "734:\tlearn: 0.0031839\ttotal: 15.2s\tremaining: 5.48s\n",
      "735:\tlearn: 0.0031704\ttotal: 15.2s\tremaining: 5.46s\n",
      "736:\tlearn: 0.0031614\ttotal: 15.2s\tremaining: 5.43s\n",
      "737:\tlearn: 0.0031495\ttotal: 15.2s\tremaining: 5.41s\n",
      "738:\tlearn: 0.0031365\ttotal: 15.3s\tremaining: 5.39s\n",
      "739:\tlearn: 0.0031365\ttotal: 15.3s\tremaining: 5.37s\n",
      "740:\tlearn: 0.0031317\ttotal: 15.3s\tremaining: 5.34s\n",
      "741:\tlearn: 0.0031248\ttotal: 15.3s\tremaining: 5.32s\n",
      "742:\tlearn: 0.0031173\ttotal: 15.3s\tremaining: 5.3s\n",
      "743:\tlearn: 0.0031077\ttotal: 15.3s\tremaining: 5.27s\n",
      "744:\tlearn: 0.0031036\ttotal: 15.3s\tremaining: 5.25s\n",
      "745:\tlearn: 0.0030970\ttotal: 15.4s\tremaining: 5.23s\n",
      "746:\tlearn: 0.0030858\ttotal: 15.4s\tremaining: 5.2s\n",
      "747:\tlearn: 0.0030762\ttotal: 15.4s\tremaining: 5.18s\n",
      "748:\tlearn: 0.0030688\ttotal: 15.4s\tremaining: 5.16s\n",
      "749:\tlearn: 0.0030580\ttotal: 15.4s\tremaining: 5.14s\n",
      "750:\tlearn: 0.0030488\ttotal: 15.4s\tremaining: 5.11s\n",
      "751:\tlearn: 0.0030392\ttotal: 15.4s\tremaining: 5.09s\n",
      "752:\tlearn: 0.0030349\ttotal: 15.4s\tremaining: 5.07s\n",
      "753:\tlearn: 0.0030229\ttotal: 15.5s\tremaining: 5.04s\n",
      "754:\tlearn: 0.0030097\ttotal: 15.5s\tremaining: 5.02s\n",
      "755:\tlearn: 0.0029997\ttotal: 15.5s\tremaining: 5s\n",
      "756:\tlearn: 0.0029929\ttotal: 15.5s\tremaining: 4.98s\n",
      "757:\tlearn: 0.0029774\ttotal: 15.5s\tremaining: 4.95s\n",
      "758:\tlearn: 0.0029650\ttotal: 15.5s\tremaining: 4.93s\n",
      "759:\tlearn: 0.0029583\ttotal: 15.5s\tremaining: 4.91s\n",
      "760:\tlearn: 0.0029492\ttotal: 15.5s\tremaining: 4.88s\n",
      "761:\tlearn: 0.0029380\ttotal: 15.6s\tremaining: 4.86s\n",
      "762:\tlearn: 0.0029334\ttotal: 15.6s\tremaining: 4.84s\n",
      "763:\tlearn: 0.0029257\ttotal: 15.6s\tremaining: 4.82s\n",
      "764:\tlearn: 0.0029107\ttotal: 15.6s\tremaining: 4.79s\n",
      "765:\tlearn: 0.0029015\ttotal: 15.6s\tremaining: 4.77s\n",
      "766:\tlearn: 0.0028952\ttotal: 15.6s\tremaining: 4.75s\n",
      "767:\tlearn: 0.0028908\ttotal: 15.6s\tremaining: 4.72s\n",
      "768:\tlearn: 0.0028751\ttotal: 15.7s\tremaining: 4.7s\n",
      "769:\tlearn: 0.0028609\ttotal: 15.7s\tremaining: 4.68s\n",
      "770:\tlearn: 0.0028609\ttotal: 15.7s\tremaining: 4.66s\n",
      "771:\tlearn: 0.0028555\ttotal: 15.7s\tremaining: 4.64s\n",
      "772:\tlearn: 0.0028492\ttotal: 15.7s\tremaining: 4.61s\n",
      "773:\tlearn: 0.0028393\ttotal: 15.7s\tremaining: 4.59s\n",
      "774:\tlearn: 0.0028393\ttotal: 15.7s\tremaining: 4.57s\n",
      "775:\tlearn: 0.0028320\ttotal: 15.7s\tremaining: 4.54s\n",
      "776:\tlearn: 0.0028212\ttotal: 15.8s\tremaining: 4.52s\n",
      "777:\tlearn: 0.0028149\ttotal: 15.8s\tremaining: 4.5s\n",
      "778:\tlearn: 0.0028074\ttotal: 15.8s\tremaining: 4.48s\n",
      "779:\tlearn: 0.0028074\ttotal: 15.8s\tremaining: 4.46s\n",
      "780:\tlearn: 0.0028074\ttotal: 15.8s\tremaining: 4.43s\n",
      "781:\tlearn: 0.0027990\ttotal: 15.8s\tremaining: 4.41s\n",
      "782:\tlearn: 0.0027924\ttotal: 15.8s\tremaining: 4.39s\n",
      "783:\tlearn: 0.0027867\ttotal: 15.8s\tremaining: 4.37s\n",
      "784:\tlearn: 0.0027867\ttotal: 15.9s\tremaining: 4.34s\n",
      "785:\tlearn: 0.0027811\ttotal: 15.9s\tremaining: 4.32s\n",
      "786:\tlearn: 0.0027811\ttotal: 15.9s\tremaining: 4.3s\n",
      "787:\tlearn: 0.0027811\ttotal: 15.9s\tremaining: 4.28s\n",
      "788:\tlearn: 0.0027698\ttotal: 15.9s\tremaining: 4.26s\n",
      "789:\tlearn: 0.0027698\ttotal: 15.9s\tremaining: 4.23s\n",
      "790:\tlearn: 0.0027596\ttotal: 15.9s\tremaining: 4.21s\n",
      "791:\tlearn: 0.0027596\ttotal: 16s\tremaining: 4.19s\n",
      "792:\tlearn: 0.0027596\ttotal: 16s\tremaining: 4.17s\n",
      "793:\tlearn: 0.0027549\ttotal: 16s\tremaining: 4.15s\n",
      "794:\tlearn: 0.0027549\ttotal: 16s\tremaining: 4.12s\n",
      "795:\tlearn: 0.0027482\ttotal: 16s\tremaining: 4.1s\n",
      "796:\tlearn: 0.0027421\ttotal: 16s\tremaining: 4.08s\n",
      "797:\tlearn: 0.0027421\ttotal: 16s\tremaining: 4.06s\n",
      "798:\tlearn: 0.0027421\ttotal: 16s\tremaining: 4.04s\n",
      "799:\tlearn: 0.0027368\ttotal: 16.1s\tremaining: 4.01s\n",
      "800:\tlearn: 0.0027368\ttotal: 16.1s\tremaining: 3.99s\n",
      "801:\tlearn: 0.0027368\ttotal: 16.1s\tremaining: 3.97s\n",
      "802:\tlearn: 0.0027291\ttotal: 16.1s\tremaining: 3.95s\n",
      "803:\tlearn: 0.0027209\ttotal: 16.1s\tremaining: 3.93s\n",
      "804:\tlearn: 0.0027164\ttotal: 16.1s\tremaining: 3.91s\n",
      "805:\tlearn: 0.0027164\ttotal: 16.1s\tremaining: 3.88s\n",
      "806:\tlearn: 0.0027164\ttotal: 16.1s\tremaining: 3.86s\n",
      "807:\tlearn: 0.0027033\ttotal: 16.2s\tremaining: 3.84s\n",
      "808:\tlearn: 0.0026959\ttotal: 16.2s\tremaining: 3.82s\n",
      "809:\tlearn: 0.0026959\ttotal: 16.2s\tremaining: 3.8s\n",
      "810:\tlearn: 0.0026903\ttotal: 16.2s\tremaining: 3.77s\n",
      "811:\tlearn: 0.0026903\ttotal: 16.2s\tremaining: 3.75s\n",
      "812:\tlearn: 0.0026848\ttotal: 16.2s\tremaining: 3.73s\n",
      "813:\tlearn: 0.0026848\ttotal: 16.2s\tremaining: 3.71s\n",
      "814:\tlearn: 0.0026848\ttotal: 16.2s\tremaining: 3.69s\n",
      "815:\tlearn: 0.0026848\ttotal: 16.3s\tremaining: 3.67s\n",
      "816:\tlearn: 0.0026801\ttotal: 16.3s\tremaining: 3.65s\n",
      "817:\tlearn: 0.0026739\ttotal: 16.3s\tremaining: 3.62s\n",
      "818:\tlearn: 0.0026629\ttotal: 16.3s\tremaining: 3.6s\n",
      "819:\tlearn: 0.0026519\ttotal: 16.3s\tremaining: 3.58s\n",
      "820:\tlearn: 0.0026449\ttotal: 16.3s\tremaining: 3.56s\n",
      "821:\tlearn: 0.0026369\ttotal: 16.3s\tremaining: 3.54s\n",
      "822:\tlearn: 0.0026369\ttotal: 16.4s\tremaining: 3.52s\n",
      "823:\tlearn: 0.0026278\ttotal: 16.4s\tremaining: 3.5s\n",
      "824:\tlearn: 0.0026278\ttotal: 16.4s\tremaining: 3.48s\n",
      "825:\tlearn: 0.0026187\ttotal: 16.4s\tremaining: 3.45s\n",
      "826:\tlearn: 0.0026187\ttotal: 16.4s\tremaining: 3.43s\n",
      "827:\tlearn: 0.0026113\ttotal: 16.4s\tremaining: 3.41s\n",
      "828:\tlearn: 0.0026030\ttotal: 16.4s\tremaining: 3.39s\n",
      "829:\tlearn: 0.0026030\ttotal: 16.4s\tremaining: 3.37s\n",
      "830:\tlearn: 0.0026030\ttotal: 16.5s\tremaining: 3.35s\n",
      "831:\tlearn: 0.0026030\ttotal: 16.5s\tremaining: 3.33s\n",
      "832:\tlearn: 0.0026030\ttotal: 16.5s\tremaining: 3.3s\n",
      "833:\tlearn: 0.0026030\ttotal: 16.5s\tremaining: 3.28s\n",
      "834:\tlearn: 0.0026030\ttotal: 16.5s\tremaining: 3.26s\n",
      "835:\tlearn: 0.0025963\ttotal: 16.5s\tremaining: 3.24s\n",
      "836:\tlearn: 0.0025962\ttotal: 16.5s\tremaining: 3.22s\n",
      "837:\tlearn: 0.0025851\ttotal: 16.6s\tremaining: 3.2s\n",
      "838:\tlearn: 0.0025700\ttotal: 16.6s\tremaining: 3.18s\n",
      "839:\tlearn: 0.0025640\ttotal: 16.6s\tremaining: 3.16s\n",
      "840:\tlearn: 0.0025640\ttotal: 16.6s\tremaining: 3.13s\n",
      "841:\tlearn: 0.0025524\ttotal: 16.6s\tremaining: 3.11s\n",
      "842:\tlearn: 0.0025524\ttotal: 16.6s\tremaining: 3.09s\n",
      "843:\tlearn: 0.0025524\ttotal: 16.6s\tremaining: 3.07s\n",
      "844:\tlearn: 0.0025524\ttotal: 16.6s\tremaining: 3.05s\n",
      "845:\tlearn: 0.0025524\ttotal: 16.6s\tremaining: 3.03s\n",
      "846:\tlearn: 0.0025461\ttotal: 16.7s\tremaining: 3.01s\n",
      "847:\tlearn: 0.0025342\ttotal: 16.7s\tremaining: 2.99s\n",
      "848:\tlearn: 0.0025342\ttotal: 16.7s\tremaining: 2.97s\n",
      "849:\tlearn: 0.0025342\ttotal: 16.7s\tremaining: 2.95s\n",
      "850:\tlearn: 0.0025342\ttotal: 16.7s\tremaining: 2.93s\n",
      "851:\tlearn: 0.0025250\ttotal: 16.7s\tremaining: 2.9s\n",
      "852:\tlearn: 0.0025250\ttotal: 16.7s\tremaining: 2.88s\n",
      "853:\tlearn: 0.0025201\ttotal: 16.8s\tremaining: 2.86s\n",
      "854:\tlearn: 0.0025201\ttotal: 16.8s\tremaining: 2.84s\n",
      "855:\tlearn: 0.0025201\ttotal: 16.8s\tremaining: 2.82s\n",
      "856:\tlearn: 0.0025201\ttotal: 16.8s\tremaining: 2.8s\n",
      "857:\tlearn: 0.0025201\ttotal: 16.8s\tremaining: 2.78s\n",
      "858:\tlearn: 0.0025201\ttotal: 16.8s\tremaining: 2.76s\n",
      "859:\tlearn: 0.0025133\ttotal: 16.8s\tremaining: 2.74s\n",
      "860:\tlearn: 0.0025133\ttotal: 16.8s\tremaining: 2.72s\n",
      "861:\tlearn: 0.0025133\ttotal: 16.9s\tremaining: 2.7s\n",
      "862:\tlearn: 0.0025133\ttotal: 16.9s\tremaining: 2.68s\n",
      "863:\tlearn: 0.0025058\ttotal: 16.9s\tremaining: 2.66s\n",
      "864:\tlearn: 0.0025002\ttotal: 16.9s\tremaining: 2.64s\n",
      "865:\tlearn: 0.0024899\ttotal: 16.9s\tremaining: 2.62s\n",
      "866:\tlearn: 0.0024899\ttotal: 16.9s\tremaining: 2.6s\n",
      "867:\tlearn: 0.0024843\ttotal: 16.9s\tremaining: 2.58s\n",
      "868:\tlearn: 0.0024843\ttotal: 16.9s\tremaining: 2.55s\n",
      "869:\tlearn: 0.0024843\ttotal: 17s\tremaining: 2.53s\n",
      "870:\tlearn: 0.0024843\ttotal: 17s\tremaining: 2.51s\n",
      "871:\tlearn: 0.0024773\ttotal: 17s\tremaining: 2.49s\n",
      "872:\tlearn: 0.0024692\ttotal: 17s\tremaining: 2.47s\n",
      "873:\tlearn: 0.0024647\ttotal: 17s\tremaining: 2.45s\n",
      "874:\tlearn: 0.0024598\ttotal: 17s\tremaining: 2.43s\n",
      "875:\tlearn: 0.0024598\ttotal: 17s\tremaining: 2.41s\n",
      "876:\tlearn: 0.0024598\ttotal: 17s\tremaining: 2.39s\n",
      "877:\tlearn: 0.0024598\ttotal: 17.1s\tremaining: 2.37s\n",
      "878:\tlearn: 0.0024532\ttotal: 17.1s\tremaining: 2.35s\n",
      "879:\tlearn: 0.0024458\ttotal: 17.1s\tremaining: 2.33s\n",
      "880:\tlearn: 0.0024410\ttotal: 17.1s\tremaining: 2.31s\n",
      "881:\tlearn: 0.0024410\ttotal: 17.1s\tremaining: 2.29s\n",
      "882:\tlearn: 0.0024410\ttotal: 17.1s\tremaining: 2.27s\n",
      "883:\tlearn: 0.0024410\ttotal: 17.1s\tremaining: 2.25s\n",
      "884:\tlearn: 0.0024322\ttotal: 17.1s\tremaining: 2.23s\n",
      "885:\tlearn: 0.0024252\ttotal: 17.2s\tremaining: 2.21s\n",
      "886:\tlearn: 0.0024178\ttotal: 17.2s\tremaining: 2.19s\n",
      "887:\tlearn: 0.0024125\ttotal: 17.2s\tremaining: 2.17s\n",
      "888:\tlearn: 0.0024060\ttotal: 17.2s\tremaining: 2.15s\n",
      "889:\tlearn: 0.0024060\ttotal: 17.2s\tremaining: 2.13s\n",
      "890:\tlearn: 0.0024060\ttotal: 17.2s\tremaining: 2.11s\n",
      "891:\tlearn: 0.0023956\ttotal: 17.2s\tremaining: 2.08s\n",
      "892:\tlearn: 0.0023897\ttotal: 17.2s\tremaining: 2.07s\n",
      "893:\tlearn: 0.0023897\ttotal: 17.3s\tremaining: 2.05s\n",
      "894:\tlearn: 0.0023830\ttotal: 17.3s\tremaining: 2.02s\n",
      "895:\tlearn: 0.0023797\ttotal: 17.3s\tremaining: 2s\n",
      "896:\tlearn: 0.0023797\ttotal: 17.3s\tremaining: 1.99s\n",
      "897:\tlearn: 0.0023727\ttotal: 17.3s\tremaining: 1.97s\n",
      "898:\tlearn: 0.0023726\ttotal: 17.3s\tremaining: 1.95s\n",
      "899:\tlearn: 0.0023644\ttotal: 17.3s\tremaining: 1.93s\n",
      "900:\tlearn: 0.0023644\ttotal: 17.4s\tremaining: 1.91s\n",
      "901:\tlearn: 0.0023577\ttotal: 17.4s\tremaining: 1.89s\n",
      "902:\tlearn: 0.0023577\ttotal: 17.4s\tremaining: 1.87s\n",
      "903:\tlearn: 0.0023535\ttotal: 17.4s\tremaining: 1.85s\n",
      "904:\tlearn: 0.0023478\ttotal: 17.4s\tremaining: 1.83s\n",
      "905:\tlearn: 0.0023478\ttotal: 17.4s\tremaining: 1.81s\n",
      "906:\tlearn: 0.0023412\ttotal: 17.4s\tremaining: 1.79s\n",
      "907:\tlearn: 0.0023362\ttotal: 17.4s\tremaining: 1.77s\n",
      "908:\tlearn: 0.0023295\ttotal: 17.5s\tremaining: 1.75s\n",
      "909:\tlearn: 0.0023295\ttotal: 17.5s\tremaining: 1.73s\n",
      "910:\tlearn: 0.0023295\ttotal: 17.5s\tremaining: 1.71s\n",
      "911:\tlearn: 0.0023258\ttotal: 17.5s\tremaining: 1.69s\n",
      "912:\tlearn: 0.0023181\ttotal: 17.5s\tremaining: 1.67s\n",
      "913:\tlearn: 0.0023113\ttotal: 17.5s\tremaining: 1.65s\n",
      "914:\tlearn: 0.0023113\ttotal: 17.5s\tremaining: 1.63s\n",
      "915:\tlearn: 0.0023066\ttotal: 17.5s\tremaining: 1.61s\n",
      "916:\tlearn: 0.0022983\ttotal: 17.6s\tremaining: 1.59s\n",
      "917:\tlearn: 0.0022927\ttotal: 17.6s\tremaining: 1.57s\n",
      "918:\tlearn: 0.0022894\ttotal: 17.6s\tremaining: 1.55s\n",
      "919:\tlearn: 0.0022806\ttotal: 17.6s\tremaining: 1.53s\n",
      "920:\tlearn: 0.0022744\ttotal: 17.6s\tremaining: 1.51s\n",
      "921:\tlearn: 0.0022693\ttotal: 17.6s\tremaining: 1.49s\n",
      "922:\tlearn: 0.0022693\ttotal: 17.6s\tremaining: 1.47s\n",
      "923:\tlearn: 0.0022651\ttotal: 17.6s\tremaining: 1.45s\n",
      "924:\tlearn: 0.0022651\ttotal: 17.7s\tremaining: 1.43s\n",
      "925:\tlearn: 0.0022651\ttotal: 17.7s\tremaining: 1.41s\n",
      "926:\tlearn: 0.0022651\ttotal: 17.7s\tremaining: 1.39s\n",
      "927:\tlearn: 0.0022564\ttotal: 17.7s\tremaining: 1.37s\n",
      "928:\tlearn: 0.0022530\ttotal: 17.7s\tremaining: 1.35s\n",
      "929:\tlearn: 0.0022454\ttotal: 17.7s\tremaining: 1.33s\n",
      "930:\tlearn: 0.0022369\ttotal: 17.7s\tremaining: 1.31s\n",
      "931:\tlearn: 0.0022309\ttotal: 17.7s\tremaining: 1.29s\n",
      "932:\tlearn: 0.0022256\ttotal: 17.8s\tremaining: 1.27s\n",
      "933:\tlearn: 0.0022195\ttotal: 17.8s\tremaining: 1.25s\n",
      "934:\tlearn: 0.0022104\ttotal: 17.8s\tremaining: 1.24s\n",
      "935:\tlearn: 0.0022049\ttotal: 17.8s\tremaining: 1.22s\n",
      "936:\tlearn: 0.0022001\ttotal: 17.8s\tremaining: 1.2s\n",
      "937:\tlearn: 0.0021941\ttotal: 17.8s\tremaining: 1.18s\n",
      "938:\tlearn: 0.0021941\ttotal: 17.8s\tremaining: 1.16s\n",
      "939:\tlearn: 0.0021885\ttotal: 17.8s\tremaining: 1.14s\n",
      "940:\tlearn: 0.0021805\ttotal: 17.9s\tremaining: 1.12s\n",
      "941:\tlearn: 0.0021742\ttotal: 17.9s\tremaining: 1.1s\n",
      "942:\tlearn: 0.0021682\ttotal: 17.9s\tremaining: 1.08s\n",
      "943:\tlearn: 0.0021682\ttotal: 17.9s\tremaining: 1.06s\n",
      "944:\tlearn: 0.0021643\ttotal: 17.9s\tremaining: 1.04s\n",
      "945:\tlearn: 0.0021643\ttotal: 17.9s\tremaining: 1.02s\n",
      "946:\tlearn: 0.0021643\ttotal: 17.9s\tremaining: 1s\n",
      "947:\tlearn: 0.0021556\ttotal: 17.9s\tremaining: 984ms\n",
      "948:\tlearn: 0.0021502\ttotal: 18s\tremaining: 965ms\n",
      "949:\tlearn: 0.0021447\ttotal: 18s\tremaining: 946ms\n",
      "950:\tlearn: 0.0021400\ttotal: 18s\tremaining: 927ms\n",
      "951:\tlearn: 0.0021400\ttotal: 18s\tremaining: 907ms\n",
      "952:\tlearn: 0.0021335\ttotal: 18s\tremaining: 888ms\n",
      "953:\tlearn: 0.0021259\ttotal: 18s\tremaining: 869ms\n",
      "954:\tlearn: 0.0021259\ttotal: 18s\tremaining: 850ms\n",
      "955:\tlearn: 0.0021259\ttotal: 18s\tremaining: 830ms\n",
      "956:\tlearn: 0.0021259\ttotal: 18.1s\tremaining: 811ms\n",
      "957:\tlearn: 0.0021259\ttotal: 18.1s\tremaining: 792ms\n",
      "958:\tlearn: 0.0021259\ttotal: 18.1s\tremaining: 773ms\n",
      "959:\tlearn: 0.0021259\ttotal: 18.1s\tremaining: 754ms\n",
      "960:\tlearn: 0.0021259\ttotal: 18.1s\tremaining: 735ms\n",
      "961:\tlearn: 0.0021259\ttotal: 18.1s\tremaining: 715ms\n",
      "962:\tlearn: 0.0021259\ttotal: 18.1s\tremaining: 696ms\n",
      "963:\tlearn: 0.0021197\ttotal: 18.1s\tremaining: 678ms\n",
      "964:\tlearn: 0.0021197\ttotal: 18.2s\tremaining: 659ms\n",
      "965:\tlearn: 0.0021197\ttotal: 18.2s\tremaining: 640ms\n",
      "966:\tlearn: 0.0021155\ttotal: 18.2s\tremaining: 621ms\n",
      "967:\tlearn: 0.0021155\ttotal: 18.2s\tremaining: 601ms\n",
      "968:\tlearn: 0.0021155\ttotal: 18.2s\tremaining: 582ms\n",
      "969:\tlearn: 0.0021155\ttotal: 18.2s\tremaining: 563ms\n",
      "970:\tlearn: 0.0021155\ttotal: 18.2s\tremaining: 544ms\n",
      "971:\tlearn: 0.0021155\ttotal: 18.2s\tremaining: 526ms\n",
      "972:\tlearn: 0.0021155\ttotal: 18.3s\tremaining: 507ms\n",
      "973:\tlearn: 0.0021155\ttotal: 18.3s\tremaining: 488ms\n",
      "974:\tlearn: 0.0021155\ttotal: 18.3s\tremaining: 469ms\n",
      "975:\tlearn: 0.0021155\ttotal: 18.3s\tremaining: 450ms\n",
      "976:\tlearn: 0.0021155\ttotal: 18.3s\tremaining: 431ms\n",
      "977:\tlearn: 0.0021155\ttotal: 18.3s\tremaining: 412ms\n",
      "978:\tlearn: 0.0021155\ttotal: 18.3s\tremaining: 393ms\n",
      "979:\tlearn: 0.0021155\ttotal: 18.3s\tremaining: 374ms\n",
      "980:\tlearn: 0.0021101\ttotal: 18.4s\tremaining: 356ms\n",
      "981:\tlearn: 0.0021037\ttotal: 18.4s\tremaining: 337ms\n",
      "982:\tlearn: 0.0021037\ttotal: 18.4s\tremaining: 318ms\n",
      "983:\tlearn: 0.0020998\ttotal: 18.4s\tremaining: 299ms\n",
      "984:\tlearn: 0.0020997\ttotal: 18.4s\tremaining: 280ms\n",
      "985:\tlearn: 0.0020949\ttotal: 18.4s\tremaining: 261ms\n",
      "986:\tlearn: 0.0020904\ttotal: 18.4s\tremaining: 243ms\n",
      "987:\tlearn: 0.0020904\ttotal: 18.4s\tremaining: 224ms\n",
      "988:\tlearn: 0.0020904\ttotal: 18.5s\tremaining: 205ms\n",
      "989:\tlearn: 0.0020904\ttotal: 18.5s\tremaining: 187ms\n",
      "990:\tlearn: 0.0020904\ttotal: 18.5s\tremaining: 168ms\n",
      "991:\tlearn: 0.0020839\ttotal: 18.5s\tremaining: 149ms\n",
      "992:\tlearn: 0.0020770\ttotal: 18.5s\tremaining: 130ms\n",
      "993:\tlearn: 0.0020770\ttotal: 18.5s\tremaining: 112ms\n",
      "994:\tlearn: 0.0020715\ttotal: 18.5s\tremaining: 93.1ms\n",
      "995:\tlearn: 0.0020715\ttotal: 18.5s\tremaining: 74.5ms\n",
      "996:\tlearn: 0.0020714\ttotal: 18.6s\tremaining: 55.8ms\n",
      "997:\tlearn: 0.0020657\ttotal: 18.6s\tremaining: 37.2ms\n",
      "998:\tlearn: 0.0020577\ttotal: 18.6s\tremaining: 18.6ms\n",
      "999:\tlearn: 0.0020537\ttotal: 18.6s\tremaining: 0us\n",
      "XGBClassifier: 0.9932985852568875 \n",
      "\n",
      "\n",
      "CatBoostClassifier: 0.9962880475129918 \n",
      "\n",
      "\n",
      "SVC: 0.9667673716012084 \n",
      "\n",
      "\n",
      "RandomForestClassifier: 0.9926470588235294 \n",
      "\n",
      "\n",
      "ExtraTreesClassifier: 0.9875640087783468 \n",
      "\n",
      "\n",
      "KNeighborsClassifier: 0.9796533534287867 \n",
      "\n",
      "\n",
      "LGBMClassifier: 0.9910581222056632 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"XGBClassifier\": XGBClassifier(),\n",
    "    \"CatBoostClassifier\": CatBoostClassifier(),\n",
    "    \"SVC\": SVC(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "    \"ExtraTreesClassifier\": ExtraTreesClassifier(),\n",
    "    \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
    "    \"LGBMClassifier\": LGBMClassifier()\n",
    "}\n",
    "\n",
    "score = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model = model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    score[name] = f1_score(y_test, y_pred)\n",
    "\n",
    "for name, score in score.items():\n",
    "    print(f\"{name}: {score} \\n\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: <function lightgbm_classification at 0x7fe96f3c1310> \n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.29s/trial, best loss: 0.010185185185185186]\n",
      "100%|██████████| 2/2 [00:00<00:00,  1.68trial/s, best loss: 0.010185185185185186]\n",
      "100%|██████████| 3/3 [00:00<00:00,  1.03trial/s, best loss: 0.010185185185185186]\n",
      "100%|██████████| 4/4 [00:09<00:00,  9.40s/trial, best loss: 0.010185185185185186]\n",
      "100%|██████████| 5/5 [00:04<00:00,  4.41s/trial, best loss: 0.010185185185185186]\n",
      "100%|██████████| 6/6 [00:03<00:00,  3.64s/trial, best loss: 0.010185185185185186]\n",
      "100%|██████████| 7/7 [00:09<00:00,  9.14s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 8/8 [00:04<00:00,  4.15s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 9/9 [00:00<00:00,  1.35trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 10/10 [00:50<00:00, 50.47s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 11/11 [00:21<00:00, 21.06s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 12/12 [00:00<00:00,  1.21trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 13/13 [00:00<00:00,  1.89trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 14/14 [00:06<00:00,  6.23s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 15/15 [00:05<00:00,  5.33s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 16/16 [00:03<00:00,  3.06s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 17/17 [00:01<00:00,  1.22s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 18/18 [00:09<00:00,  9.35s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 19/19 [00:09<00:00,  9.99s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 20/20 [00:05<00:00,  5.95s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 21/21 [00:08<00:00,  8.13s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 22/22 [00:05<00:00,  5.74s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 23/23 [00:03<00:00,  3.00s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 24/24 [00:33<00:00, 33.94s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 25/25 [00:33<00:00, 33.25s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 26/26 [00:10<00:00, 10.85s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 27/27 [00:01<00:00,  1.87s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 28/28 [00:00<00:00,  2.44trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 29/29 [00:21<00:00, 21.42s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 30/30 [00:01<00:00,  1.46s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 31/31 [00:01<00:00,  1.08s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 32/32 [00:12<00:00, 12.33s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 33/33 [00:02<00:00,  2.47s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 34/34 [00:12<00:00, 12.76s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 35/35 [00:06<00:00,  6.29s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 36/36 [00:03<00:00,  3.03s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 37/37 [00:47<00:00, 47.93s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 38/38 [00:33<00:00, 33.30s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 39/39 [00:01<00:00,  1.70s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 40/40 [00:09<00:00,  9.64s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 41/41 [00:08<00:00,  8.89s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 42/42 [00:01<00:00,  1.80s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 43/43 [00:04<00:00,  4.41s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 44/44 [00:37<00:00, 37.56s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 45/45 [00:04<00:00,  4.12s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 46/46 [00:13<00:00, 13.90s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 47/47 [00:00<00:00,  1.46trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 48/48 [00:08<00:00,  8.13s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 49/49 [00:06<00:00,  6.59s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 50/50 [00:04<00:00,  4.47s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 51/51 [00:02<00:00,  2.31s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 52/52 [00:04<00:00,  4.38s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 53/53 [00:00<00:00,  1.56trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 54/54 [00:01<00:00,  1.66s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 55/55 [00:01<00:00,  1.05s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 56/56 [00:00<00:00,  2.62trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 57/57 [00:10<00:00, 10.24s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 58/58 [00:18<00:00, 18.49s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 59/59 [00:02<00:00,  2.84s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 60/60 [00:06<00:00,  6.24s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 61/61 [00:01<00:00,  1.07s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 62/62 [00:03<00:00,  3.73s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 63/63 [00:00<00:00,  2.29trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 64/64 [00:32<00:00, 32.59s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 65/65 [00:03<00:00,  3.15s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 66/66 [00:02<00:00,  2.83s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 67/67 [00:01<00:00,  1.16s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 68/68 [00:01<00:00,  1.23s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 69/69 [00:05<00:00,  5.09s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 70/70 [00:07<00:00,  7.76s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 71/71 [00:14<00:00, 14.16s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 72/72 [00:02<00:00,  2.42s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 73/73 [00:05<00:00,  5.44s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 74/74 [00:12<00:00, 12.63s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 75/75 [00:12<00:00, 12.77s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 76/76 [00:04<00:00,  4.61s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 77/77 [00:02<00:00,  2.35s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 78/78 [00:42<00:00, 42.22s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 79/79 [00:02<00:00,  2.87s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 80/80 [00:03<00:00,  3.47s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 81/81 [00:19<00:00, 19.00s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 82/82 [00:03<00:00,  3.14s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 83/83 [00:13<00:00, 13.16s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 84/84 [00:00<00:00,  2.86trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 85/85 [00:00<00:00,  1.21trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 86/86 [00:03<00:00,  3.08s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 87/87 [00:09<00:00,  9.13s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 88/88 [00:25<00:00, 25.49s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 89/89 [00:22<00:00, 22.27s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 90/90 [00:01<00:00,  1.06s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 91/91 [00:03<00:00,  3.76s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 92/92 [00:00<00:00,  1.02trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 93/93 [00:01<00:00,  1.50s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 94/94 [00:04<00:00,  4.36s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 95/95 [00:31<00:00, 31.21s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 96/96 [00:26<00:00, 26.78s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 97/97 [00:08<00:00,  8.67s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 98/98 [00:06<00:00,  6.17s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 99/99 [00:03<00:00,  3.84s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 100/100 [00:09<00:00,  9.22s/trial, best loss: 0.00462962962962965]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: <function k_neighbors_classifier at 0x7fe96f957a60> \n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.67trial/s, best loss: 0.0064814814814815325]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.62trial/s, best loss: 0.0064814814814815325]\n",
      "100%|██████████| 3/3 [00:00<00:00,  1.72trial/s, best loss: 0.0064814814814815325]\n",
      "100%|██████████| 4/4 [00:00<00:00,  1.24trial/s, best loss: 0.0064814814814815325]\n",
      "100%|██████████| 5/5 [00:00<00:00,  8.11trial/s, best loss: 0.0064814814814815325]\n",
      "100%|██████████| 6/6 [00:00<00:00,  3.12trial/s, best loss: 0.0064814814814815325]\n",
      "100%|██████████| 7/7 [00:00<00:00,  2.68trial/s, best loss: 0.0064814814814815325]\n",
      "100%|██████████| 8/8 [00:00<00:00,  3.36trial/s, best loss: 0.0037037037037036535]\n",
      "100%|██████████| 9/9 [00:00<00:00,  6.26trial/s, best loss: 0.0037037037037036535]\n",
      "100%|██████████| 10/10 [00:00<00:00,  7.33trial/s, best loss: 0.0037037037037036535]\n",
      "100%|██████████| 11/11 [00:00<00:00,  2.59trial/s, best loss: 0.0037037037037036535]\n",
      "100%|██████████| 12/12 [00:00<00:00,  2.31trial/s, best loss: 0.0037037037037036535]\n",
      "100%|██████████| 13/13 [00:00<00:00,  2.72trial/s, best loss: 0.0037037037037036535]\n",
      "100%|██████████| 14/14 [00:00<00:00,  7.05trial/s, best loss: 0.0037037037037036535]\n",
      "100%|██████████| 15/15 [00:00<00:00,  9.45trial/s, best loss: 0.0037037037037036535]\n",
      "100%|██████████| 16/16 [00:00<00:00,  2.74trial/s, best loss: 0.0037037037037036535]\n",
      "100%|██████████| 17/17 [00:05<00:00,  5.73s/trial, best loss: 0.0037037037037036535]\n",
      "100%|██████████| 18/18 [00:00<00:00,  9.77trial/s, best loss: 0.0]\n",
      "100%|██████████| 19/19 [00:00<00:00,  2.70trial/s, best loss: 0.0]\n",
      "100%|██████████| 20/20 [00:00<00:00,  2.72trial/s, best loss: 0.0]\n",
      "100%|██████████| 21/21 [00:00<00:00,  2.92trial/s, best loss: 0.0]\n",
      "100%|██████████| 22/22 [00:00<00:00,  1.85trial/s, best loss: 0.0]\n",
      "100%|██████████| 23/23 [00:00<00:00,  2.93trial/s, best loss: 0.0]\n",
      "100%|██████████| 24/24 [00:23<00:00, 23.10s/trial, best loss: 0.0]\n",
      "100%|██████████| 25/25 [00:00<00:00,  5.80trial/s, best loss: 0.0]\n",
      "100%|██████████| 26/26 [00:00<00:00,  2.72trial/s, best loss: 0.0]\n",
      "100%|██████████| 27/27 [00:00<00:00,  9.16trial/s, best loss: 0.0]\n",
      "100%|██████████| 28/28 [00:00<00:00,  2.52trial/s, best loss: 0.0]\n",
      "100%|██████████| 29/29 [00:23<00:00, 23.39s/trial, best loss: 0.0]\n",
      "100%|██████████| 30/30 [00:00<00:00,  2.89trial/s, best loss: 0.0]\n",
      "100%|██████████| 31/31 [00:24<00:00, 24.37s/trial, best loss: 0.0]\n",
      "100%|██████████| 32/32 [00:00<00:00,  4.72trial/s, best loss: 0.0]\n",
      "100%|██████████| 33/33 [00:00<00:00,  1.52trial/s, best loss: 0.0]\n",
      "100%|██████████| 34/34 [00:04<00:00,  4.50s/trial, best loss: 0.0]\n",
      "100%|██████████| 35/35 [00:10<00:00, 10.02s/trial, best loss: 0.0]\n",
      "100%|██████████| 36/36 [00:00<00:00,  1.96trial/s, best loss: 0.0]\n",
      "100%|██████████| 37/37 [00:00<00:00,  2.52trial/s, best loss: 0.0]\n",
      "100%|██████████| 38/38 [00:00<00:00,  2.75trial/s, best loss: 0.0]\n",
      "100%|██████████| 39/39 [00:00<00:00,  2.82trial/s, best loss: 0.0]\n",
      "100%|██████████| 40/40 [00:00<00:00,  1.51trial/s, best loss: 0.0]\n",
      "100%|██████████| 41/41 [00:00<00:00,  1.70trial/s, best loss: 0.0]\n",
      "100%|██████████| 42/42 [00:00<00:00,  1.72trial/s, best loss: 0.0]\n",
      "100%|██████████| 43/43 [00:00<00:00,  2.93trial/s, best loss: 0.0]\n",
      "100%|██████████| 44/44 [00:00<00:00,  1.74trial/s, best loss: 0.0]\n",
      "100%|██████████| 45/45 [00:24<00:00, 24.36s/trial, best loss: 0.0]\n",
      "100%|██████████| 46/46 [00:00<00:00,  1.28trial/s, best loss: 0.0]\n",
      "100%|██████████| 47/47 [00:00<00:00,  4.77trial/s, best loss: 0.0]\n",
      "100%|██████████| 48/48 [00:00<00:00,  2.86trial/s, best loss: 0.0]\n",
      "100%|██████████| 49/49 [00:00<00:00,  2.81trial/s, best loss: 0.0]\n",
      "100%|██████████| 50/50 [00:07<00:00,  7.77s/trial, best loss: 0.0]\n",
      "100%|██████████| 51/51 [00:00<00:00,  2.53trial/s, best loss: 0.0]\n",
      "100%|██████████| 52/52 [00:00<00:00,  2.95trial/s, best loss: 0.0]\n",
      "100%|██████████| 53/53 [00:00<00:00,  1.80trial/s, best loss: 0.0]\n",
      "100%|██████████| 54/54 [00:00<00:00,  2.91trial/s, best loss: 0.0]\n",
      "100%|██████████| 55/55 [00:23<00:00, 23.44s/trial, best loss: 0.0]\n",
      "100%|██████████| 56/56 [00:00<00:00,  9.82trial/s, best loss: 0.0]\n",
      "100%|██████████| 57/57 [00:00<00:00,  1.27trial/s, best loss: 0.0]\n",
      "100%|██████████| 58/58 [00:00<00:00,  4.05trial/s, best loss: 0.0]\n",
      "100%|██████████| 59/59 [00:30<00:00, 30.78s/trial, best loss: 0.0]\n",
      "100%|██████████| 60/60 [00:00<00:00,  3.54trial/s, best loss: 0.0]\n",
      "100%|██████████| 61/61 [00:00<00:00,  5.74trial/s, best loss: 0.0]\n",
      "100%|██████████| 62/62 [00:00<00:00,  6.49trial/s, best loss: 0.0]\n",
      "100%|██████████| 63/63 [00:00<00:00,  2.12trial/s, best loss: 0.0]\n",
      "100%|██████████| 64/64 [00:33<00:00, 33.63s/trial, best loss: 0.0]\n",
      "100%|██████████| 65/65 [00:00<00:00,  4.76trial/s, best loss: 0.0]\n",
      "100%|██████████| 66/66 [00:00<00:00,  4.23trial/s, best loss: 0.0]\n",
      "100%|██████████| 67/67 [00:07<00:00,  7.82s/trial, best loss: 0.0]\n",
      "100%|██████████| 68/68 [00:00<00:00,  5.06trial/s, best loss: 0.0]\n",
      "100%|██████████| 69/69 [00:00<00:00,  1.81trial/s, best loss: 0.0]\n",
      "100%|██████████| 70/70 [00:00<00:00,  2.55trial/s, best loss: 0.0]\n",
      "100%|██████████| 71/71 [00:00<00:00,  1.36trial/s, best loss: 0.0]\n",
      "100%|██████████| 72/72 [00:00<00:00,  1.68trial/s, best loss: 0.0]\n",
      "100%|██████████| 73/73 [00:30<00:00, 30.63s/trial, best loss: 0.0]\n",
      "100%|██████████| 74/74 [00:00<00:00,  1.22trial/s, best loss: 0.0]\n",
      "100%|██████████| 75/75 [00:00<00:00,  3.93trial/s, best loss: 0.0]\n",
      "100%|██████████| 76/76 [00:24<00:00, 24.51s/trial, best loss: 0.0]\n",
      "100%|██████████| 77/77 [00:00<00:00,  2.29trial/s, best loss: 0.0]\n",
      "100%|██████████| 78/78 [00:00<00:00,  2.11trial/s, best loss: 0.0]\n",
      "100%|██████████| 79/79 [00:00<00:00,  1.00trial/s, best loss: 0.0]\n",
      "100%|██████████| 80/80 [00:00<00:00,  1.12trial/s, best loss: 0.0]\n",
      "100%|██████████| 81/81 [00:00<00:00,  1.21trial/s, best loss: 0.0]\n",
      "100%|██████████| 82/82 [00:34<00:00, 34.46s/trial, best loss: 0.0]\n",
      "100%|██████████| 83/83 [00:00<00:00,  4.12trial/s, best loss: 0.0]\n",
      "100%|██████████| 84/84 [00:00<00:00,  1.29trial/s, best loss: 0.0]\n",
      "100%|██████████| 85/85 [00:00<00:00,  2.53trial/s, best loss: 0.0]\n",
      "100%|██████████| 86/86 [00:00<00:00,  1.99trial/s, best loss: 0.0]\n",
      "100%|██████████| 87/87 [00:00<00:00,  5.57trial/s, best loss: 0.0]\n",
      "100%|██████████| 88/88 [00:00<00:00,  4.64trial/s, best loss: 0.0]\n",
      "100%|██████████| 89/89 [00:31<00:00, 31.94s/trial, best loss: 0.0]\n",
      "100%|██████████| 90/90 [00:00<00:00,  1.35trial/s, best loss: 0.0]\n",
      "100%|██████████| 91/91 [00:00<00:00,  4.38trial/s, best loss: 0.0]\n",
      "100%|██████████| 92/92 [00:00<00:00,  2.01trial/s, best loss: 0.0]\n",
      "100%|██████████| 93/93 [00:00<00:00,  2.32trial/s, best loss: 0.0]\n",
      "100%|██████████| 94/94 [00:00<00:00,  5.06trial/s, best loss: 0.0]\n",
      "100%|██████████| 95/95 [00:00<00:00,  1.28trial/s, best loss: 0.0]\n",
      "100%|██████████| 96/96 [00:00<00:00,  3.16trial/s, best loss: 0.0]\n",
      "100%|██████████| 97/97 [00:00<00:00,  5.58trial/s, best loss: 0.0]\n",
      "100%|██████████| 98/98 [00:00<00:00, 11.39trial/s, best loss: 0.0]\n",
      "100%|██████████| 99/99 [00:31<00:00, 31.95s/trial, best loss: 0.0]\n",
      "100%|██████████| 100/100 [00:00<00:00,  4.33trial/s, best loss: 0.0]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: <function xgboost_classification at 0x7fe96f3f00d0> \n",
      "\n",
      "\n",
      "[19:30:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 1/1 [00:53<00:00, 53.28s/trial, best loss: 0.011111111111111072]\n",
      "[19:31:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 2/2 [00:37<00:00, 37.55s/trial, best loss: 0.011111111111111072]\n",
      "[19:31:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 3/3 [00:11<00:00, 11.31s/trial, best loss: 0.011111111111111072]\n",
      "[19:32:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 4/4 [01:00<00:00, 60.09s/trial, best loss: 0.011111111111111072]\n",
      "[19:33:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 5/5 [00:33<00:00, 33.01s/trial, best loss: 0.0037037037037036535]\n",
      "[19:33:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 6/6 [00:19<00:00, 19.49s/trial, best loss: 0.0037037037037036535]\n",
      "[19:33:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 7/7 [00:33<00:00, 33.17s/trial, best loss: 0.0037037037037036535]\n",
      "[19:34:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 8/8 [00:01<00:00,  1.27s/trial, best loss: 0.0037037037037036535]\n",
      "[19:34:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 9/9 [00:50<00:00, 50.85s/trial, best loss: 0.0037037037037036535]\n",
      "[19:35:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 10/10 [00:19<00:00, 19.40s/trial, best loss: 0.0037037037037036535]\n",
      "[19:35:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 11/11 [00:03<00:00,  3.14s/trial, best loss: 0.0037037037037036535]\n",
      "[19:35:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 12/12 [00:29<00:00, 29.61s/trial, best loss: 0.0037037037037036535]\n",
      "[19:36:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 13/13 [00:22<00:00, 22.84s/trial, best loss: 0.0037037037037036535]\n",
      "[19:36:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 14/14 [00:06<00:00,  6.18s/trial, best loss: 0.0037037037037036535]\n",
      "[19:36:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 15/15 [00:30<00:00, 30.17s/trial, best loss: 0.0037037037037036535]\n",
      "[19:37:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 16/16 [01:00<00:00, 60.05s/trial, best loss: 0.0037037037037036535]\n",
      "[19:38:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 17/17 [00:52<00:00, 52.74s/trial, best loss: 0.0037037037037036535]\n",
      "[19:39:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 18/18 [00:06<00:00,  6.97s/trial, best loss: 0.0037037037037036535]\n",
      "[19:39:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 19/19 [00:36<00:00, 36.90s/trial, best loss: 0.0037037037037036535]\n",
      "[19:39:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 20/20 [00:13<00:00, 13.86s/trial, best loss: 0.0037037037037036535]\n",
      "[19:40:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 21/21 [00:51<00:00, 51.45s/trial, best loss: 0.0037037037037036535]\n",
      "[19:40:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 22/22 [00:15<00:00, 15.39s/trial, best loss: 0.0037037037037036535]\n",
      "[19:41:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 23/23 [00:08<00:00,  8.02s/trial, best loss: 0.0037037037037036535]\n",
      "[19:41:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 24/24 [01:00<00:00, 60.06s/trial, best loss: 0.0037037037037036535]\n",
      "[19:42:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 25/25 [00:13<00:00, 13.62s/trial, best loss: 0.0037037037037036535]\n",
      "[19:42:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 26/26 [00:19<00:00, 19.36s/trial, best loss: 0.0037037037037036535]\n",
      "[19:42:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 27/27 [00:27<00:00, 27.53s/trial, best loss: 0.0037037037037036535]\n",
      "[19:43:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 28/28 [01:00<00:00, 60.04s/trial, best loss: 0.0037037037037036535]\n",
      "[19:44:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 29/29 [00:10<00:00, 10.47s/trial, best loss: 0.0037037037037036535]\n",
      "[19:44:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 30/30 [00:07<00:00,  7.29s/trial, best loss: 0.0037037037037036535]\n",
      "[19:44:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 31/31 [00:50<00:00, 50.75s/trial, best loss: 0.0037037037037036535]\n",
      "[19:45:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 32/32 [00:08<00:00,  8.43s/trial, best loss: 0.0037037037037036535]\n",
      "[19:45:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 33/33 [00:04<00:00,  4.40s/trial, best loss: 0.0037037037037036535]\n",
      "[19:45:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 34/34 [00:08<00:00,  8.54s/trial, best loss: 0.0037037037037036535]\n",
      "[19:45:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 35/35 [01:00<00:00, 60.07s/trial, best loss: 0.0037037037037036535]\n",
      "[19:46:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 36/36 [00:26<00:00, 26.41s/trial, best loss: 0.0037037037037036535]\n",
      "[19:47:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 37/37 [00:27<00:00, 27.52s/trial, best loss: 0.0037037037037036535]\n",
      "[19:47:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 38/38 [00:16<00:00, 16.89s/trial, best loss: 0.0037037037037036535]\n",
      "[19:47:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 39/39 [00:01<00:00,  1.77s/trial, best loss: 0.0037037037037036535]\n",
      "[19:47:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 40/40 [01:00<00:00, 60.08s/trial, best loss: 0.0037037037037036535]\n",
      "[19:49:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 41/41 [01:00<00:00, 60.04s/trial, best loss: 0.0037037037037036535]\n",
      "[19:50:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 42/42 [01:00<00:00, 60.06s/trial, best loss: 0.0037037037037036535]\n",
      "[19:51:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 43/43 [00:03<00:00,  3.18s/trial, best loss: 0.0037037037037036535]\n",
      "[19:51:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 44/44 [01:00<00:00, 60.07s/trial, best loss: 0.0037037037037036535]\n",
      "[19:52:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 45/45 [00:07<00:00,  7.51s/trial, best loss: 0.0037037037037036535]\n",
      "[19:52:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 46/46 [00:02<00:00,  2.02s/trial, best loss: 0.0037037037037036535]\n",
      "[19:52:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 47/47 [01:00<00:00, 60.08s/trial, best loss: 0.0037037037037036535]\n",
      "[19:53:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 48/48 [00:05<00:00,  5.13s/trial, best loss: 0.0037037037037036535]\n",
      "[19:53:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 49/49 [00:10<00:00, 10.54s/trial, best loss: 0.0037037037037036535]\n",
      "[19:53:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 50/50 [00:15<00:00, 15.84s/trial, best loss: 0.0037037037037036535]\n",
      "[19:53:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 51/51 [00:06<00:00,  6.34s/trial, best loss: 0.0037037037037036535]\n",
      "[19:53:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 52/52 [00:02<00:00,  2.29s/trial, best loss: 0.0037037037037036535]\n",
      "[19:53:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 53/53 [00:41<00:00, 41.98s/trial, best loss: 0.0037037037037036535]\n",
      "[19:54:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 54/54 [00:24<00:00, 24.14s/trial, best loss: 0.0037037037037036535]\n",
      "[19:54:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 55/55 [00:10<00:00, 10.95s/trial, best loss: 0.0037037037037036535]\n",
      "[19:55:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 56/56 [00:28<00:00, 28.62s/trial, best loss: 0.0037037037037036535]\n",
      "[19:55:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 57/57 [01:00<00:00, 60.07s/trial, best loss: 0.0037037037037036535]\n",
      "[19:56:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 58/58 [00:27<00:00, 27.98s/trial, best loss: 0.0037037037037036535]\n",
      "[19:57:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 59/59 [00:43<00:00, 43.26s/trial, best loss: 0.0037037037037036535]\n",
      "[19:57:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 60/60 [01:00<00:00, 60.07s/trial, best loss: 0.0037037037037036535]\n",
      "[19:58:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 61/61 [01:00<00:00, 60.03s/trial, best loss: 0.0037037037037036535]\n",
      "[19:59:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 62/62 [00:07<00:00,  7.61s/trial, best loss: 0.0037037037037036535]\n",
      "[19:59:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 63/63 [00:10<00:00, 10.57s/trial, best loss: 0.0037037037037036535]\n",
      "[20:00:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 64/64 [00:06<00:00,  6.04s/trial, best loss: 0.0037037037037036535]\n",
      "[20:00:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 65/65 [00:04<00:00,  4.81s/trial, best loss: 0.0037037037037036535]\n",
      "[20:00:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 66/66 [01:00<00:00, 60.08s/trial, best loss: 0.0037037037037036535]\n",
      "[20:01:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 67/67 [00:18<00:00, 18.62s/trial, best loss: 0.0037037037037036535]\n",
      "[20:01:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 68/68 [00:17<00:00, 17.19s/trial, best loss: 0.0037037037037036535]\n",
      "[20:01:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 69/69 [00:11<00:00, 11.89s/trial, best loss: 0.0037037037037036535]\n",
      "[20:02:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 70/70 [01:00<00:00, 60.08s/trial, best loss: 0.0037037037037036535]\n",
      "[20:03:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 71/71 [01:00<00:00, 60.09s/trial, best loss: 0.0037037037037036535]\n",
      "[20:04:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 72/72 [01:00<00:00, 60.06s/trial, best loss: 0.0037037037037036535]\n",
      "[20:05:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 73/73 [01:00<00:00, 60.08s/trial, best loss: 0.0037037037037036535]\n",
      "[20:06:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 74/74 [00:43<00:00, 43.20s/trial, best loss: 0.0037037037037036535]\n",
      "[20:06:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 75/75 [00:48<00:00, 48.82s/trial, best loss: 0.0037037037037036535]\n",
      "[20:07:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 76/76 [00:02<00:00,  2.80s/trial, best loss: 0.0037037037037036535]\n",
      "[20:07:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 77/77 [00:47<00:00, 47.53s/trial, best loss: 0.0037037037037036535]\n",
      "[20:08:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 78/78 [00:02<00:00,  2.01s/trial, best loss: 0.0037037037037036535]\n",
      "[20:08:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 79/79 [01:00<00:00, 60.08s/trial, best loss: 0.0037037037037036535]\n",
      "[20:09:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 80/80 [01:00<00:00, 60.03s/trial, best loss: 0.0037037037037036535]\n",
      "[20:10:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 81/81 [00:11<00:00, 11.74s/trial, best loss: 0.0037037037037036535]\n",
      "[20:10:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 82/82 [00:20<00:00, 20.05s/trial, best loss: 0.0037037037037036535]\n",
      "[20:11:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 83/83 [00:07<00:00,  7.01s/trial, best loss: 0.0037037037037036535]\n",
      "[20:11:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 84/84 [00:13<00:00, 13.94s/trial, best loss: 0.0037037037037036535]\n",
      "[20:11:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 85/85 [00:17<00:00, 17.98s/trial, best loss: 0.0037037037037036535]\n",
      "[20:11:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 86/86 [01:00<00:00, 60.09s/trial, best loss: 0.0037037037037036535]\n",
      "[20:12:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 87/87 [00:31<00:00, 31.00s/trial, best loss: 0.0037037037037036535]\n",
      "[20:13:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 88/88 [00:16<00:00, 16.20s/trial, best loss: 0.0037037037037036535]\n",
      "[20:13:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 89/89 [00:39<00:00, 39.91s/trial, best loss: 0.0037037037037036535]\n",
      "[20:14:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 90/90 [00:20<00:00, 20.08s/trial, best loss: 0.0037037037037036535]\n",
      "[20:14:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 91/91 [00:03<00:00,  3.50s/trial, best loss: 0.0037037037037036535]\n",
      "[20:14:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 92/92 [01:00<00:00, 60.07s/trial, best loss: 0.0037037037037036535]\n",
      "[20:15:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 93/93 [00:41<00:00, 41.44s/trial, best loss: 0.0037037037037036535]\n",
      "[20:16:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 94/94 [00:50<00:00, 50.97s/trial, best loss: 0.0037037037037036535]\n",
      "[20:17:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 95/95 [00:12<00:00, 12.73s/trial, best loss: 0.0037037037037036535]\n",
      "[20:17:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 96/96 [00:46<00:00, 46.35s/trial, best loss: 0.0037037037037036535]\n",
      "[20:18:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 97/97 [00:05<00:00,  5.92s/trial, best loss: 0.0037037037037036535]\n",
      "[20:18:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 98/98 [00:29<00:00, 29.84s/trial, best loss: 0.0037037037037036535]\n",
      "[20:18:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 99/99 [00:12<00:00, 12.20s/trial, best loss: 0.0037037037037036535]\n",
      "[20:18:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 100/100 [00:04<00:00,  4.69s/trial, best loss: 0.0037037037037036535]\n",
      "[20:18:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:19:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: <function decision_tree_classifier at 0x7fe96fc9a040> \n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.73trial/s, best loss: 0.054629629629629584]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.50trial/s, best loss: 0.028703703703703676]\n",
      "100%|██████████| 3/3 [00:00<00:00,  8.48trial/s, best loss: 0.028703703703703676]\n",
      "100%|██████████| 4/4 [00:00<00:00,  1.92trial/s, best loss: 0.025925925925925908]\n",
      "100%|██████████| 5/5 [00:00<00:00,  8.36trial/s, best loss: 0.025925925925925908]\n",
      "100%|██████████| 6/6 [00:00<00:00,  2.07trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 7/7 [00:00<00:00,  1.35trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 8/8 [00:00<00:00,  8.35trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 9/9 [00:00<00:00, 13.47trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.93trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 11/11 [00:00<00:00, 24.51trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 12/12 [00:00<00:00,  7.25trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 13/13 [00:00<00:00, 12.21trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 14/14 [00:00<00:00,  3.09trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 15/15 [00:00<00:00,  9.37trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 16/16 [00:00<00:00, 12.19trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 17/17 [00:00<00:00, 15.93trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 18/18 [00:00<00:00,  4.59trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 19/19 [00:00<00:00,  6.73trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 20/20 [00:00<00:00, 13.94trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 21/21 [00:00<00:00,  4.67trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 22/22 [00:00<00:00,  8.55trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 23/23 [00:00<00:00, 10.29trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 24/24 [00:00<00:00,  1.41trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 25/25 [00:00<00:00, 11.65trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 26/26 [00:00<00:00,  4.22trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 27/27 [00:00<00:00, 10.09trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 28/28 [00:00<00:00,  3.88trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 29/29 [00:00<00:00,  8.59trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 30/30 [00:00<00:00, 14.38trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 31/31 [00:00<00:00, 11.70trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 32/32 [00:00<00:00,  6.39trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 33/33 [00:00<00:00,  3.62trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 34/34 [00:00<00:00, 11.88trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 35/35 [00:00<00:00, 17.37trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 36/36 [00:00<00:00, 10.23trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 37/37 [00:00<00:00,  9.25trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 38/38 [00:00<00:00, 13.62trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 39/39 [00:00<00:00,  8.93trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 40/40 [00:00<00:00, 25.38trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 41/41 [00:00<00:00, 13.19trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 42/42 [00:00<00:00, 18.11trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 43/43 [00:00<00:00,  2.05trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 44/44 [00:00<00:00,  4.15trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 45/45 [00:00<00:00,  8.08trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 46/46 [00:00<00:00, 19.64trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 47/47 [00:00<00:00, 15.10trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 48/48 [00:00<00:00,  4.81trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 49/49 [00:00<00:00,  7.91trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 50/50 [00:00<00:00, 13.48trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 51/51 [00:00<00:00, 10.92trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 52/52 [00:00<00:00, 20.62trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 53/53 [00:00<00:00, 10.82trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 54/54 [00:00<00:00, 18.56trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 55/55 [00:00<00:00, 13.26trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 56/56 [00:00<00:00,  8.11trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 57/57 [00:00<00:00,  2.37trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 58/58 [00:00<00:00, 10.72trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 59/59 [00:00<00:00,  5.34trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 60/60 [00:00<00:00, 11.35trial/s, best loss: 0.02314814814814814]\n",
      "100%|██████████| 61/61 [00:00<00:00,  1.95trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 62/62 [00:00<00:00, 14.36trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 63/63 [00:00<00:00, 13.14trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 64/64 [00:00<00:00,  5.13trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 65/65 [00:00<00:00,  5.33trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 66/66 [00:00<00:00, 10.40trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 67/67 [00:00<00:00,  2.44trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 68/68 [00:00<00:00,  2.37trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 69/69 [00:00<00:00,  8.19trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 70/70 [00:00<00:00, 12.24trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 71/71 [00:00<00:00,  1.75trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 72/72 [00:00<00:00, 18.27trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 73/73 [00:00<00:00, 16.84trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 74/74 [00:00<00:00,  7.35trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 75/75 [00:00<00:00, 13.60trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 76/76 [00:00<00:00, 16.22trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 77/77 [00:00<00:00,  3.96trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 78/78 [00:00<00:00, 14.77trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 79/79 [00:00<00:00, 15.15trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 80/80 [00:00<00:00, 16.78trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 81/81 [00:00<00:00,  6.47trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 82/82 [00:00<00:00, 15.11trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 83/83 [00:00<00:00, 17.46trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 84/84 [00:00<00:00,  2.24trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 85/85 [00:00<00:00,  3.42trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 86/86 [00:00<00:00,  8.82trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 87/87 [00:00<00:00, 16.16trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 88/88 [00:00<00:00, 16.25trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 89/89 [00:00<00:00, 10.95trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 90/90 [00:00<00:00, 14.06trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 91/91 [00:00<00:00, 11.84trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 92/92 [00:00<00:00, 10.29trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 93/93 [00:00<00:00, 16.86trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 94/94 [00:00<00:00,  3.62trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 95/95 [00:00<00:00,  8.55trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 96/96 [00:00<00:00, 14.99trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 97/97 [00:00<00:00,  3.95trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 98/98 [00:00<00:00,  2.92trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 99/99 [00:00<00:00,  3.21trial/s, best loss: 0.019444444444444486]\n",
      "100%|██████████| 100/100 [00:00<00:00,  2.28trial/s, best loss: 0.019444444444444486]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: <function extra_trees_classifier at 0x7fe96fdfbc10> \n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.79trial/s, best loss: 0.012037037037037068]\n",
      "100%|██████████| 2/2 [00:00<00:00,  1.07trial/s, best loss: 0.012037037037037068]\n",
      "100%|██████████| 3/3 [00:00<00:00,  1.01trial/s, best loss: 0.012037037037037068]\n",
      "100%|██████████| 4/4 [00:00<00:00,  1.76trial/s, best loss: 0.012037037037037068]\n",
      "100%|██████████| 5/5 [00:00<00:00,  2.29trial/s, best loss: 0.012037037037037068]\n",
      "100%|██████████| 6/6 [00:00<00:00,  1.22trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 7/7 [00:04<00:00,  4.72s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 8/8 [00:00<00:00,  6.45trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 9/9 [00:00<00:00,  4.82trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 10/10 [00:00<00:00,  6.24trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 11/11 [00:00<00:00,  1.21trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 12/12 [00:17<00:00, 17.41s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 13/13 [00:01<00:00,  1.82s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 14/14 [00:00<00:00,  2.68trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 15/15 [00:05<00:00,  5.15s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 16/16 [00:02<00:00,  2.59s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 17/17 [00:02<00:00,  2.61s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 18/18 [00:05<00:00,  5.65s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 19/19 [00:00<00:00,  2.17trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 20/20 [00:05<00:00,  5.85s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 21/21 [00:00<00:00,  2.28trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 22/22 [00:00<00:00,  3.19trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 23/23 [00:21<00:00, 21.85s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 24/24 [00:04<00:00,  4.11s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 25/25 [00:00<00:00,  2.63trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 26/26 [00:02<00:00,  2.57s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 27/27 [00:02<00:00,  2.09s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 28/28 [00:01<00:00,  1.11s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 29/29 [00:03<00:00,  3.08s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 30/30 [00:04<00:00,  4.32s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 31/31 [00:00<00:00,  1.33trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 32/32 [00:07<00:00,  7.01s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 33/33 [00:00<00:00,  4.83trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 34/34 [00:07<00:00,  7.33s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 35/35 [00:00<00:00,  2.29trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 36/36 [00:06<00:00,  6.70s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 37/37 [00:00<00:00,  4.28trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 38/38 [00:05<00:00,  5.92s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 39/39 [00:00<00:00,  4.61trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 40/40 [00:04<00:00,  4.69s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 41/41 [00:10<00:00, 10.43s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 42/42 [00:04<00:00,  4.56s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 43/43 [00:00<00:00,  4.84trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 44/44 [00:00<00:00,  8.64trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 45/45 [00:04<00:00,  4.16s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 46/46 [00:03<00:00,  3.51s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 47/47 [00:00<00:00,  1.88trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 48/48 [00:00<00:00,  1.63trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 49/49 [00:08<00:00,  8.47s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 50/50 [00:01<00:00,  1.72s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 51/51 [00:02<00:00,  2.27s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 52/52 [00:05<00:00,  5.53s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 53/53 [00:00<00:00,  3.04trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 54/54 [00:01<00:00,  1.22s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 55/55 [00:00<00:00,  2.12trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 56/56 [00:33<00:00, 33.07s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 57/57 [00:00<00:00,  9.20trial/s, best loss: 0.00462962962962965]\n",
      "100%|██████████| 58/58 [00:02<00:00,  2.69s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 59/59 [00:00<00:00,  4.54trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 60/60 [00:05<00:00,  5.22s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 61/61 [00:13<00:00, 13.03s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 62/62 [00:06<00:00,  6.74s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 63/63 [00:00<00:00,  3.83trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 64/64 [00:10<00:00, 10.84s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 65/65 [00:08<00:00,  8.95s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 66/66 [00:03<00:00,  3.88s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 67/67 [00:01<00:00,  1.72s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 68/68 [00:00<00:00,  1.62trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 69/69 [00:11<00:00, 11.69s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 70/70 [00:01<00:00,  1.39s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 71/71 [00:00<00:00,  1.54trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 72/72 [00:00<00:00,  1.03trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 73/73 [00:00<00:00,  6.17trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 74/74 [00:05<00:00,  5.31s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 75/75 [00:00<00:00,  4.34trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 76/76 [00:01<00:00,  1.28s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 77/77 [00:01<00:00,  1.37s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 78/78 [00:00<00:00,  5.02trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 79/79 [00:00<00:00,  2.20trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 80/80 [00:04<00:00,  4.63s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 81/81 [00:00<00:00, 11.59trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 82/82 [00:00<00:00,  1.56trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 83/83 [00:04<00:00,  4.43s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 84/84 [00:00<00:00,  1.89trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 85/85 [00:00<00:00,  1.18trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 86/86 [00:00<00:00,  1.26trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 87/87 [00:09<00:00,  9.18s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 88/88 [00:02<00:00,  2.76s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 89/89 [00:01<00:00,  1.24s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 90/90 [00:00<00:00,  4.46trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 91/91 [00:02<00:00,  2.81s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 92/92 [00:01<00:00,  1.37s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 93/93 [00:03<00:00,  3.72s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 94/94 [00:05<00:00,  5.95s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 95/95 [00:01<00:00,  1.98s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 96/96 [00:09<00:00,  9.79s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 97/97 [00:00<00:00,  6.20trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 98/98 [00:00<00:00,  2.47trial/s, best loss: 0.002777777777777768]\n",
      "100%|██████████| 99/99 [00:07<00:00,  7.42s/trial, best loss: 0.002777777777777768]\n",
      "100%|██████████| 100/100 [00:00<00:00,  2.95trial/s, best loss: 0.002777777777777768]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from hpsklearn import HyperoptEstimator, lightgbm_classification, k_neighbors_classifier, xgboost_classification, decision_tree_classifier, extra_trees_classifier\n",
    "\n",
    "def hyperopt_class(classifier, max_evals=5):\n",
    "\n",
    "    name = str(classifier)\n",
    "\n",
    "    estim = HyperoptEstimator(classifier=classifier(name=name),max_evals=max_evals,trial_timeout=60)\n",
    "\n",
    "    estim.fit(X_train, y_train)\n",
    "\n",
    "    best_model = estim.best_model()['learner']\n",
    "    best_model.fit(X_train, y_train)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    return {'classifier': classifier, 'best_params': estim.best_model()['learner'], 'f1_score': f1_score(y_test, y_pred), 'acc': accuracy_score(y_test, y_pred)}\n",
    "\n",
    "\n",
    "models = [lightgbm_classification, k_neighbors_classifier, xgboost_classification, decision_tree_classifier, extra_trees_classifier]\n",
    "\n",
    "output = []\n",
    "\n",
    "for model in models:\n",
    "    print(f'Model: {model} \\n\\n')\n",
    "    output.append(hyperopt_class(model, max_evals=100))\n",
    "    print('\\n\\n\\n\\n\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier': <function lightgbm_classification at 0x7fe96f3c1310>, 'best_params': LGBMClassifier(colsample_bytree=0.6207269777244252,\n",
      "               learning_rate=0.0029474866844230833, max_delta_step=0,\n",
      "               max_depth=8, min_child_weight=5, n_estimators=5800, n_jobs=1,\n",
      "               num_leaves=5, objective='binary', reg_alpha=0.7346299827889231,\n",
      "               reg_lambda=1.5451858945439936, scale_pos_weight=1, seed=3,\n",
      "               subsample=0.9812624101121081), 'f1_score': 0.9955423476968797, 'acc': 0.9955555555555555}\n",
      "\n",
      "\n",
      "\n",
      "{'classifier': <function k_neighbors_classifier at 0x7fe96f957a60>, 'best_params': KNeighborsClassifier(leaf_size=34, metric='l2', n_jobs=1, n_neighbors=2,\n",
      "                     p=3.792056314260763, weights='distance'), 'f1_score': 0.9880059970014992, 'acc': 0.9881481481481481}\n",
      "\n",
      "\n",
      "\n",
      "{'classifier': <function xgboost_classification at 0x7fe96f3f00d0>, 'best_params': XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "              colsample_bylevel=0.6857742853862165, colsample_bynode=1,\n",
      "              colsample_bytree=0.9137232779724352, enable_categorical=False,\n",
      "              gamma=0.003952066639286737, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.10043257432266611,\n",
      "              max_delta_step=0, max_depth=4, min_child_weight=2, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=4800, n_jobs=1,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0.016104614017919332, reg_lambda=2.0568240600018446,\n",
      "              scale_pos_weight=1, seed=0, subsample=0.92652933484954,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None), 'f1_score': 0.9940387481371087, 'acc': 0.9940740740740741}\n",
      "\n",
      "\n",
      "\n",
      "{'classifier': <function decision_tree_classifier at 0x7fe96fc9a040>, 'best_params': DecisionTreeClassifier(criterion='entropy', max_features=0.7138163167651351,\n",
      "                       random_state=4), 'f1_score': 0.9768137621540762, 'acc': 0.977037037037037}\n",
      "\n",
      "\n",
      "\n",
      "{'classifier': <function extra_trees_classifier at 0x7fe96fdfbc10>, 'best_params': ExtraTreesClassifier(class_weight='balanced', criterion='entropy',\n",
      "                     max_features=0.721987073412862, n_estimators=118, n_jobs=1,\n",
      "                     random_state=3, verbose=False), 'f1_score': 0.9925373134328358, 'acc': 0.9925925925925926}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in output:\n",
    "    print(f\"{i}\\n\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.08s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 2/2 [00:21<00:00, 21.72s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 3/3 [00:12<00:00, 12.81s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 4/4 [00:03<00:00,  3.80s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 5/5 [00:17<00:00, 17.01s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 6/6 [00:02<00:00,  2.51s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 7/7 [00:04<00:00,  4.28s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 8/8 [00:34<00:00, 34.68s/trial, best loss: 0.00462962962962965]\n",
      "100%|██████████| 9/9 [00:01<00:00,  1.82s/trial, best loss: 0.0]\n",
      "100%|██████████| 10/10 [00:01<00:00,  1.44s/trial, best loss: 0.0]\n",
      "100%|██████████| 11/11 [00:04<00:00,  4.62s/trial, best loss: 0.0]\n",
      "100%|██████████| 12/12 [00:00<00:00,  1.70trial/s, best loss: 0.0]\n",
      "100%|██████████| 13/13 [00:04<00:00,  4.48s/trial, best loss: 0.0]\n",
      "100%|██████████| 14/14 [00:00<00:00,  1.42trial/s, best loss: 0.0]\n",
      "100%|██████████| 15/15 [00:00<00:00,  1.06trial/s, best loss: 0.0]\n",
      "100%|██████████| 16/16 [00:01<00:00,  1.31s/trial, best loss: 0.0]\n",
      "100%|██████████| 17/17 [00:01<00:00,  1.97s/trial, best loss: 0.0]\n",
      "100%|██████████| 18/18 [00:21<00:00, 21.36s/trial, best loss: 0.0]\n",
      "100%|██████████| 19/19 [00:02<00:00,  2.84s/trial, best loss: 0.0]\n",
      "100%|██████████| 20/20 [00:11<00:00, 11.97s/trial, best loss: 0.0]\n",
      "100%|██████████| 21/21 [00:01<00:00,  1.17s/trial, best loss: 0.0]\n",
      "100%|██████████| 22/22 [00:07<00:00,  7.66s/trial, best loss: 0.0]\n",
      "100%|██████████| 23/23 [00:02<00:00,  2.03s/trial, best loss: 0.0]\n",
      "100%|██████████| 24/24 [00:00<00:00,  1.04trial/s, best loss: 0.0]\n",
      "100%|██████████| 25/25 [00:01<00:00,  1.84s/trial, best loss: 0.0]\n",
      "100%|██████████| 26/26 [00:00<00:00,  2.88trial/s, best loss: 0.0]\n",
      "100%|██████████| 27/27 [00:00<00:00,  1.17trial/s, best loss: 0.0]\n",
      "100%|██████████| 28/28 [00:37<00:00, 37.54s/trial, best loss: 0.0]\n",
      "100%|██████████| 29/29 [00:04<00:00,  4.38s/trial, best loss: 0.0]\n",
      "100%|██████████| 30/30 [00:07<00:00,  7.72s/trial, best loss: 0.0]\n",
      "100%|██████████| 31/31 [00:23<00:00, 23.04s/trial, best loss: 0.0]\n",
      "100%|██████████| 32/32 [00:10<00:00, 10.96s/trial, best loss: 0.0]\n",
      "100%|██████████| 33/33 [00:02<00:00,  2.68s/trial, best loss: 0.0]\n",
      "100%|██████████| 34/34 [00:02<00:00,  2.85s/trial, best loss: 0.0]\n",
      "100%|██████████| 35/35 [00:01<00:00,  1.39s/trial, best loss: 0.0]\n",
      "100%|██████████| 36/36 [00:38<00:00, 38.69s/trial, best loss: 0.0]\n",
      "100%|██████████| 37/37 [00:03<00:00,  3.81s/trial, best loss: 0.0]\n",
      "100%|██████████| 38/38 [00:03<00:00,  3.80s/trial, best loss: 0.0]\n",
      "100%|██████████| 39/39 [00:07<00:00,  7.14s/trial, best loss: 0.0]\n",
      "100%|██████████| 40/40 [00:38<00:00, 38.57s/trial, best loss: 0.0]\n",
      "100%|██████████| 41/41 [00:30<00:00, 30.43s/trial, best loss: 0.0]\n",
      "100%|██████████| 42/42 [00:01<00:00,  1.05s/trial, best loss: 0.0]\n",
      "100%|██████████| 43/43 [00:04<00:00,  4.09s/trial, best loss: 0.0]\n",
      "100%|██████████| 44/44 [00:17<00:00, 17.70s/trial, best loss: 0.0]\n",
      "100%|██████████| 45/45 [00:02<00:00,  2.36s/trial, best loss: 0.0]\n",
      "100%|██████████| 46/46 [00:17<00:00, 17.58s/trial, best loss: 0.0]\n",
      "100%|██████████| 47/47 [00:09<00:00,  9.30s/trial, best loss: 0.0]\n",
      "100%|██████████| 48/48 [00:03<00:00,  3.10s/trial, best loss: 0.0]\n",
      "100%|██████████| 49/49 [00:01<00:00,  1.29s/trial, best loss: 0.0]\n",
      "100%|██████████| 50/50 [00:01<00:00,  1.53s/trial, best loss: 0.0]\n",
      "100%|██████████| 51/51 [00:44<00:00, 44.70s/trial, best loss: 0.0]\n",
      "100%|██████████| 52/52 [00:06<00:00,  6.20s/trial, best loss: 0.0]\n",
      "100%|██████████| 53/53 [00:03<00:00,  3.47s/trial, best loss: 0.0]\n",
      "100%|██████████| 54/54 [00:02<00:00,  2.04s/trial, best loss: 0.0]\n",
      "100%|██████████| 55/55 [00:47<00:00, 47.73s/trial, best loss: 0.0]\n",
      "100%|██████████| 56/56 [00:06<00:00,  6.07s/trial, best loss: 0.0]\n",
      "100%|██████████| 57/57 [00:10<00:00, 10.68s/trial, best loss: 0.0]\n",
      "100%|██████████| 58/58 [00:00<00:00,  1.03trial/s, best loss: 0.0]\n",
      "100%|██████████| 59/59 [00:00<00:00,  1.19trial/s, best loss: 0.0]\n",
      "100%|██████████| 60/60 [00:18<00:00, 18.20s/trial, best loss: 0.0]\n",
      "0.9955621301775147\n"
     ]
    }
   ],
   "source": [
    "from hpsklearn import HyperoptEstimator, lightgbm_classification\n",
    "\n",
    "estim = HyperoptEstimator(classifier=lightgbm_classification('wdad'), max_evals=60, trial_timeout=60)\n",
    "\n",
    "estim.fit(X_train, y_train)\n",
    "\n",
    "print(f1_score(y_test, estim.predict(X_test)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "LGBMClassifier(colsample_bytree=0.5860119241469383,\n               learning_rate=0.04150111261311509, max_delta_step=0, max_depth=9,\n               min_child_weight=2, n_estimators=1000, n_jobs=1, num_leaves=73,\n               objective='binary', reg_alpha=0.030939233306764107,\n               reg_lambda=1.9219756930633642, scale_pos_weight=1, seed=3,\n               subsample=0.9991674651501217)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estim.best_model()['learner']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.997779422649889\n"
     ]
    }
   ],
   "source": [
    "lgbm = LGBMClassifier(colsample_bytree=0.5860119241469383,\n",
    "                learning_rate=0.04150111261311509, max_delta_step=0, max_depth=9,\n",
    "                min_child_weight=2, n_estimators=1000, n_jobs=1, num_leaves=73,\n",
    "                objective='binary', reg_alpha=0.030939233306764107,\n",
    "                reg_lambda=1.9219756930633642, scale_pos_weight=1, seed=3,\n",
    "                subsample=0.9991674651501217)\n",
    "lgbm.fit(X_train, y_train)\n",
    "y_pred = lgbm.predict(X_test)\n",
    "score = f1_score(y_test, y_pred)\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6822383559158721, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6822383559158721\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.2723230225751747, subsample=1.0 will be ignored. Current value: bagging_fraction=1.2723230225751747\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6822383559158721, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6822383559158721\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.2723230225751747, subsample=1.0 will be ignored. Current value: bagging_fraction=1.2723230225751747\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6822383559158721, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6822383559158721\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.2723230225751747, subsample=1.0 will be ignored. Current value: bagging_fraction=1.2723230225751747\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6822383559158721, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6822383559158721\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.2723230225751747, subsample=1.0 will be ignored. Current value: bagging_fraction=1.2723230225751747\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6822383559158721, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6822383559158721\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.2723230225751747, subsample=1.0 will be ignored. Current value: bagging_fraction=1.2723230225751747\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.123986189785354, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.123986189785354\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7773940599434842, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7773940599434842\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.123986189785354, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.123986189785354\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7773940599434842, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7773940599434842\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.123986189785354, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.123986189785354\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7773940599434842, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7773940599434842\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.123986189785354, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.123986189785354\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7773940599434842, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7773940599434842\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.123986189785354, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.123986189785354\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7773940599434842, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7773940599434842\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5552655300126049, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5552655300126049\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9372669894012043, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9372669894012043\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5552655300126049, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5552655300126049\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9372669894012043, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9372669894012043\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5552655300126049, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5552655300126049\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9372669894012043, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9372669894012043\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5552655300126049, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5552655300126049\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9372669894012043, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9372669894012043\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5552655300126049, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5552655300126049\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9372669894012043, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9372669894012043\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.1775951011858778, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.1775951011858778\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.1582404027370377, subsample=1.0 will be ignored. Current value: bagging_fraction=1.1582404027370377\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.1775951011858778, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.1775951011858778\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.1582404027370377, subsample=1.0 will be ignored. Current value: bagging_fraction=1.1582404027370377\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.1775951011858778, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.1775951011858778\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.1582404027370377, subsample=1.0 will be ignored. Current value: bagging_fraction=1.1582404027370377\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.1775951011858778, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.1775951011858778\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.1582404027370377, subsample=1.0 will be ignored. Current value: bagging_fraction=1.1582404027370377\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.1775951011858778, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.1775951011858778\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.1582404027370377, subsample=1.0 will be ignored. Current value: bagging_fraction=1.1582404027370377\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8144998404119062, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8144998404119062\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.2150757106748675, subsample=1.0 will be ignored. Current value: bagging_fraction=1.2150757106748675\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8144998404119062, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8144998404119062\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.2150757106748675, subsample=1.0 will be ignored. Current value: bagging_fraction=1.2150757106748675\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8144998404119062, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8144998404119062\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.2150757106748675, subsample=1.0 will be ignored. Current value: bagging_fraction=1.2150757106748675\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8144998404119062, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8144998404119062\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.2150757106748675, subsample=1.0 will be ignored. Current value: bagging_fraction=1.2150757106748675\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8144998404119062, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8144998404119062\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.2150757106748675, subsample=1.0 will be ignored. Current value: bagging_fraction=1.2150757106748675\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.1962740756235506, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.1962740756235506\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.153889832356085, subsample=1.0 will be ignored. Current value: bagging_fraction=1.153889832356085\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.1962740756235506, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.1962740756235506\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.153889832356085, subsample=1.0 will be ignored. Current value: bagging_fraction=1.153889832356085\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.1962740756235506, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.1962740756235506\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.153889832356085, subsample=1.0 will be ignored. Current value: bagging_fraction=1.153889832356085\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.1962740756235506, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.1962740756235506\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.153889832356085, subsample=1.0 will be ignored. Current value: bagging_fraction=1.153889832356085\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.1962740756235506, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.1962740756235506\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.153889832356085, subsample=1.0 will be ignored. Current value: bagging_fraction=1.153889832356085\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.155762815176661, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.155762815176661\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6284938489690644, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6284938489690644\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.155762815176661, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.155762815176661\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6284938489690644, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6284938489690644\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.155762815176661, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.155762815176661\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6284938489690644, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6284938489690644\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.155762815176661, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.155762815176661\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6284938489690644, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6284938489690644\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.155762815176661, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.155762815176661\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6284938489690644, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6284938489690644\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.2779683336641057, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.2779683336641057\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8056763802429538, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8056763802429538\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.2779683336641057, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.2779683336641057\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8056763802429538, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8056763802429538\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.2779683336641057, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.2779683336641057\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8056763802429538, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8056763802429538\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.2779683336641057, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.2779683336641057\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8056763802429538, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8056763802429538\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.2779683336641057, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.2779683336641057\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8056763802429538, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8056763802429538\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.000816908244937, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.000816908244937\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6195369069380916, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6195369069380916\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.000816908244937, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.000816908244937\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6195369069380916, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6195369069380916\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.000816908244937, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.000816908244937\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6195369069380916, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6195369069380916\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.000816908244937, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.000816908244937\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6195369069380916, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6195369069380916\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.000816908244937, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.000816908244937\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6195369069380916, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6195369069380916\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0062626730233108, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0062626730233108\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0498877731543903, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0498877731543903\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0062626730233108, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0062626730233108\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0498877731543903, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0498877731543903\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0062626730233108, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0062626730233108\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0498877731543903, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0498877731543903\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0062626730233108, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0062626730233108\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0498877731543903, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0498877731543903\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0062626730233108, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0062626730233108\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0498877731543903, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0498877731543903\n",
      "[LightGBM] [Warning] Unknown parameter: bagging_frequency\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5552655300126049, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5552655300126049\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9372669894012043, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9372669894012043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "45 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/lightgbm/sklearn.py\", line 890, in fit\n",
      "    super().fit(X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets,\n",
      "  File \"/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/lightgbm/sklearn.py\", line 683, in fit\n",
      "    self._Booster = train(params, train_set,\n",
      "  File \"/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/lightgbm/engine.py\", line 228, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/lightgbm/basic.py\", line 2229, in __init__\n",
      "    train_set.construct()\n",
      "  File \"/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/lightgbm/basic.py\", line 1468, in construct\n",
      "    self._lazy_init(self.data, label=self.label,\n",
      "  File \"/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/lightgbm/basic.py\", line 1270, in _lazy_init\n",
      "    self.__init_from_np2d(data, params_str, ref_dataset)\n",
      "  File \"/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/lightgbm/basic.py\", line 1312, in __init_from_np2d\n",
      "    _safe_call(_LIB.LGBM_DatasetCreateFromMat(\n",
      "  File \"/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/lightgbm/basic.py\", line 110, in _safe_call\n",
      "    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))\n",
      "lightgbm.basic.LightGBMError: Check failed: (bagging_fraction) <= (1.0) at /tmp/pip-req-build-e9e4uj3x/compile/src/io/config_auto.cpp, line 347 .\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/lightgbm/sklearn.py\", line 890, in fit\n",
      "    super().fit(X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets,\n",
      "  File \"/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/lightgbm/sklearn.py\", line 683, in fit\n",
      "    self._Booster = train(params, train_set,\n",
      "  File \"/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/lightgbm/engine.py\", line 228, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/lightgbm/basic.py\", line 2229, in __init__\n",
      "    train_set.construct()\n",
      "  File \"/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/lightgbm/basic.py\", line 1468, in construct\n",
      "    self._lazy_init(self.data, label=self.label,\n",
      "  File \"/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/lightgbm/basic.py\", line 1270, in _lazy_init\n",
      "    self.__init_from_np2d(data, params_str, ref_dataset)\n",
      "  File \"/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/lightgbm/basic.py\", line 1312, in __init_from_np2d\n",
      "    _safe_call(_LIB.LGBM_DatasetCreateFromMat(\n",
      "  File \"/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/lightgbm/basic.py\", line 110, in _safe_call\n",
      "    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))\n",
      "lightgbm.basic.LightGBMError: Check failed: (feature_fraction) <= (1.0) at /tmp/pip-req-build-e9e4uj3x/compile/src/io/config_auto.cpp, line 363 .\n",
      "\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.99649188        nan        nan        nan\n",
      "        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score reached: 0.9964918843721454 with params: {'bagging_fraction': 0.9372669894012043, 'bagging_frequency': 5, 'feature_fraction': 0.5552655300126049, 'max_depth': 9} \n"
     ]
    }
   ],
   "source": [
    "#classifiers = [CatBoostClassifier(), XGBClassifier(), LGBMClassifier()]\n",
    "\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "from  hyperopt import hp\n",
    "\n",
    "ctb_reg_params = {\n",
    "    'learning_rate':     hp.choice('learning_rate',     np.arange(0.05, 1, 0.05)),\n",
    "    'max_depth':         hp.choice('max_depth',         np.arange(5, 16, 1, dtype=int)),\n",
    "    'colsample_bylevel': hp.choice('colsample_bylevel', np.arange(0.3, 0.8, 0.1)),\n",
    "    'n_estimators':      hp.choice('n_estimators', np.arange(100, 2000, 100)),\n",
    "    'eval_metric':       hp.choice('eval_metric', ['RMSE', 'MAPE', 'MAE']),\n",
    "    'od_wait': 5 # overfitting detector\n",
    "}\n",
    "\n",
    "# Initialize a RandomizedSearchCV object using 5-fold CV-\n",
    "rs_cv = RandomizedSearchCV(scoring='f1', estimator=LGBMClassifier(), param_distributions=rs_params, cv = 5, n_iter=10)\n",
    "rs_cv.fit(X_train, y_train)\n",
    "print('Best score reached: {} with params: {} '.format(rs_cv.best_score_, rs_cv.best_params_))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:33:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 1/1 [00:46<00:00, 46.91s/trial, best loss: 0.011126373626373587]\n",
      "[17:34:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 2/2 [00:16<00:00, 16.51s/trial, best loss: 0.011126373626373587]\n",
      "[17:34:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 3/3 [00:18<00:00, 18.54s/trial, best loss: 0.007417582417582391]\n",
      "[17:34:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 4/4 [00:04<00:00,  4.03s/trial, best loss: 0.007417582417582391]\n",
      "100%|██████████| 5/5 [01:00<00:00, 60.09s/trial, best loss: 0.007417582417582391]\n",
      "[17:35:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 6/6 [00:09<00:00,  9.82s/trial, best loss: 0.007417582417582391]\n",
      "[17:36:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 7/7 [01:00<00:00, 60.07s/trial, best loss: 0.007417582417582391]\n",
      "[17:37:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 8/8 [00:20<00:00, 20.16s/trial, best loss: 0.007417582417582391]\n",
      "[17:37:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 9/9 [00:06<00:00,  6.61s/trial, best loss: 0.007417582417582391]\n",
      "[17:37:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 10/10 [00:16<00:00, 16.62s/trial, best loss: 0.007417582417582391]\n",
      "[17:37:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 11/11 [00:13<00:00, 13.51s/trial, best loss: 0.007417582417582391]\n",
      "[17:38:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 12/12 [00:55<00:00, 55.33s/trial, best loss: 0.007417582417582391]\n",
      "100%|██████████| 13/13 [01:00<00:00, 60.09s/trial, best loss: 0.007417582417582391]\n",
      "[17:39:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 14/14 [01:00<00:00, 60.07s/trial, best loss: 0.007417582417582391]\n",
      "100%|██████████| 15/15 [01:00<00:00, 60.02s/trial, best loss: 0.007417582417582391]\n",
      "[17:41:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 16/16 [00:06<00:00,  6.10s/trial, best loss: 0.007417582417582391]\n",
      "100%|██████████| 17/17 [01:00<00:00, 60.09s/trial, best loss: 0.007417582417582391]\n",
      "[17:43:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 18/18 [00:01<00:00,  1.62s/trial, best loss: 0.007417582417582391]\n",
      "[17:43:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 19/19 [00:12<00:00, 12.97s/trial, best loss: 0.007417582417582391]\n",
      "[17:43:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 20/20 [00:06<00:00,  6.42s/trial, best loss: 0.007417582417582391]\n",
      "100%|██████████| 21/21 [01:00<00:00, 60.09s/trial, best loss: 0.007417582417582391]\n",
      "100%|██████████| 22/22 [01:00<00:00, 60.08s/trial, best loss: 0.007417582417582391]\n",
      "[17:45:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 23/23 [01:00<00:00, 60.08s/trial, best loss: 0.007417582417582391]\n",
      "[17:46:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 24/24 [00:28<00:00, 28.23s/trial, best loss: 0.007417582417582391]\n",
      "100%|██████████| 25/25 [01:00<00:00, 60.09s/trial, best loss: 0.007417582417582391]\n",
      "100%|██████████| 26/26 [01:00<00:00, 60.06s/trial, best loss: 0.007417582417582391]\n",
      "100%|██████████| 27/27 [01:00<00:00, 60.09s/trial, best loss: 0.007417582417582391]\n",
      "[17:49:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 28/28 [00:09<00:00,  9.42s/trial, best loss: 0.007417582417582391]\n",
      "[17:50:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 29/29 [00:25<00:00, 25.19s/trial, best loss: 0.007417582417582391]\n",
      "[17:50:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 30/30 [00:03<00:00,  3.11s/trial, best loss: 0.007417582417582391]\n",
      "[17:50:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 31/31 [00:13<00:00, 13.56s/trial, best loss: 0.007417582417582391]\n",
      "[17:50:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 32/32 [00:38<00:00, 38.59s/trial, best loss: 0.007417582417582391]\n",
      "[17:51:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 33/33 [01:00<00:00, 60.08s/trial, best loss: 0.007417582417582391]\n",
      "[17:52:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 34/34 [00:06<00:00,  6.67s/trial, best loss: 0.007417582417582391]\n",
      "[17:52:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 35/35 [00:18<00:00, 18.59s/trial, best loss: 0.007417582417582391]\n",
      "[17:52:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 36/36 [00:01<00:00,  1.32s/trial, best loss: 0.007417582417582391]\n",
      "[17:52:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 37/37 [01:00<00:00, 60.07s/trial, best loss: 0.007417582417582391]\n",
      "[17:53:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 38/38 [01:00<00:00, 60.08s/trial, best loss: 0.007417582417582391]\n",
      "[17:54:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 39/39 [01:00<00:00, 60.08s/trial, best loss: 0.007417582417582391]\n",
      "100%|██████████| 40/40 [01:00<00:00, 60.09s/trial, best loss: 0.007417582417582391]\n",
      "[17:56:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 41/41 [01:00<00:00, 60.08s/trial, best loss: 0.007417582417582391]\n",
      "[17:57:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 42/42 [00:36<00:00, 36.73s/trial, best loss: 0.007417582417582391]\n",
      "100%|██████████| 43/43 [01:00<00:00, 60.08s/trial, best loss: 0.007417582417582391]\n",
      "[17:59:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      " 98%|█████████▊| 43/44 [00:54<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-221:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/michal/anaconda3/envs/CDV/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "\n",
      "  File \"/home/michal/anaconda3/envs/CDV/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "\n",
      "  File \"/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/hpsklearn/estimator/_cost_fn.py\", line 191, in _cost_fn\n",
      "    learner.fit(XEXfit, yfit)\n",
      "\n",
      "  File \"/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/xgboost/core.py\", line 506, in inner_f\n",
      "    return f(**kwargs)\n",
      "\n",
      "  File \"/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/xgboost/sklearn.py\", line 1250, in fit\n",
      "    self._Booster = train(\n",
      "\n",
      "  File \"/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/xgboost/training.py\", line 188, in train\n",
      "    bst = _train_internal(params, dtrain,\n",
      "\n",
      "  File \"/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/xgboost/training.py\", line 81, in _train_internal\n",
      "    bst.update(dtrain, i, obj)\n",
      "\n",
      "  File \"/home/michal/anaconda3/envs/CDV/lib/python3.9/site-packages/xgboost/core.py\", line 1680, in update\n",
      "    _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[18:00:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-21-33e5c45e0348>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mestim\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mHyperoptEstimator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mregressor\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mxgboost_classification\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'my_gb'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_evals\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m50\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrial_timeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m60\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mestim\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf1_score\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_test\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mestim\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_test\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/CDV/lib/python3.9/site-packages/hpsklearn/estimator/estimator.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y, EX_list, valid_size, n_folds, cv_shuffle, warm_start, random_state)\u001B[0m\n\u001B[1;32m    478\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    479\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrefit\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 480\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_retrain_best_model_on_full_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mEX_list\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mEX_list\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    481\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    482\u001B[0m     def predict(self, X,\n",
      "\u001B[0;32m~/anaconda3/envs/CDV/lib/python3.9/site-packages/hpsklearn/estimator/estimator.py\u001B[0m in \u001B[0;36m_retrain_best_model_on_full_data\u001B[0;34m(self, X, y, EX_list)\u001B[0m\n\u001B[1;32m    393\u001B[0m             )\n\u001B[1;32m    394\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 395\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_best_learner\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mXEX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    396\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    397\u001B[0m     def fit(self, X, y,\n",
      "\u001B[0;32m~/anaconda3/envs/CDV/lib/python3.9/site-packages/xgboost/core.py\u001B[0m in \u001B[0;36minner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    504\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    505\u001B[0m             \u001B[0mkwargs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 506\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    507\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    508\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0minner_f\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/CDV/lib/python3.9/site-packages/xgboost/sklearn.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001B[0m\n\u001B[1;32m   1248\u001B[0m         )\n\u001B[1;32m   1249\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1250\u001B[0;31m         self._Booster = train(\n\u001B[0m\u001B[1;32m   1251\u001B[0m             \u001B[0mparams\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1252\u001B[0m             \u001B[0mtrain_dmatrix\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/CDV/lib/python3.9/site-packages/xgboost/training.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001B[0m\n\u001B[1;32m    186\u001B[0m     \u001B[0mBooster\u001B[0m \u001B[0;34m:\u001B[0m \u001B[0ma\u001B[0m \u001B[0mtrained\u001B[0m \u001B[0mbooster\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    187\u001B[0m     \"\"\"\n\u001B[0;32m--> 188\u001B[0;31m     bst = _train_internal(params, dtrain,\n\u001B[0m\u001B[1;32m    189\u001B[0m                           \u001B[0mnum_boost_round\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnum_boost_round\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    190\u001B[0m                           \u001B[0mevals\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mevals\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/CDV/lib/python3.9/site-packages/xgboost/training.py\u001B[0m in \u001B[0;36m_train_internal\u001B[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001B[0m\n\u001B[1;32m     79\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbefore_iteration\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbst\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtrain\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevals\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     80\u001B[0m             \u001B[0;32mbreak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 81\u001B[0;31m         \u001B[0mbst\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdtrain\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     82\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mafter_iteration\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbst\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtrain\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevals\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     83\u001B[0m             \u001B[0;32mbreak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/CDV/lib/python3.9/site-packages/xgboost/core.py\u001B[0m in \u001B[0;36mupdate\u001B[0;34m(self, dtrain, iteration, fobj)\u001B[0m\n\u001B[1;32m   1678\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1679\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mfobj\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1680\u001B[0;31m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001B[0m\u001B[1;32m   1681\u001B[0m                                                     \u001B[0mctypes\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mc_int\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miteration\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1682\u001B[0m                                                     dtrain.handle))\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from hpsklearn import HyperoptEstimator, xgboost_classification\n",
    "\n",
    "estim = HyperoptEstimator(regressor=xgboost_classification('my_gb'), max_evals=50, trial_timeout=60)\n",
    "\n",
    "estim.fit(X_train, y_train)\n",
    "\n",
    "print(f1_score(y_test, estim.predict(X_test)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}